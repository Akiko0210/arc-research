This website uses cookies to deliver some of our products and services as well as for analytics and to provide you a more personalized experience. Click here to learn more. By continuing to use this site, you agree to our use of cookies. We've also updated our Privacy Notice. Click here to see what's new.
Weitao Song, Xin Li, Yuanjin Zheng, Yue Liu, and Yongtian Wang






We propose a novel method to construct an optical see-through retinal-projection near-eye display using the Maxwellian view and a holographic method. To provide a dynamic full-color virtual image, a single phase-only spatial light modulator (SLM) was employed in conjunction with a multiplexing-encoding holographic method. Holographic virtual images can be directly projected onto the retina using an optical see-through eyepiece. The virtual image is sufficiently clear when the crystal lens can focus at different depths; the presented method can resolve convergence and accommodation conflict during the use of near-eye displays. To verify the proposed method, a proof-of-concept prototype was developed to provide vivid virtual images alongside real-world ones.

© 2021 Optical Society of America under the terms of the OSA Open Access Publishing Agreement
Near-eye displays are the most promising mobile displays for artificial intelligence computing, and have thus received significant attention from academia and industries [1–3]. Most commercial near-eye displays can only generate one virtual display depth, so virtual images are blurred when the crystalline lens focuses on an area away from this virtual plane. Many studies have reported visual confusion and fatigue in association with these displays [4,5], and these are exacerbated in augmented reality display applications.
To solve these problems, virtual images can be generated using methods capable of more accurately reconstructing real-world objects. Multiple focal planes and varifocal planes can be used in near-eye displays to generate virtual three-dimensional (3D) objects at different depths. Multiple virtual planes can be generated using spatial-multiplexed [6,7] or time-multiplexed methods [8–10], and the virtual depth can be changed by adjusting dynamic optical or mechanical properties [11,12]. Multiple focal planes and varifocal planes can help to alleviate the problems of visual confusion and fatigue, but do not solve them. Light-field displays provide direct sufficient light into the user’s pupils for correct focus, imitating the process observed in the real world. A pinhole array [13–15] or micro-lens array [16–18] is often employed as an SLM to sample the light from pixels on the display panel on, and create the light field of a 3D scene within the exit pupil of the eyepiece. To make full use of the pixels, many optimization methods [19–22] have been applied to improve display performance according to the characteristics of the human visual system (HVS). In these methods, limited pixels are used to provide depth, angular or wavefront information, which can lead to low-resolution virtual images. Methods that create holographic images [23,24] have also been attractive, as they contain both amplitude and phase information. Holographic methods and near-eye displays can be combined based on wave optics. The light wavefront is often controlled by SLMs [25–27], which can control all light information of the virtual images. Nevertheless, holographic near-eye displays still suffer from low resolution. Moreover, due to étendue conservation [28], the product of the field of view and exit pupil in near-eye displays should be less than that of the diffraction angle and effective aperture of the SLM. The étendue of SLMs (limit of the diffraction angle and effective aperture) commonly used in holographic displays is often very small, and cannot achieve both a large field of view and large eye box. This problem also limits the development of holographic near-eye displays.
A near-eye display with an accommodation-free virtual image could also resolve the convergence and accommodation conflict. The virtual image has a large depth of field, and the image is clear regardless of whether the eye fixates on a near or distant object. Most accommodation-free displays use the Maxwellian view, which is based on an experiment conducted by James Clerk Maxwell in 1868 [29]. Thin parallel beams, which can be provided by a specially designed illumination system, are emitted from an SLM, such as a liquid crystal display (LCD), digital micromirror device (DMD) or liquid crystal on silicon (LCoS), and converge at the center of the pupil via a lens system and are directly projected onto the retina [30,31]. When the method was first introduced, it suffered from a small exit pupil, so was not tolerable by all users (depending on the interpupillary distance), and allowed little room for the eyes to swivel within their sockets such that vignetting occurred. Many works have reported the convergence of multiple beam centers to extend the exit pupil, and accommodation-free near-eye displays have received attention [32–34]. As mentioned above, the holographic display method allows full control of light information but has limitations associated with étendue conservation. Combining the holographic method with the Maxwellian view allows an accommodation-free virtual image to be directly projected onto the human retina by a near-eye display, thus resolving the problems of visual confusion and fatigue. Flexible control of virtual images in a holographic display with the Maxwellian view has also been demonstrated via wavefront modulation [35]. Different types of reconstruction methods such as the spherical wave type and the plane wave type have been analyzed in holographic retinal near-eye displays [36]. Moreover, other methods to solve the convergence and accommodation conflict can also been introduced to provide more information to human eyes [37]. The aim of holographic retinal displays is to generate a dynamic, high-resolution full-color accommodation-free virtual scene alongside a real-world one. However, to the best of our knowledge, only one-color static images have been demonstrated using this method. Spatial multiplexing or time multiplexing will increase the complexity of the system by introducing more display devices or a high refresh rate SLM.
In this study, a full-color retinal-projection near-eye display using a multiplexing-encoding dynamic holographic method was developed. One single phase-only SLM and three-color lasers are used to obtain dynamic full-color images. An optics see-through eyepiece is used to project the virtual image onto the human retina, while allowing real-world light to be observed simultaneously. A proof-of-concept experiment is reported to demonstrate that the proposed method can provide a vivid virtual image.
Figure 1 shows a diagram of our proposed full-color retinal-projection near-eye display. Red (R), green (G) and blue (B) lasers are unitized to illuminate one phase-only SLM. Multiplexing encoding and complex amplitude modulation methods have been used to obtain computer-generated holograms (CGHs). Holographic patterns can be generated and dynamically loaded onto the SLM to reconstruct a virtual image in the air. The used phase-only SLM is a reflective-type opto-electro modulator. The optical axis can be designed vertically along with the modulation plane of the SLM, and a beam splitter should be introduced into the optical path in practical optical system due to the reflective-type. Thus, the power efficiency would be reduced and the system will be more complex. A tilt SLM with little angle will not introduce any obvious noise which has been provided in previous works [38]. Due to the advantages of this design, we consequently used this structure as shown in Fig. 1. Spatial filtering and other optical techniques can be used to filter the unwanted light and control the position of the image. As mentioned above, a holographic image with a small field of view cannot be easily viewed by the human eye due to the limitations associated with étendue conservation. An eyepiece is used to project the holographic image onto the human retina, which can be immersive or optically transparent. As shown in Fig. 1, the eyepiece consists of a see-through optical combiner and an optical lens that converge the light arising from the holographic image. Via the see-through eyepiece, a full-color accommodation-free holographic image can be observed along a real-world image.
Fig. 1. Schematic diagram of a full-color retinal-projection near-eye display using multiplexing-encoding dynamic holographic method.
Download Full Size | PDF
To realize a high-quality color holographic display, we employed multiplexing encoding and complex amplitude modulation methods to obtain CGHs [39,40]. Complex amplitude information cannot be directly loaded onto a commercial SLM, as they can only manipulate one degree of freedom of light. To overcome this issue, the kinoform technique is commonly applied, where the target scene is set as a diffuser and its amplitude on a hologram plane is considered as a constant; consequently, some information is lost. Complex amplitude modulation is an effective method for high-quality holographic reconstruction, and is used in this work. The complex amplitude distribution of the target image propagating to the hologram plane is given by E(x,y) = E0(x,y)exp[iφ(x,y)], where E0(x,y) is the amplitude, φ(x,y) is the phase, and i is an imaginary unit. The amplitude distribution can be recorded using pure phase optical holography based on the bleaching method, as follows:

The phase factor kysin ϕ indicates that the target image is reconstructed with a tilt angle of ϕ.
A color image can be divided into R, G, and B components. Since these components are encoded in a single CGH with corresponding wavelengths, the crosstalk associated with illumination via three wavelengths should be considered. As shown in Fig. 2(a), the R, G, and B components of the target images are encoded based on off-axis holography, where the reference beams are planar waves with different incident angles along the horizontal direction. Hence, a CGH can be written as

Fig. 2. Schematic of the CGH encoding and its reconstruction. (a) encoding CGH; (b) reconstructing target image reconstructed.
Download Full Size | PDF
During reconstruction, the illumination beam for each component is incident from the angle shown in Fig. 2(b), and can be described as

Equation (5c) represents the zero orders for R, G, and B, Eq. (5d) is the target color image, Eq. (5e) describes unwanted images in the first order, and Eothers(x,y) represents other orders in the reconstruction and background noise in the display. Theoretically, these terms are separated from each other spatially, however, as the maximum diffraction angle of a commercial SLM is only a few degrees, the target color image is not clearly distinguishable from the noise. We applied “4f filtering” to filter out unwanted terms and avoid the zero order of the SLM.
In retinal-projection near-eye displays, thin parallel beams from the virtual image converge at the center of the pupil via the lens system and are directly projected onto the retina. Ideally, the size of the converged point should be infinitesimal, such that the accommodation function of the crystal lens in the eye will not work. In actual systems, light beams from pixels cannot be infinitely thin, and the size of the converged point can only be controlled to be far smaller than that of the human pupil. Thus, the virtual image has a large depth of field, and the image will be clear no matter whether the human eye fixates on a near or distant object.
Figure 3(a) shows the natural process of viewing real-world objects and the retinal-projection near-eye display. When we observe the real world, light from a 3D scene passes through the pupil and is focused on the retina via the accommodation function of the crystal lens. The size of the pupil, Ee, is the entrance pupil when the human eye is treated as an ordinary lens system. For the retinal-projection near-eye display, the focal depth of the virtual image is located at a distance, ld, from the human eye, and the size of the converged beam of the virtual image is denoted as EC. Supposing the focal length of the human eye is fe, and the diameter of the circle of confusion on the retina is set as ɛ, then, according to the Gauss formula, the relationship between the focal length and the object and image distances is given as

Based on the circle of confusion, half of the depth of focus Δl’ can be formulated as

Fig. 3. (a) Comparison between the natural viewing and Maxwell-view near-eye displays. (b) the characteristics of the retinal-projection near-eye display method.
Download Full Size | PDF
Thus, the front depth of field, lf, and rear depth of field, lr, satisfy the following equation:

According to Eqs (6–8), the depth of field (the summation of lr and lf) is given by Eq. (9), which shows that the depth of field will increase with a decrease in the size of the converged beam. Therefore, during the design of the retinal-projection near-eye display, the size of the converged beam should be controlled to be as small as possible to obtain a virtual image with a very large field of view.

In our near-eye display using a multiplexing-encoding holographic method, the size of the generated holographic image is y0 and the diffraction angle is θ. The focal length of the eyepiece is f, and either an immersive or optical see-through eyepiece can be treated as a convex lens. The size of the converged beam EH can be calculated using Eq. (10), and the field of view ω is given by Eq. (11).

The central position of the depth field is also very important, and should be controlled during the design process. The distance of virtual images li and yi can be obtained as follows:

Optical experiments were performed to test the proposed method. The experimental set-up for the tests is shown in Fig. 4. In the experiments, three laser diodes (LDs) with wavelengths of 632, 532 and 430 nm were used, and parallel beams were obtained via spatial filter elements and collimator lenses. The resolution of the SLM was 1,920 × 1,080 pixels, with a pixel pitch of 8 µm. CGHs was obtained using the multiplexing-encoding method, and a holographic image could be reconstructed in air when the holograms were loaded onto the SLM under illumination from the three LDs. Filling defects cause a multiple order diffraction effect, such as grating, and impair image quality. Moreover, full-color images are generated by combining the virtual images from the three color channels, and multiple order diffraction from the different channels can also affect the image quality. To solve this problem, a 4f lens system and band-pass filter (BPF) were used to eliminate unwanted light, so dynamic holographic scenes with high image quality could be generated anywhere.
Fig. 4. Setup of full-color retinal-projection near-eye display using multiplexing-encoding dynamic holographic method.
Download Full Size | PDF
A see-through eyepiece with a bird-bath structure, consisting of a beam splitter and a convex reflective mirror, was used to converge the holographic image light. The rays from the holographic image are transmitted through a half mirror inside a cube prism and reflected by the convex reflective mirror. The eyepiece material was China K9 glass, which has the same parameters as BK7 glass, and the size of the cube prism was 25.4 mm. The radius of the reflective mirror was 75.8 mm, with an effective focal length of 30.6 mm, and the eye relief distance was 20 mm from the eyepiece. The holographic virtual image was located around 1.5 m from the convergence point. In terms of holographic images generated by SLM, the reconstructed distance and the size of reconstructed images could be flexible as long as the images in the viewing zone. In the experimental set-up, the holographic images were generated around the position of SLM, and the size of the images is set as the same as that of SLM, 15.36mm × 8.64mm. Due to the usage of 4f system, the size of generated image near the focal plane of eyepiece will be the same size. According to the Gaussian lens formula, the effective focal length (30.6 mm) and the virtual image position (around 1.5m), the field of view will be 16.4° × 28.7°, and the diagonal field of view can be 32°. Obviously, the structure and these parameters can be optimized by designing the optical system. A Sony camera was employed to emulate the accommodation function of the human eye, and two toys were used as reference objects, located at distances of 0.5 and 2 m, respectively. The real-world 3D objects were imaged by the camera lens, which was focused on different positions. When the position of the CMOS camera is fixed, only one position can be clearly focused on by the sensor; objects at other depths will be blurred due to defocusing. The light beam of a holographic image will focus on a very small point, and the virtual image can be projected directly onto the sensor as the camera lens will not work.
Due to the Maxwellian view, 2D images are of sufficiently high quality; there is no need for 3D images. Two sample videos of a model of the Earth and a butterfly were used as the dynamic virtual holographic images. One video was separated into a series of 2D pictures, and kinoform holograms 1,920 × 1,080 pixels in size were generated for each 2D image using the multiplexing-encoding dynamic holographic method. In our experiment, the maximum full diffraction angle of the used SLM is 3.36°, and the angle difference between adjacent color channels should smaller than 1.12°, which is 1/3 of maximum full diffraction angle according to Eq. (5). Furthermore, a tilt SLM with a little angle was used in the experiment, which can make the system compact and increase the light efficiency. In this kind of structure, the smaller the tilt angle, the better performance will be obtained. Thus, the maximum angle of color light was set as small as 4°, and the angle difference between adjacent color channels was set as 0.97°. During the calculation and simulation process, the incident angles of the R, G and B reference lights were 2.03°, 3.0°, and 3.97°, respectively, as in the experimental set-up. The holograms of the 1,920 × 1,080-pixel 2D color images generated using our method can be obtained in advance and loaded into the SLM, and the calculation process can be conducted in real time when GPUs and parallel calculation methods are used.
Figures 5(a)–5(b) shows selected frames from the videos, and Figs. 5(c)–5(d) shows the color images using simulation results. The information displayed using the experimental set up is shown in Figs. 5(e)–5(h), and was captured when the camera was focused at depths of 0.5 m (2 diopters) and 2 m (0.5 diopters). When the camera focuses on a nearby location, the nearer real object is clear and the farther real object is blurred, and vice versa. The generated virtual holographic images are very clear at both focusing depths, and can be directly observed by the human eye alongside real-world images. For the simulated and actual holographic images, image details can be represented accurately, but colors cannot. There is an effect of nonlinear amplitude mapping between the original video images and the reconstructed ones, and there is a color difference between the actual light source and the ideal RGB source. This discrepancy can be improved and corrected by colorimetric characterization, which has already been applied in commercial display products. To further verify the presented method, two videos showing a continuous change in the sensor position of the camera, which can focus at depths from 0.2–3 m, are provided in Visualizations 1 and 2. The frame rate of dynamic holographic video determined by the spatial light modulator (SLM) in our system, and the maximum frame rate of the used SLM in the experiment (Holoeye PLUTO 2) can reach 60 Hz. The high-frame-rate display is not the core contribution of our work, and the practical rate in our experiment was set 25 Hz. Dynamic virtual holographic images can also be seen in these two videos, alongside real-world images. In the experimental videos, ghost images or stray light can be found. They are generated by multiple reflections and transmission on the surfaces of eyepiece, and the process can be analyzed by raytracing the light path. It can be eliminated or alleviated the ghost images or stray light by optimizing the optical system in the future work. The experimental results demonstrated that our method can produce a holographic scene with a very large field of view that can be directly projected onto the retina and viewed alongside real-world images.
Fig. 5. (a-b) selected 2D frames from the videos. (c-d) simulation color 2D images using the multiplexing-encoding holographic method (e-f) Captured images when the camera is focused at 2 m (0.5 diopter). (g-h) Captured images when the camera is focused at 0.5 m (2 diopter). Captured video from a continuous change in the sensor position of the camera, which can focus at depths from 0.2–3 m. Videos for continuous change of the focus position of camera are shown in Visualization 1 and Visualization 2.
Download Full Size | PDF
This paper describes how dynamic holographic virtual images were directly projected onto the human retina using the proposed near-eye display method with multiplexing-encoding holography. Full-color images can be generated using a single phase-only SLM with three different-colored lasers, and a see-through eyepiece renders the virtual images free of accommodation, along with the full-depth real-world images. The principles and characteristics of the method have been presented in detail. The experimental results demonstrated the validity of the presented method, and this work provides valuable insights into holographic and retinal-projection near-eye displays. Future works will aim to improve display performance and apply the proposed method to other structures of optical see-through near-eye displays to make them more compact.
National Key Research and Development Program of China (2020YFC1523103); National Natural Science Foundation of China (62002018, 61727808); A*STAR RIE2020 AME Programmatic Funding (A18A7b0058).
The authors declare no conflicts of interest.
1. O. Cakmakci and J. Rolland, “Head-Worn Displays: A Review,” J. Disp. Technol. 2(3), 199–216 (2006). [CrossRef]  
2. H. Hua, “Enabling focus cues in head-mounted displays,” Proc. IEEE 105(5), 805–824 (2017). [CrossRef]  
3. G. A. Koulieris, K. Akşit, M. Stengel, R. K. Mantiuk, K. Mania, and C. Richardt, “Near-Eye Display and Tracking Technologies for Virtual and Augmented Reality,” In Computer Graphics Forum, 38(2), 493–519 (2019).
4. D. M. Hoffman, A. R. Girshick, K. Akeley, and M. S. Banks, “Vergence–accommodation conflicts hinder visual performance and cause visual fatigue,” Journal of Vision 8(3), 33 (2008). [CrossRef]  
5. G. Kramida, “Resolving the Vergence-Accommodation Conflict in Head-Mounted Displays,” IEEE Trans. Visual. Comput. Graphics 22(7), 1912–1931 (2016). [CrossRef]  
6. J. Rolland, M. Krueger, and A. Goon, “Multifocal planes head-mounted displays,” Appl. Opt. 39(19), 3209–3215 (2000). [CrossRef]  
7. K. Akeley, S. J. Watt, A. R. Girshick, and M. S. Banks, “A stereo display prototype with multiple focal distances,” ACM Trans. Graph. 23(3), 804–813 (2004). [CrossRef]  
8. G. D. Love, D. M. Hoffman, P. J. W. Hands, J. Gao, A. K. Kirby, and M. S. Banks, “High-speed switchable lens enables the development of a volumetric stereoscopic display,” Opt. Express 17(18), 15716–15725 (2009). [CrossRef]  
9. S. Liu, H. Hua, and D. Cheng, “A novel prototype for an optical see-through head-mounted display with addressable focus cues,” IEEE Trans. Visual. Comput. Graphics 16(3), 381–393 (2009). [CrossRef]  
10. Q. Chen, Z. Peng, Y. Li, S. Liu, P. Zhou, J. Gu, J. Lu, L. Yao, M. Wang, and Y. Su, “Multi-plane augmented reality display based on cholesteric liquid crystal reflective films,” Opt. Express 27(9), 12039–12047 (2019). [CrossRef]  
11. N. Matsuda, A. Fix, and D. Lanman, “Focal surface displays,” ACM Trans. Graph. 36(4), 1–14 (2017). [CrossRef]  
12. X. Xia, Y. Guan, A. State, P. Chakravarthula, K. Rathinavel, T. J. Cham, and H. Fuchs, “Towards a Switchable AR/VR Near-eye Display with Accommodation-Vergence and Eyeglass Prescription Support,” IEEE Trans. Visual. Comput. Graphics 25(11), 3114–3124 (2019). [CrossRef]  
13. A. Maimone, D. Lanman, K. Rathinavel, K. Keller, D. Luebke, and H. Fuchs, “Pinlight displays: wide field of view augmented reality eyeglasses using defocused point light sources,” ACM Trans. Graph. 33(4), 1–11 (2014). [CrossRef]  
14. K. Akşit, J. Kautz, and D. Luebke, “Slim near-eye display using pinhole aperture arrays,” Appl. Opt. 54(11), 3422–3427 (2015). [CrossRef]  
15. W. Song, Y. Wang, D. Cheng, and Y. Liu, “Light f ield head-mounted display with correct focus cue using micro structure array,” Chin. Opt. Lett. 12(6), 060010 (2014). [CrossRef]  
16. C. Yao, D. Cheng, T. Yang, and Y. Wang, “Design of an optical see-through light-field near-eye display using a discrete lenslet array,” Opt. Express 26(14), 18292–18301 (2018). [CrossRef]  
17. H. Huang and H. Hua, “High-performance integral-imaging-based light field augmented reality display using freeform optics,” Opt. Express 26(13), 17578–17590 (2018). [CrossRef]  
18. D. Lanman and D. Luebke, “Near-eye light field displays,” ACM Trans. Graph. 32(6), 1–10 (2013). [CrossRef]  
19. M. Xu and H. Hua, “Systematic method for modeling and characterizing multilayer light field displays,” Opt. Express 28(2), 1014–1036 (2020). [CrossRef]  
20. F. Huang, K. Chen, and G. Wetzstein, “The light field stereoscope: immersive computer graphics via factored near-eye light field displays with focus cues,” ACM Trans. Graph. 34(4), 1–12 (2010). [CrossRef]  
21. D. Chen, X. Sang, X. Yu, X. Zeng, S. Xie, and N. Guo, “Performance improvement of compressive light field display with the viewing-position-dependent weight distribution,” Opt. Express 24(26), 29781–29793 (2016). [CrossRef]  
22. M. Liu, C. Lu, H. Li, and X. Liu, “Near eye light field display based on human visual features,” Opt. Express 25(9), 9886 (2017). [CrossRef]  
23. Y. Pan, J. Liu, X. Li, and Y. Wang, “A review of dynamic holographic three-dimensional display: algorithms, devices, and systems,” IEEE Trans. Ind. Inf. 12(4), 1599–1610 (2016). [CrossRef]  
24. K. Wakunami, P. Y. Hsieh, R. Oi, T. Senoh, H. Sasaki, Y. Ichihashi, M. Okui, Y. P. Huang, and K. Yamamoto, “Projection-type see-through holographic three-dimensional display,” Nat. Commun. 7(1), 12954–7 (2016). [CrossRef]  
25. A. Maimone, A. Georgiou, and J. Kollin, “Holographic near-eye displays for virtual and augmented reality,” ACM Trans. Graph. 36(4), 1–16 (2017). [CrossRef]  
26. C. Chang, W. Cui, and L. Gao, “Foveated holographic near-eye 3D display,” Opt. Express 28(2), 1345–1356 (2020). [CrossRef]  
27. Q. Gao, J. Liu, J. Han, and X. Li, “Monocular 3D see-through head-mounted display via complex amplitude modulation,” Opt. Express 24(15), 17372–17383 (2016). [CrossRef]  
28. J. Chaves, Introduction to nonimaging optics. CRC press, 103–108 (2017)
29. G. Westheimer, “The Maxwellian View,” Vision Res. 6(11-12), 669–682 (1966). [CrossRef]  
30. M. Sugawara, M. Suzuki, and N. Miyauchi, “Late-News Paper: Retinal imaging laser (or LED) eyewear with focus-free and augmented reality,” Sid Symposium Digest of Technical Papers 47(1), 164–167 (2016). [CrossRef]  
31. Y. Ochiai, K. Otao, Y. Itoh, S. Imai, K. Takazawa, H. Osone, A. Mori, and I. Suzuki, “Make your own retinal projector: retinal near-eye displays via metamaterials,”, In ACM SIGGRAPH 2018 Posters1–2 (2018).
32. C. Jang, K. Bang, S. Moon, J. Kim, S. Lee, and B. Lee, “Retinal 3D: augmented reality near-eye display via pupil-tracked light field projection on retina,” ACM Trans. Graph. 36(6), 1–13 (2017). [CrossRef]  
33. J. Kim, Y. Jeong, M. Stengel, K. Aksit, R. Albert, B. Boudaoud, T. Greer, W. Lopes, Z. Majercik, P. Shirley, J. Spjut, M. McGuire, and D. Luebke, “Foveated AR: dynamically-foveated augmented reality display,” ACM Trans. Graph. 38(4), 1–15 (2019). [CrossRef]  
34. S. Kim and J. Park, “Optical see-through Maxwellian near-to-eye display with an enlarged eyebox,” Opt. Express 43(4), 767–770 (2018). [CrossRef]  
35. Y. Takaki and N. Fujimoto, “Flexible retinal image formation by holographic Maxwellian-view display,” Opt. Express 26(18), 22985–22999 (2018). [CrossRef]  
36. J. S. Lee, Y. K. Kim, M. Y. Lee, and Y. H. Won, “Enhanced see-through near-eye display using time-division multiplexing of a Maxwellian-view and holographic display,” Opt. Express 27(2), 689–701 (2019). [CrossRef]  
37. Z. Wang, X. Zhang, G. Lv, Q. Feng, H. Ming, and A. Wang, “Hybrid holographic Maxwellian near-eye display based on spherical wave and plane wave reconstruction for augmented reality display,” Opt. Express 29(4), 4927–4935 (2021). [CrossRef]  
38. T. Kozacki, “Holographic display with tilted spatial light modulator,” Appl. Opt. 50(20), 3579–3588 (2011). [CrossRef]  
39. X. Li, J. Liu, J. Jia, Y. Pan, and Y. Wang, “3D dynamic holographic display by modulating complex amplitude experimentally,” Opt. Express 21(18), 20577–20587 (2013). [CrossRef]  
40. G. Xue, J. Liu, X. Li, J. Jia, Z. Zhang, B. Hu, and Y. Wang, “Multiplexing encoding method for full-color dynamic 3D holographic display,” Opt. Express 22(15), 18473–18482 (2014). [CrossRef]  










Abstract
1. Introduction
2. Principle of a full-color retinal-projection near-eye display using a multiplexing-encoding dynamic holographic method
3. Experimental set-up and results
4. Conclusions
Funding
Disclosures
References
This website uses cookies to deliver some of our products and services as well as for analytics and to provide you a more personalized experience. Click here to learn more. By continuing to use this site, you agree to our use of cookies. We've also updated our Privacy Notice. Click here to see what's new.
Youngjin Jo, Chanhyung Yoo, Kiseung Bang, Byounghyo Lee, and Byoungho Lee




We introduce an approach to expand the eye-box in a retinal-projection-based near-eye display. The retinal projection display has the advantage of providing clear images in a wide depth range; however, it has difficulty in practical use with a narrow eye-box. Here, we propose a method to enhance the eye-box of the retinal projection display by generating multiple independent viewpoints, maintaining a wide depth of field. The method prevents images projected from multiple viewpoints from overlapping one other in the retina. As a result, our proposed system can provide a continuous image over a wide viewing angle without an eye tracker or image update. We discuss the optical design for the proposed method and verify its feasibility through simulation and experiment.

© 2020 Optical Society of America under the terms of the OSA Open Access Publishing Agreement
Augmented reality (AR) can play an important role in connecting the digital and physical world in the Fourth Industrial Revolution. In particular, AR near-eye display (AR NED) is receiving much attention for its applicability to various industries. Unlike AR based on hand-held devices, AR NEDs, such as glasses, are worn on the head, which will help to facilitate access to virtual spaces anywhere in everyday life. To achieve this, AR NEDs should have high portability and be able to provide the real scene clearly. An image combiner allows AR NEDs to deliver virtual images simultaneously with high optical transparency. Various methods for the image combiner have been considered. Depending on which method is adopted, visual performance and a form factor of AR NEDs are primarily determined. Among them, diffractive elements have advantages of being thin and transparent. In particular, holographic optical elements (HOEs) are capable of recording the desired volume grating from the type of lens to the complex wavefront using the interference of light. For this reason, research on using the HOE for AR NEDs has been actively conducted [1].
For AR NEDs to become a part of our lives, further consideration, such as visual fatigue, should be regarded to avoid discomfort from wearing for a long time. In the NEDs, visual fatigue could arise when there is a difference between the real scene and the displayed image. When looking at an object in the real world, depth information is obtained from physical cues such as accommodation and vergence derived from the eye, as well as psychological cues. In a typical NED, the virtual image is physically located at a certain depth by a fixed focal lens. As the object is positioned away from this depth, the vergence distance provided by binocular disparity is inconsistent with the focus distance. If the difference is too large, the vergence–accommodation conflict (VAC) could cause the user to feel discomfort [2–4].
For a solution to this problem, many methods have been proposed, such as generating multiple focal planes [5–9] and adjusting the location of a focal plane according to the vergence distance [10–14]. Holographic displays that can create objects at desired depths through wavefront modulation can also effectively reduce the VAC problem [15,16]. However, an additional optical system can make a system bulky, and a hologram requires many resources for computation. AR NEDs, with the support of external real-time interaction, still need further development of hardware. The retinal projection display (RPD) adopts a relatively simple optical system and increases the depth of field (DOF) [17–21]. It can provide a vivid image in a wide depth range without retinal blur. It has been proven by researchers that this focus-free system can alleviate the VAC problem [22,23].
The RPD uses a lens to focus the image into a small point of the pupil and project it to the retina. This makes retinal blur be less sensitive to the user’s accommodation. Depending on the size of the exit-pupil and the sensor’s resolution, the depth range that can provide clear images will be determined. However, there is a problem in which the eye-box that can view images is limited because they are focused at one point. This may cause the image to be blank if the eye deviates from the eye-box by displacement due to eye rotation. In the RPD using a HOE as the image combiner, several methods have been proposed to extend the small eye-box. Previous research was to provide multiple viewpoints [24,25] or to dynamically shift the viewpoint according to the location of the pupil [19,26,27]. Figure 1(a) indicates the implementation of a dynamic eye-box by adjusting the incident angle for the HOE. It requires an eye tracker as well as a mechanical device such as a moving HOE module or steering mirror. Another approach is to increase the number of viewpoints by simultaneously recording multiple angles of the lens on the HOE, as shown in Fig. 1(b). In this case, there is a problem in that each viewpoint image overlaps or there exists a blank while the user changes the viewpoint [28]. Also, the eye tracker is essential, and the images should be updated according to the viewpoint.
Fig. 1. Eye-box expanding methods in the RPD using HOE: (a) shifting the viewpoint by projecting the images onto the HOE in different angles and (b) recording the HOE at different angles. Both methods should track the user’s gaze and provide a corresponding image. (c) The proposed method is capable of providing continuous images within the eye-box without an additional tracker.
Download Full Size | PDF

In this paper, a novel method to expand the eye-box of the RPD is proposed. The proposed method constructs independent viewpoints that can implement retinal projection with a wide DOF without an eye tracker. As shown in Fig. 1(c), multiple HOEs were designed to form different viewpoints according to the spatial position of HOEs. Each viewpoint image represents a different area of the perspective scene and does not overlap with each other in the projection plane. Therefore, this method is relatively free of eye rotation with a continuous and natural transition between viewpoints. In Section 2, the principle of the proposed method is introduced with a theoretical analysis of the RPD. Then, we prove the concept of the proposed display system through simulation using LightTools and Zemax in Section 3. A demonstration of the experiment and a discussion of the system take place in Sections 4 and 5, respectively. We conclude with the discussion of the future works to improve the proposed system in Section 6.
The optical system for AR NEDs can be considered in three parts: generating, relaying, and combining the images. Of these, the image combiner, which merges real scenes and virtual images, has a direct effect on the performance of the NED system [29]. This element should have a transparent property to view the external scene and serve to deliver the virtual image. In this paper, the lightguide with a HOE is utilized as an image combiner. The HOE is a transparent element that can reproduce the wavefront, which is usually recorded to act as a lens or mirror. Due to an angular selectivity of the volume gratings in the HOE, high efficiency is obtained only for the recorded wavelength [30–32]. With these advantages and thin characteristic, the HOE is suitable for the image combiner of the AR NED.
The typical designs of HOE-based AR NEDs are described in Fig. 2. The HOEs in both systems are recorded as a concave mirror that focuses the incident light, which enlarges the display image. At this time, the eye-box is defined as an area where the image can be viewed with a uniform field of view (FOV). It depends on the divergence angle of the display light and the efficiency of the HOE. The system in Fig. 2(a) is a basic configuration for the propagation of the image in free space. It is necessary to correct the image distortion caused by the oblique incidence and a difference in native resolution according to the incident position to the HOE [28]. Compared to this, the system in Fig. 2(b) relays the image to the HOE through the total internal reflection in the lightguide. Although the system becomes more complex, the spatial uniformity of the image can be improved.
Fig. 2. Configuration of the retinal projection displays. (a) Free-space propagation method in which the image from the display directly enters the image combiner from the air and (b) lightguide method in which the light propagates in the glass. (c) By guiding on-axis with a half-mirror inside the lightguide, off-axis aberration in the image combiner can be reduced. (d) In the case of on-axis incidence, the maximum field of view is determined by the eye-relief and lightguide thickness. The simulation is conducted under the guiding angle of 60°.
Download Full Size | PDF

When a lightguide is utilized, it can be divided into on- and off-axis incidence according to the direction in which the image enters the combiner, as shown in Figs. 2(b) and 2(c). For on-axis incidence, a reflective optics is necessary to match the focal plane of the image perpendicular to the optical axis of the HOE after the total internal reflection. In this paper, a half-mirror is used inside the lightguide. The discussion in the case of the off-axis incidence is described in Section 5.A. The θm is the slanted angle of the half-mirror, and it should be half of the guiding angle θg to reflect the image vertically at the end of the lightguide, as shown in Fig. 2(c). At a guiding angle smaller than 60°, the area reflected twice from the half-mirror results in an overlap, and in the opposite case, the lightguide thickness increases. Therefore, we choose 60° for the guiding angle. The maximum FOV supported by the lightguide-based RPD for eye-relief and lightguide thickness is shown in Fig. 2(d).
The light rays entering the aperture–lens system, such as the eye lens, are focused on the image plane according to the lens formula, as shown in Fig. 3(a). In the case of being out of focus due to the change in the focal plane, defocus blur will occur on a sensor. Since the lights converge into a small exit-pupil, the RPD can be considered a system with a small aperture [33]. It results in a wide depth range in which the image can be sharply focused. As the DOF increases, the RPD can effectively reduce the VAC problem that could cause visual fatigue in AR NEDs.
Fig. 3. (a) Definition of DOF in imaging systems. It is possible to define the closest and farthest object plane where the defocus blur is within the sensor’s pixel pitch. (b) DOF and spatial frequency corresponding to the Rayleigh criterion versus aperture size when the focal length is 4.6 mm with a wavelength of 530 nm.
Download Full Size | PDF

In the optical system with an aperture, the focus distance giving the maximum DOF can be expressed with the hyperfocal distance [34]. Here, dn denotes the nearest object plane of acceptable sharpness, df is the farthest object plane, and f is the focal length of the lens. DOF can be calculated as the difference between dn and df. When the farthest object plane is at infinity, the hyperfocal distance dh is defined as 2dn and can be calculated as follows:

Here, N represents the F-number of the optical system, and the circle of confusion c is considered as the pixel pitch at the sensor plane. Figure 3(b) shows the DOF for the exit-pupil size. We designed the spot size at the pupil of the RPD based on the relation between the exit-pupil and DOF. For calculation, we utilized the camera specification instead of the eye to verify the principle through experimental results. Based on a 2.9 mm aperture size and a 4.3 mm distance from the sensor with a 1.4µm pixel pitch used in the experiment, the aperture size to make the RPD system cover the wide depth range from 0.5 m to infinity can be calculated as 0.32 mm.
Conventional flat panel displays, non-holographic displays, have a large viewing angle with scattered light. To make the exit-pupil small for a wide DOF, we need to filter the light from the display, as shown in Fig. 4. In this case, the image resolution can be limited by diffraction. For diffraction through an aperture, the angular resolution by the Rayleigh criterion can be expressed by
Fig. 4. Illustration of a conventional spatial filtering technique for increasing the DOF by reducing the exit-pupil in the RPD.
Download Full Size | PDF

Figure 3(b) shows that the spatial frequency increases in proportion to the aperture size. It means that the maximum resolution the RPD supports is determined by the size of exit-pupil as the aperture. The smaller the exit-pupil, the wider the DOF, but the resolution of the RPD is inversely reduced. In addition, the size of the exit-pupil is related to the numerical aperture (NA) of the lens relaying the image. Focusing with a high NA lens increases the FOV, and the exit-pupil decreases proportionally. In a conventional optical system, it is a property of light, commonly known as étendue, limiting a value between the FOV and exit-pupil [36]. This trade-off relationship can be expressed as follows:

Several studies have been conducted to generate multiple viewpoints to extend the eye-box. The methods utilize multiplexed HOEs in which different converging beams are recorded [24,25]. In this case, multiple viewpoints are generated simultaneously, even if a single image is relayed. Therefore, the gap distance between viewpoints should be set well to avoid interference with each other. However, it is difficult to fully cover the various eye pupil sizes that change depending on the environment. As shown in Fig. 5, the proposed method divides the HOE area to form multiple independent viewpoints. At this time, images projected from each viewpoint are continuous and do not interfere with each other in the retina. Therefore, there is no overlap or blank problem, and it has the advantage of not requiring an eye tracker for updating the images. In addition, in the case of multiplexed HOEs, the efficiency of the HOE is shared in the process of multiplexing viewpoints, but the proposed method can achieve high efficiency by using multiple HOEs.
Fig. 5. Simplified concept diagram of the proposed method. For a conventional RPD with a single viewpoint, the image could disappear due to eye rotation or movement. In contrast, the proposed method can create multiple viewpoints by segmenting the HOE and sorting the images into appropriate HOEs without overlapping.
Download Full Size | PDF

On the other hand, as a trade-off of expanding the eye-box, the FOV at a single viewpoint decreases by the number of viewpoints. It can be alleviated by selecting a gap distance smaller than the pupil to observe several viewpoints simultaneously. For example, if the gap distance is set to 2 mm, two or three viewpoints enter the pupil together in a situation where the pupil size is larger than 4 mm. For this, the projected images should be a continuous and independent area for each viewpoint. Unlike the case of increasing the eye-box by reducing the FOV while maintaining the étendue, the proposed method maintains the size of the exit-pupil in the extended eye-box, so that a wide DOF can be obtained. Also, it can effectively increase a viewing angle for eye rotation.
Figure 6(a) shows how to set the gap distance between multiple HOEs to avoid an overlap in the sensor plane. From similar triangles, the gap distance between HOEs is given by
Fig. 6. Condition that (a) projected images do not overlap at the sensor plane and that (b) a displacement occurs. The displacement between the projected images is due to changes in the focal length. The degrees of displacement according to the (c) focus distance and (d) gap distance are represented. The further the focal plane is from the hyperfocal distance, the more displacement that occurs. The maximum displacement within the DOF increases in proportion to the gap distance between viewpoints.
Download Full Size | PDF

The overall configuration of the proposed method is shown in Fig. 7. In detail, the image produced by the spatial light modulator (SLM) is relayed to the lightguide through the spatial filtering to reduce the exit-pupil. We adopted a micro–electro-mechanical system (MEMS) mirror that rapidly changes the reflection angle of incidence light. It is located between 4f relay optics, which performs a filtering process and simultaneously orients the filtered image to different areas on the lightguide. Then, the images of different areas are guided to the HOEs and focused on different viewpoints. By adopting the time-multiplexing technique with the MEMS mirror, the problem of a dead area caused by the gaps between viewpoints can be mitigated. The images projected from different viewpoints are set not to overlap each other. In summary, the proposed method can effectively solve the image problem, such as an overlap or blank caused by viewpoint replication in multi-view RPD. This method can apply to the NED without an additional optical path, and the eye-box can be improved through the time-multiplexing method, which can further increase the number of viewpoints if the refresh rate of the display is supported.
Fig. 7. Entire system of the proposed method.
Download Full Size | PDF
Fig. 8. Raytracing simulation results of image formation using LightTools. Conventional RPD using fixed exit-pupil has a narrow eye-box, which is disadvantageous for eye rotation. In the case of a dynamic eye-box that moves the exit-pupil according to the pupil position, the update of image and precise eye tracking is necessary. The proposed method can remain continuously aligned to gaze direction during eye rotation so that it can provide an effective FOV near the central area. All methods of simulation have the same area of HOE.
Download Full Size | PDF

We validated that the proposed method provides a continuous eye-box through simulation work. The previous methods using a single viewpoint were compared with the case of using multiple independent viewpoints. Due to the limitation of the raytracing simulation, it considered only the parallel rays with the ideal condition. Figure 8 shows the extended eye-box simulation under the change of gaze direction using the eye model in LightTools. The pupil size and the gap distance between the viewpoints were set to 5 mm and 2 mm, respectively, with five viewpoints. When the area of the HOE is set to 23.2mm×8.3mm and the eye-relief is 20 mm, the maximum FOV is calculated as 60∘×23∘ with the case of using one viewpoint. Compared to this, the horizontal FOV of the proposed method is 38° at the center with three viewpoints. Although the maximum FOV was reduced, it shows that the proposed method can provide the continuous image over a wide viewing angle.
Before the experiment, first, we determined several boundaries for (1) maximum thickness of the lightguide and (2) target depth range. When the eye-relief of the system is fixed to a specific value, the thicker the lightguide, the wider the guided area, and thus a wide FOV can be achieved. However, a thick lightguide is not suitable for the AR NED where portability is important, so we limited it to less than 10 mm. The limitation of the lightguide thickness can be solved by adopting an image update method, which is described in Section 5.B. Then, we choose the exit-pupil size and floating distance of the display for supporting a wide depth range. As the target depth range is set to be from 0.5 m to infinity, the display focused at the hyperfocal distance of 1 m gives the maximum DOF. From Eq. (1), an optical system is designed to make the exit-pupil coincide with 0.32 mm for the two-diopter depth range. We utilized a smartphone camera as mentioned above for the calculation. Considering the aperture size of the camera sensor, three viewpoints were used with the gap distance of 1 mm.
Figure 9 shows the simulation results using Zemax. We utilize a polarizing beam splitter (PBS) to shorten the optical path and simply implement 4f relay through polarization-selective transmission and reflection. The focal length of the relay lens is 50 mm.
Fig. 9. Layout of the optical design for expanding the eye-box in Zemax.
Download Full Size | PDF

In detail, the collimated light from an LED source is polarized along the y axis while passing through a linear polarizer. The y-polarized light transmits through the PBS, and then reflects on the SLM, which modulates the polarization state of the reflected light. This allows eight-bit grayscale while reflecting on the PBS with x-axis polarization. The modulated image is focused by the relay lens and filtered to reduce the exit-pupil size in the Fourier plane. At this point, the x-polarized light is converted into a circular polarization by placing the quarter-wave plate (QWP) with the optical axis at 45° from the x axis. Then, as the light reflected by the MEMS mirror located in the Fourier plane again undergoes retardation on the QWP, the polarization state consequently rotates 90° from the x axis to the z axis. The z-polarized light passes through PBS and enters the lightguide.
Since HOEs are made to form different viewpoints, it is necessary to adjust a light incident position in the lightguide for each viewpoint. The MEMS mirror located between a 4f relay lens can control the spatial position of the reflected image while performing the filtering function. There is a spacing between the incident images equal to the spacing in the HOEs. The exit-pupil focused through HOEs is determined by multiplying the size of the MEMS mirror by the focal length ratio of the HOE and the relay lens. A MEMS mirror with an aperture of 0.8 mm is used for the simulation, where the exit-pupil is 0.32 mm, as shown in the simulation result. All the distances between the optics are optimized to focus on the hyperfocal distance of 1 m.
The experimental setup for recording multiple independent viewpoints in the HOE is shown in Fig. 10. The HOE was recorded by adopting the conjugate reconstruction method to avoid errors caused by changes in the optical axis of the lens for each viewpoint. Instead of recording a converging wave as a signal wave, we recorded a diverging wave generated by an objective lens with a high NA that can cover all the viewing range. The diverging waves from the objective lens are recorded in the HOE on a glass sequentially by using a linear stage. For each recording, the objective lens was moved by the gap distance of the viewpoints. The areas that we did not want to record were blocked by masks precisely manufactured using a 3D printer. In the reconstruction process, the relayed images are incident in the opposite direction of the recorded reference wave and converge to each viewpoint as the conjugated signal wave. The objective lens of 0.50 NA and a 532 nm laser are used in the recording process. The distance from the object lens to the HOE is set to 20 mm.
Fig. 10. Recording setup for generating multiple viewpoints (top left), recording process for each viewpoint (bottom), rendering image of the masks (middle right), and recorded HOE after curing process (top right).
Download Full Size | PDF

The experimental setup is built to validate the design of the proposed method, as shown in Fig. 11. A cage system was used to precisely control the distance between the optics in a compact space. The system specifications are the same as those set in Section 3.C. The size of the SLM we used was 4.64mm×8.26mm with full high-definition (FHD) resolution (Selcos). Fiber-coupled LED Green (Thorlabs M530F2) was used as the light source, and it was collimated by a condenser lens of 20 mm focal length. A commercial MEMS mirror, A3I8.2-800AL (Mirrorcle Tech.), was used to adjust the incident light at high speed. Its clear aperture diameter was 0.8 mm, which limits the exit-pupil size.
Fig. 11. Photographs of experimental setup for the proposed RPD with three independent viewpoints.
Download Full Size | PDF

In the system, the SLM is updated with images of each viewpoint sequentially, and the MEMS mirror changes an angle to form the appropriate viewpoints. To make the gap distance between incident images 1 mm, the angle of the MEMS mirror is set to −3.2∘, 0°, and 3.2° for each viewpoint. Since the display has a frame rate of 60 Hz, three viewpoints are reproduced at the speed of 20 Hz. For synchronization between the MEMS mirror and the SLM, OpenGL library is used in a C/C++ programmed tool.
Fig. 12. Photographs of experimental results: (a) AR scene of the test image with different focus distances and (b) resolution target.
Download Full Size | PDF
Fig. 13. Experimental results to demonstrate the feasibility of independent viewpoints (Visualization 1). We captured the images according to the camera’s rotation angle (source image courtesy of “SimplePoly Urban,” https://www.cgtrader.com). Corresponding simulation results represent the horizontal FOV that varies with the rotation angle. It shows a continuous change for each viewpoint marked by the colors. In the case of 2 mm distance, the side images are cut off slightly while entering the lightguide (dashed white line). It is because the lightguide is designed for 1 mm gap distance instead of 2 mm.
Download Full Size | PDF

The experimental result was taken with a smartphone (F/1.5 and 4.6 mm focal length) to capture in a condition of short eye-relief. The smartphone was combined with the rotation stage to confirm the continuous change of images according to the rotation. Distance from the rotation axis was set to 12 mm equal to the eye.
Figure 12 shows the experimental results at the center viewpoint. The FOV is approximately 13° in the horizontal direction and 23° in the vertical direction. To demonstrate that the proposed system can support the desired depth range, we captured the photographs by changing the focus distance from 0.2 m to optical infinity. As presented in Fig. 12(a), it provides clear images in the desired DOF. We also evaluate the resolution of the proposed system by using a USAF 1951 target (969×1024 pixels). For comparison, we performed a diffraction-limited incoherent imaging simulation in the same condition [37]. Figure 12(b) shows the vertical line profiles of group number 0 in the experiment and simulation. As a result, the system performance was shown to be near the diffraction limit of the imaging system, which means that the resolution of the system is limited only by aperture diffraction and not by aberrations.
We demonstrate proofs of concept of multiple independent viewpoints by changing the rotation angle. The experiment was conducted in two cases where the gap distance was 1 mm and 2 mm. As shown in Fig. 13, it can provide continuous images corresponding to the eye rotation without any eye tracker. In the case of 1 mm distance, three viewpoints are projected onto the sensor plane simultaneously at the center. The diagonal FOV of the system is 44°. In the experiment with the gap distance of 2 mm, two viewpoints come in when the rotation angle is −6° and 6°, as shown in the figures. In conclusion, we confirmed that independent and continuous images are provided according to the gaze direction, which fits well with the simulation results.
The proposed method generates multiple viewpoints by guiding the images through the lightguide. In the case of off-axis guiding, the area entering the HOE may double, but off-axis aberrations such as coma and astigmatism occur [30,38]. Especially when an object plane such as flat panel display is tilted, the pixels in a light guiding direction have different focal lengths, as shown in Fig. 14. According to the Scheimpflug principle, the focal plane can be made oblique by tilting a lens during the image relaying [39]. However, it leads to other off-axis aberrations, so the image quality could be worse. This problem can be mitigated theoretically by modulating the wavefront. In many studies on holographic NEDs, the correction of off-axis aberrations of HOEs has been conducted [15,16]. Alternatively, pre-compensation can be performed easily using a holographic printer for the recording process of HOEs [40,41].
Fig. 14. (a) Illustration for the case where the incident object plane is not perpendicular to the optical axis of the HOE and (b) its experimental result.
Download Full Size | PDF

As mentioned in Section 2.D, the proposed method can increase the number of viewpoints as long as the frame rate of the display supports it. Currently, high-speed displays such as digital micromirror devices (DMDs) or ferroelectric liquid crystal on silicon (FLCoS) are capable of generating a dozen viewpoints [42,43]. However, since each viewpoint is formed in different areas, the guiding area must be widened. It means that the thickness of the lightguide should be increased in proportion to the number of viewpoints.
Here, by adopting eye tracking technology with an image update, multiple viewpoints can be achieved without increasing the lightguide thickness. We apply the idea that the number of viewpoints entering the pupil is constant. As described in Fig. 15, the images projected to the lightguide are updated according to the current gaze position. For example, at the center gaze, the images corresponding to viewpoints from four to six enter the pupil. If the gaze direction shifts sideways from five to four, a continuous change of images can be provided by updating the input image at position C from six to three. For this, it is required to precisely control the efficiency of the HOEs for each reflection so that the guided images have uniform intensity in all areas.
Fig. 15. Schematic layout of image update strategy for expanding the eye-box in the AR NED with limited lightguide thickness.
Download Full Size | PDF

In this paper, we proposed a novel method to increase the eye-box of the retinal projection type NED. Our method is based on forming multiple independent viewpoints that do not overlap each other on the retina. It has an advantage in that the image problem occurring in a multiple-viewpoint RPD does not happen even if the pupil moves, so an eye tracker is not necessary. Simulation results proved that the proposed method provides continuous and effective images with an expanded eye-box. We built a proof-of-concept system in which a FOV of 13° or more was supported at a horizontal rotation angle of ±12∘. A maximum diagonal FOV of 44° was achieved for the condition of three viewpoints. We also showed that a high-speed MEMS mirror can be utilized to form multiple viewpoints based on the time-multiplexing technique and as an aperture stop for a small exit-pupil. Experimental results demonstrated that the proposed method could support a wide depth range. The proposed concept of multiple independent viewpoints can also be applied in a holographic display that can easily correct aberrations. Also, by manufacturing the HOE with a holographic printer, it will be possible to increase the number of viewpoints as desired while compensating for the aberrations. We hope this work will be a meaningful approach for practical use of RPD in the future.
Institute for Information and Communications Technology Promotion Planning and Evaluation Grant funded by the Korean Government (MSIT) (2017-0-00787).
This work is supported by the Institute for Information and Communications Technology Promotion Grant funded by the Korea Government (MSIT) (development of vision assistant HMD and contents for the legally blind and low vision).
The authors declare no conflicts of interest.
1. B. Lee, C. Yoo, and J. Jeong, “Holographic optical elements for augmented reality systems,” Proc. SPIE 11551, 1155103 (2020). [CrossRef]  
2. S. Yano, S. Ide, T. Mitsuhashi, and H. Thwaites, “A study of visual fatigue and visual comfort for 3D HDTV/HDTV images,” Displays 23, 191–201 (2002). [CrossRef]  
3. D. M. Hoffman, A. R. Girshick, K. Akeley, and M. S. Banks, “Vergence–accommodation conflicts hinder visual performance and cause visual fatigue,” J. Vis. 8(3), 33 (2008). [CrossRef]  
4. T. Shibata, J. Kim, D. M. Hoffman, and M. S. Banks, “The zone of comfort: predicting visual discomfort with stereo displays,” J. Vis. 11(8), 11 (2011). [CrossRef]  
5. K. Akeley, S. J. Watt, A. R. Girshick, and M. S. Banks, “A stereo display prototype with multiple focal distances,” ACM Trans. Graph. 23, 804–813 (2004). [CrossRef]  
6. S. Liu and H. Hua, “Time-multiplexed dual-focal plane head-mounted display with a liquid lens,” Opt. Lett. 34, 1642–1644 (2009). [CrossRef]  
7. D. Lanman and D. Luebke, “Near-eye light field displays,” ACM Trans. Graph. 32, 220 (2013). [CrossRef]  
8. R. Narain, R. A. Albert, A. Bulbul, G. J. Ward, M. S. Banks, and J. F. O’Brien, “Optimal presentation of imagery with focus cues on multi-plane displays,” ACM Trans. Graph. 34, 59 (2015). [CrossRef]  
9. S. Lee, Y. Jo, D. Yoo, J. Cho, D. Lee, and B. Lee, “Tomographic near-eye displays,” Nat. Commun. 10, 2497 (2019). [CrossRef]  
10. S. Ravikumar, K. Akeley, and M. S. Banks, “Creating effective focus cues in multi-plane 3D displays,” Opt. Express 19, 20940–20952 (2011). [CrossRef]  
11. R. Konrad, E. A. Cooper, and G. Wetzstein, “Novel optical configurations for virtual reality: evaluating user preference and performance with focus-tunable and monovision near-eye displays,” in Proceedings of the CHI Conference on Human Factors in Computing Systems (Association for Computing Machinery, 2016), pp. 1211–1220.
12. D. Dunn, C. Tippets, K. Torell, P. Kellnhofer, K. Akşit, P. Didyk, K. Myszkowski, D. Luebke, and H. Fuchs, “Wide field of view varifocal near-eye display using see-through deformable membrane mirrors,” IEEE Trans. Vis. Comp. Graph. 23, 1322–1331 (2017). [CrossRef]  
13. K. Akşit, W. Lopes, J. Kim, P. Shirley, and D. Luebke, “Near-eye varifocal augmented reality display using see-through screens,” ACM Trans. Graph. 36, 189 (2017). [CrossRef]  
14. N. Padmanaban, R. Konrad, T. Stramer, E. A. Cooper, and G. Wetzstein, “Optimizing virtual reality for all users through gaze-contingent and adaptive focus displays,” Proc. Natl. Acad. Sci. USA 114, 2183–2188 (2017). [CrossRef]  
15. A. Maimone, A. Georgiou, and J. S. Kollin, “Holographic near-eye displays for virtual and augmented reality,” ACM Trans. Graph. 36, 85 (2017). [CrossRef]  
16. C. Jang, K. Bang, G. Li, and B. Lee, “Holographic near-eye display with expanded eye-box,” ACM Trans. Graph. 37, 195 (2018). [CrossRef]  
17. G. Westheimer, “The Maxwellian view,” Vis. Res. 6, 669–682 (1966). [CrossRef]  
18. M. von Waldkirch, P. Lukowicz, and G. Tröster, “Defocusing simulations on a retinal scanning display for quasi accommodation-free viewing,” Opt. Express 11, 3220–3233 (2003). [CrossRef]  
19. C. Jang, K. Bang, S. Moon, J. Kim, S. Lee, and B. Lee, “Retinal 3D: augmented reality near-eye display via pupil-tracked light field projection on retina,” ACM Trans. Graph. 36, 190 (2017). [CrossRef]  
20. Y. Wu, C. P. Chen, L. Mi, W. Zhang, J. Zhao, Y. Lu, W. Guo, B. Yu, Y. Li, and N. Maitlo, “Design of retinal-projection-based near-eye display with contact lens,” Opt. Express 26, 11553–11567 (2018). [CrossRef]  
21. L. Mi, C. P. Chen, Y. Lu, W. Zhang, J. Chen, and N. Maitlo, “Design of lensless retinal scanning display with diffractive optical element,” Opt. Express 27, 20493–20507 (2019). [CrossRef]  
22. R. Konrad, N. Padmanaban, K. Molner, E. A. Cooper, and G. Wetzstein, “Accommodation-invariant computational near-eye displays,” ACM Trans. Graph. 36, 88 (2017). [CrossRef]  
23. P. K. Shrestha, M. J. Pryn, J. Jia, J.-S. Chen, H. N. Fructuoso, A. Boev, Q. Zhang, and D. Chu, “Accommodation-free head mounted display with comfortable 3D perception and an enlarged eye-box,” Research 2019, 1 (2019). [CrossRef]  
24. S.-B. Kim and J.-H. Park, “Optical see-through Maxwellian near-to-eye display with an enlarged eyebox,” Opt. Lett. 43, 767–770 (2018). [CrossRef]  
25. J. Jeong, J. Lee, C. Yoo, S. Moon, B. Lee, and B. Lee, “Holographically customized optical combiner for eye-box extended near-eye display,” Opt. Express 27, 38006–38018 (2019). [CrossRef]  
26. J. Kim, Y. Jeong, M. Stengel, K. Akşit, R. Albert, B. Boudaoud, T. Greer, J. Kim, W. Lopes, and Z. Majercik, “Foveated AR: dynamically-foveated augmented reality display,” ACM Trans. Graph. 38, 99 (2019). [CrossRef]  
27. K. Ratnam, R. Konrad, D. Lanman, and M. Zannoli, “Retinal image quality in near-eye pupil-steered systems,” Opt. Express 27, 38289–38311 (2019). [CrossRef]  
28. C. Yoo, M. Chae, S. Moon, and B. Lee, “Retinal projection type lightguide-based near-eye display with switchable viewpoints,” Opt. Express 28, 3116–3135 (2020). [CrossRef]  
29. Y.-H. Lee, T. Zhan, and S.-T. Wu, “Prospects and challenges in augmented reality displays,” Virtual Real. Intell. Hardw. 1, 10–20 (2019). [CrossRef]  
30. D. Close, “Holographic optical elements,” Opt. Eng. 14, 145408 (1975). [CrossRef]  
31. S. C. Barden, J. A. Arns, and W. S. Colburn, “Volume-phase holographic gratings and their potential for astronomical applications,” Proc. SPIE 3355, 866–876 (1998). [CrossRef]  
32. J. Han, J. Liu, X. Yao, and Y. Wang, “Portable waveguide display system with a large field of view by integrating freeform elements and volume holograms,” Opt. Express 23, 3534–3549 (2015). [CrossRef]  
33. Y. Takaki and N. Fujimoto, “Flexible retinal image formation by holographic Maxwellian-view display,” Opt. Express 26, 22985–22999 (2018). [CrossRef]  
34. J. Conrad, “Depth of field in depth,” in Large Format Photography (2006), pp. 1–45.
35. M. Kalloniatis and C. Luu, “Visual acuity,” in Webvision: The Organization of the Retina and Visual System, H. Kolb, E. Fernandez, and R. Nelson, eds. (University of Utah Health Sciences Center, 1995).
36. G. Brooker, Modern Classical Optics (Oxford University, 2003), Vol. 8.
37. D. Voelz, Computational Fourier Optics: A MATLAB Tutorial (SPIE, 2011).
38. S. Lee, J. Cho, B. Lee, Y. Jo, C. Jang, D. Kim, and B. Lee, “Foveated retinal optimization for see-through near-eye multi-layer displays,” IEEE Access 6, 2170–2180 (2017). [CrossRef]  
39. Y. Jo, S. Lee, D. Yoo, S. Choi, D. Kim, and B. Lee, “Tomographic projector: large scale volumetric display with uniform viewing experiences,” ACM Trans. Graph. 38, 215 (2019). [CrossRef]  
40. S. Lee, B. Lee, J. Cho, C. Jang, J. Kim, and B. Lee, “Analysis and implementation of hologram lenses for see-through head-mounted display,” IEEE Photon. Technol. Lett. 29, 82–85 (2016). [CrossRef]  
41. J. Jeong, C.-K. Lee, B. Lee, S. Lee, S. Moon, G. Sung, H.-S. Lee, and B. Lee, “Holographically printed freeform mirror array for augmented reality near-eye display,” IEEE Photon. Technol. Lett. 32, 991–994 (2020). [CrossRef]  
42. T. Ueno and Y. Takaki, “Super multi-view near-eye display to solve vergence–accommodation conflict,” Opt. Express 26, 30703–30715 (2018). [CrossRef]  
43. B. Lee, D. Yoo, J. Jeong, S. Lee, D. Lee, and B. Lee, “Wide-angle speckleless DMD holographic display using structured illumination with temporal multiplexing,” Opt. Lett. 45, 2148–2151 (2020). [CrossRef]  




















Abstract
1. INTRODUCTION
2. PRINCIPLE
3. SIMULATION
4. EXPERIMENT
5. DISCUSSION
6. CONCLUSION
Funding
Acknowledgment
Disclosures
REFERENCES
This website uses cookies to deliver some of our products and services as well as for analytics and to provide you a more personalized experience. Click here to learn more. By continuing to use this site, you agree to our use of cookies. We've also updated our Privacy Notice. Click here to see what's new.
Christophe Martinez, Vladimir Krotov, Basile Meynard, and Daivid Fowler






The field of near-eye see-through devices has recently received significant media attention and financial investments. However, devices demonstrated to date suffer from significant practical limitations resulting from the conventional optics on which they are based. Potential manufacturers seek to surpass these limitations using novel optical schemes. In this paper, we propose such a potentially disruptive optical technology that may be used for this application. Conceptually, our optical scheme is situated at the interface of geometric incoherent refractive imaging and radiative coherent diffractive imaging. The generation of an image occurs as a result of data transmission through a two-dimensional network of optical waveguides that addresses a distribution of switchable holographic elements. The device acts as a wavefront generator, and the eye is the only optical system in which the image is formed. In the following we describe the device concept and characteristics, as well as the results of initial simulations.

© 2018 Optical Society of America under the terms of the OSA Open Access Publishing Agreement
The fields of augmented reality (AR), mixed reality (MR), and virtual reality (VR) have recently been subject to renewed interest due to the large opportunities offered by smartphone applications. As an extension of this personal, everyday device, smart glasses could allow users to interact directly with their favorite applications without looking at and touching a screen. Such smart glasses could generate new applications in relation to the surrounding analogic world (AR) or to a digitalized world, real or virtual (MR or VR).
To support and anticipate customer expectations, smart glass concepts and devices have been proposed with the help of graphic designers. Impressive marketing material has created a discrepancy between customer perception and the actual technological capabilities of these devices. A thin, light, aesthetically pleasing pair of glasses with low power consumption showing a bright and contrasted image with high resolution and a wide field of view is unfortunately still a dream.
Most development on smart glasses for AR applications is based on a conventional imaging scheme built on the following steps:

These technological functions are difficult to integrate in a compact way, and most of the devices produced for AR applications are still closer to a smart helmet than to smart glasses.
Based on this analysis we have tried to find an unconventional design that takes as a starting point an idealized image of lightweight, discreet smart glasses that has been brought to the consumer and uses alternative technologies to achieve a thin, light, and bright see-through device.
We found that the difficulties encountered in the optical system design are due to the steps 3 to 6 that concern the manipulation of the image from the display to the eye. To circumvent these difficulties, an obvious solution is to emit directly the wavefronts related to the image in front of the eye. This led us to develop a new kind of display that mixes integrated photonics and digitalized holography [1,2]: integrated photonics brings light from the light sources to the eye as a data transfer system, and holography transforms this data into wavefronts for the image to be projected on the retina.
In Section 2, we describe our display concept. In Section 3, we introduce the concept of the self-focusing effect allowing image formation on the retina. We then present the two main technological constituents of the system in Sections 4 and 5: the integrated photonics light distribution and the digitalized holography, respectively. Sections 6 and 7 describe estimates of the imaging properties and device power consumption, respectively.
Image formation into the eye is described in Fig. 1(a): our eye transforms a wavefront coming from the source image and focuses it on the retina. The coordinates of this image point on the retina are given by the angle of the wavefront
→
kp
that reaches the cornea.
Fig. 1. Imaging into the eye: (a) imaging of a point at infinity, (b) near-eye display, (c) near-eye display with a single optical system, (d) near-eye display with multiple lens/pinholes aperture, (e) near-eye display based on a phased array, and (f) near to eye display according to the CEA concept.
Download Full Size | PDF
A conventional display emits light at the level of the image pixels. In a near-eye configuration the spherical wavefront
→
ks
generated by each pixel has a curvature that the eye is not able to correct [Fig. 1(b)]. Even if a wavefront has the correct angular orientation, the resulting image point on the retina is blurred. In order to form the image, an optical system is introduced that reduces the curvature of the spherical wavefront and produces the planar wavefront
→
kp
with the correct angular coordinates [Fig. 1(c)].
The use of an optical system increases the volume of the optical device and leads to severe constrains in terms of field of view and eye box. Lens-free near-eye displays are possible alternatives that allow generation of planar wavefronts without the use of a single-axis optical system. An example proposed by Maimone et al. uses an integral imaging concept [3]. An array of lenses or pinholes, creates a distribution of elementary planar wavefronts with the given angular coordinates
→
k
i
p
[Fig. 1(d)]. The size of the optical system is decreased due to the reduction in size of the optics. The lens or pinhole array plays the role of a pupil expander and allows the eye box constraint to be relaxed. Another solution proposed by Sun et al. consists of a direct emission of a complex holographic wavefront
→
kh
. The device uses a phased array to generate the phase distribution of the image to be formed on the retina [Fig. 1(e)] [4].
These two solutions rely on a segmentation of the wavefront. The first case uses an incoherent wavefront angular distribution built on a conventional display. The second case uses a coherent wavefront phase distribution built on an unconventional integrated photonics device.
We propose an intermediate solution, situated at the interface of geometric incoherent refractive imaging and radiative coherent diffractive imaging. The wavefront distribution is built in a coherent way by the use of integrated photonics and holography. However, the resulting wavefront is not considered as a complex phase function calculated from the Fourier transform of the image. It is rather built as an incoherent geometrical combination of elementary coherent wavefronts resulting from the image angular point coordinates.
The principle of our concept is described in Fig. 1(f): the display emits a distribution of spherical wavefronts with a given angular orientation
→
k
i
s
. These wavefronts are mutually coherent so that a planar wavefront is generated as described in the Huygens–Fresnel principle. The eye can then focus the generated planar wavefront on the retina with the specified angular coordinate. As the only optical system needed to form the image on the retina is the eye itself, we can imagine a highly integrated architecture as a basis of the display device.
An artist’s view of our device is given in Fig. 2. Each emitting point that generates an elementary spherical wavefront is addressed by a waveguide that brings the image data to the outcoupling region. At this location a switchable outcoupling grating extracts the propagated light in the vicinity of a holographic element. This reflective elementary hologram can be considered as an orientated Bragg grating that defines the spherical wavefront angular orientation
→
k
i,j
s
.
Fig. 2. Artist’s view of the see-through display device with a zoom on one emissive point element.
Download Full Size | PDF
The surface of the glass is covered by a complex waveguide design that addresses the emissive points distribution (EPD). A distribution of outcoupling electrodes allows the activation/deactivation of the wavefront emission in order to refresh the image formation on the retina. Light that forms the image is generated by an amplitude modulated laser array that is coupled to the waveguide distribution.
If the device is made in transparent materials, with small refractive index variation, we can expect an overall transparency that could allow see-through applications.
The operating principle of the device is described in Fig. 3. In this simple case we show the rendering of a basic image of 3×3 pixels. In Fig. 3(a) we have an exploded view of the device: (1) the laser array produces the light used to form the image; (2) the routing waveguides direct the light to the waveguide output distribution; (3) the waveguide output distribution addresses the EPD; (4) the outcoupling grating layer extracts light from guided to free-space optics; (5) the electrode layer enables light extraction at specified locations; and (6) the hologram layer defines light direction and fixes the EPD phase coherence.
Fig. 3. Principle of the device operation. (a) Exploded view of the device concept, (b) description of the imaging process, and (c) the three steps necessary to project the 3×3 pixels image from an array of three laser sources and three electrodes.
Download Full Size | PDF
Our simple illustration shows the letter “T” scanning image formation in three steps [Figs. 3(b) and 3(c)]. In step 1 we emit three angular directions corresponding to three image pixels. The three lasers are emitting coherent light at three different output power levels at a green wavelength. In the drawing, each green laser is associated with a specific color (red, blue, yellow). A first set of electrodes (gray) is activated, and each light beam guided from the laser is emitted on the glass surface on an EPD of the corresponding color. We have represented only one emitted beam for each laser but each laser, i.e., each angular direction, is associated with a set of emissive holographic points in red, blue, or yellow. At this step the display projects on the retina three points corresponding to the three first angular directions [Fig. 3(b)].
In step 2 a second set of three image pixels is projected. Another set of amplitudes is given to the three laser sources, and another set of electrodes (pink) is activated in order to address the corresponding angular holographic EPD.
The same process is repeated in step 3 to project the last three image pixels. Vision persistence is used to recover the whole image [Fig. 3(b)].
This ambitious concept is based on technological steps that have to be investigated and demonstrated theoretically and practically. One of the first issues concerns the ability to form an image in relation to the Huygens–Fresnel principle. We describe this image-forming method as the self-focusing effect.
The self-focusing effect has already been introduced and demonstrated experimentally by Hong et al. in the field of optical data storage [5]. The authors have shown the ability to focus a laser from a combination of phase-adjusted laser beams. More recently this concept has found new applications in LIDAR devices [6].
In our case the self-focusing effect can be described considering both a Gaussian beam model and the multiple interferences phenomenon. Figure 4(a) shows the basic concept of multiple beams focusing into the eye. Our display is located at a distance Z1 from the eye. The eye is described as a thin lens of focal length f. The retina is located at a distance f from the surface of the eye. The display emits a collection of Gaussian beams from the EPD Mu,v. The number of emissive points is limited by the entrance pupil Π with diameter Dp, as shown in Fig. 4(b).
Fig. 4. (a) Principle of the Gaussian beams interferences from the display plane to the retina plane, (b) EPD on the display plane, and (c) the retinal plane.
Download Full Size | PDF
The particularity of our concept is that each emitted beam of a given emissive point Mu,v propagates according to the same wave vector
→
ki,j
. Geometrically, after passing through the eye lens, all these parallel beams converge to the same point Pi,j, located in the focal plane of the eye lens.
We take the approximation of an eye that can be described as a thin lens. According to the Gaussian beam formulation the waist w1 located at Mu,v is imaged by the eye at a distance Z2 given by

Equation (1) shows that if the display is positioned close to the object focal plane of the eye (σ=0), the distance Z2 is close to the focal length. The wavefronts converging on the point Pi,j are close to the waist location and can be considered as plane waves with wave vector
→
ku,v
. The behavior of the beam superposition at the point Pi,j can then be described as multiple interfering planar waves. We describe the field of the planar wave as follows:

The interference energy function I(
→
r
) is given by the sum of the beams passing through the eye lens pupil aperture Π as

As a demonstration and for a better understanding of the phenomena, we pursue our theoretical analysis with a 1D emissive distribution in the plane (y, z), as shown in Fig. 5. The wave vector
→
ku
is expressed in relation to the propagation angle αu as

Fig. 5. Simple 2D geometrical representation for the calculation of the interphase function.
Download Full Size | PDF
The interphase function at the focal plane z=f becomes

Equation (11) also shows that if the emissive points are distributed in a square periodic grid of period Λ1, that is, if yu=u×Λ1, then other maximums of energy occur at the coordinates

Equation (12) is the expression of the order of diffraction of the periodic EPD structure. It shows that the self-focusing effect can effectively focalize the energy at the targeted location but also at periodic resonances. The self-focusing effect could lead to a sharp image on the retina but, as the image is duplicated, the periodic EPD forbids effective imaging. To avoid this resonance effect one solution is to introduce randomness in the EPD, as shown in Fig. 4(b).
We have simulated the self-focusing effect using the double formalism of the Gaussian beam and multiple beam interference. Equation (3) is used in an iterative way to sum the contribution of an EPD.
We consider a Gaussian beam propagation to describe the amplitude of the beam from the display to the retina. The amplitude of the field in the plane of the display is given in a first approximation by a Gaussian function of waist w1:

The beam propagates from the display to the eye and forms an image of waist w2 close to the retina:

The simulated intensity function Is is calculated from the summation of the planar beam given in Eq. (3) with a Gaussian beam intensity weighting:

We evaluate the interference figure as a function of the period of the EPD and compare the periodic distribution with a semirandom distribution, defined by a “randomly periodic equation”:

Figure 6(a) shows the case Λ1=400 μm: only one point of the EPD belongs to the eye lens aperture (red dot on the left figure). The resulting intensity function on the retina shows only the Gaussian beam contribution. The Gaussian beam of waist given by Eq. (14) represents the blurred signal on the retina.
Fig. 6. Results of the self-focusing intensity signal for various EPD configurations. Figures on the left show the EPD in periodic (red dots) and randomly periodic (green dots) cases. Figures in the center and on the right give the intensity distributions for periodic and randomly periodic EPDs. (a) Λ1=400 μm, (b) Λ1=200 μm, (c) Λ1=100 μm, and (d) Λ1=50 μm.
Download Full Size | PDF
In Fig. 6(b) the EPD period is decreased to 200 μm. We compare the periodic distribution with five points on the EPD (red dots) and the randomly periodic distribution that give four points on the EPD (green dots). The resulting intensity function of both cases shows the periodic self-focusing effect (on the center) and the random self-focusing effect (on the right). Figures 6(c) and 6(d) show the case with Λ1=100 μm and Λ1=50 μm.
The choice of the parameter Dp on Fig. 6 allows a good view of the phenomenon as the waist w1 and w2 are visible on the same graphic. In the case of periodic EPD, the interfering beams generate a focus that is replicated on a period Λ2 given by Eq. (12). When randomness is introduce in the EPD, the diffraction orders vanish such that a speckle pattern is formed allowing a single focused spot of the image to be created. The size w1 of the emissive zone fixes the size w2 of the speckle distribution, the size Dp of the eye lens aperture Π fixes the radius of the focus δw that tends to the diffraction limit, given by

Fig. 7. Comparison between intensity cross section of the self-focusing signal in the random case of Fig. 6(d) (green curve) and the Airy function (dashed blue curve).
Download Full Size | PDF
We evaluate the efficiency of the self-focusing effect by measuring the signal-to-noise ratio (SNR) with parameters more consistent with our concept:

The SNR is calculated according to the equation

Fig. 8. Results of SNR simulations for six randomly periodic sequences of EPDs (green dots) and for a periodic EPD (red dots, extended by 10 in the abscissa axis). The minimum, mean, and maximum curves of the random sequences are presented by the solid line. The Airy diffraction-limited SNR is also given for comparison.
Download Full Size | PDF
As expected, the results show that randomness greatly improves the SNR and that the choice of the EPD has a strong impact on the imaging process. For a given period Λ1, the EPD random sequence choice can modify the SNR over several orders of magnitude.
As shown in Fig. 8, we need a small EPD period to increase the SNR and the image rendering. However, for a given emissive point size, the number of available EPDs decreases with the EPD period. As the number of available EPDs is directly related to the number of pixels of the projected image, we have to manage a compromise between the quality and the resolution of the image. This compromise differs from the standard space–bandwidth constraint (for a given display size, sharpness and resolution increase together) [8] and underlines the unconventional aspect of our approach: the increase of the image resolution reduces the image rendering.
The SNR is a first step in the characterization of the self-focusing process. The effective impact of the EPD choice on the image quality, related to resolution sharpness or contrast constraints, is currently under investigation and will be published soon.
The choice of the EPD configuration is also a research topic strongly related to technological constraints. The part of randomness introduced in the EPD must be consistent with the technological solutions used to bring light to the surface of the display. In particular, it must take into account the limitations induced by the waveguide design.
The technological principle considered to bring light to the EPD is described in Fig. 9. A waveguide distribution array guides the energy that fixes the amplitude of the emitted signal. The location ru,v of the emitted beam is determined by the intersection between the waveguide Wu and the outcoupling activation electrode Ev. These intersections between the activated electrodes and waveguides define the EPDs. As shown in Fig. 3, various EPDs are activated at the same time as different waveguides are addressed by different lasers.
Fig. 9. Principle of signal extraction from the guided mode to free-space propagation.
Download Full Size | PDF
Figure 9 shows two waveguides Wu and Wu+1 addressed by two lasers q and q′. The electrode Ev extracts the two signals toward the holographic elements (hoels) hu,v and hu+1,v. The two hoels, which belong to the EPD related to the two image point angular coordinates, reflect the signal in the given angular directions
→
ki,j
and
→
ki′,j′
.
The device parameters are given in Figs. 10 and 11. Waveguides are designed to propagate a single mode in the visible range, at the wavelength of the hologram maximum efficiency (around 532 nm for the polymer holographic material considered here). One promising technology for the manufacturing of waveguides operational in the visible range is silicon nitride [9]. Typical value parameters for the waveguide thickness eg and width wg at these wavelengths are 200 nm and 300 nm. The distance dg between the waveguides is chosen to limit the coupling to neighboring waveguides. A typical value of 1.5 μm can be considered. The SiN waveguides can be manufactured on transparent glass and covered by a SiO2 cladding of thickness ec.
Fig. 10. Cross section of the device showing the wire waveguide and hoel design.
Download Full Size | PDF
Fig. 11. (a) Lateral section of the device showing the coupling between the waveguide and the hoel. (b) The same figure during the recording process.
Download Full Size | PDF
The outcoupling grating is etched in the cladding above the waveguide. A typical grating period for a wavelength of 532 nm is around 400 nm.
The design of the waveguide including coupling/propagation constraints due to the particular random EPD choice is currently investigated theoretically and experimentally.
The electrode that activates the outcoupling grating can be made of a liquid crystal layer as proposed by Buss et al. [10]. The width we of the electrode fixes the length of the outcoupling grating [Fig. 11(a)]. Typical lengths could be of the order of a few microns. The electrodes are separated by a distance de. The total number of emissive points NEP for the whole EPD family is given by the parameters de and dg in relation to the eye pupil aperture

The principle of light coupling from the waveguide to the hoel has been described in Fig. 9. Here, we present in Fig. 11 the principle of the phase adjustment that allows the efficiency of the interferences produced by the directional hoels.
Figures 11(a) shows a lateral section of the device. A guided wave is extracted in two locations and is reflected by two hoels belonging to the same EPD. The length Lh of the hoel is related to the size of the electrode but is not limited by the interelectrode distance de. As for the width of the hologram, its length can exceed de so that neighboring hoels can overlap. We have chosen in our simulation a hoel radius that defines a waist w1=2 μm of the emitted beam.
As presented in Section 3.A, each EPD must be phase adjusted in order to self-focalize on a specific location of the retina plane. The phase shift δϕ between the extracted beams described in Fig. 11(a) is related to the optical path shift and to the grating distribution. It can hardly be controlled by a nanoscale resolution mask design over the whole device surface. Instead, phase adjustment is guaranteed by the intrinsic nanoscale resolution of the 3D hoel recording process.
The recording process is described in Fig. 11(b). The guided laser light plays the role of the reference beam, and a free-space beam coming from the same laser is used as the object beam. This beam is segmented in a multitude of elementary beams coming from a given angular direction. Activation of the outcoupling electrodes allows the creation of the interference pattern between the reference and the object beams that is recorded by the hoels for a given EPD.
When the reference beam is coupled from the waveguide, the conjugate object beam is generated in a reflective mode, as shown in Fig. 11(a). The phase adjustment is automatically recorded from the original object beam.
In order to record the hoel distribution corresponding to a given EPD, a specific recording setup has to be built. Figure 12 shows the basic concept of the setup. An optical fiber is used to split a laser beam into a reference and an object beam. The object beam is collimated in a two lens optical system from a point source given by the first fiber extremity. First lens L1 makes the image of the fiber extremity on the object focal plane of the second lens L2. The position of this image fixes the angular direction
→
ki,j
of the collimated beam. An aperture mask corresponding to the EPD is located on the image focal point of the first lens and is imaged on the hologram layer by the second lens L2. The object beam that impacts the hologram layer is collimated and phase adjusted in relation to the optical fiber position and is segmented in a collection of beam spots defined by the aperture mask. The image of the aperture mask is aligned with the EPD that is activated by the electrodes and by the optical coupling of the second optical fiber in a selected waveguide distribution (shown in the inset of Fig. 12).
Fig. 12. Recording setup for the hoel distribution manufacturing.
Download Full Size | PDF
The recording setup shown in Figs. 11(b) and 12 uses a collimated planar object wavefront. It is designed to form an image at infinity for each eye. The modification of the fiber longitudinal position in the recording setup allows the device to form an image at a fixed, given viewing distance by modifying the curvature of the object wavefront. In this case, the management of a symmetric off-axis pixels distribution for each eye should be used to reduce the vergence accomodation conflict (VAC) that usually limits conventional smart glasses approaches [12]. Our concept allows viewing an image in different plane locations with limited VAC. However, the manufacturing process fixes the plane location for a given display device.
Numerous questions remain regarding the technological process for the realization of the holographic device. Uncertainties around the recording duration, the material behavior, and the possible replication methods have to be investigated to validate a potential commercial interest to the concept. However, the impressive achievements of commercially feasible terabit-scale hologram storage and hologram printers imply a technical maturity that should be applicable to our approach [13,14].
We are currently evaluating the hoel recording process and have presented some initial design considerations [15]. A collaboration with a polymeric holographic material supplier has been initiated and should allow us to soon record the first hoel distribution in order to validate the self-focusing effect in an image-forming process.
The sharpness of an image formed by an optical system is generally characterized by its modulation transfer function (MTF). This function gives the efficiency of the system for the rendering of a spatial frequency with a given contrast. The MTF can be calculated from the Fourier transform of the point spread function (PSF) that is the impulse intensity distribution.
Figure 7 shows the PSF of an optimal self-focusing effect (green curve). The calculation takes into account a perfect lens as the optical imaging system and the signal in its central part is very close to the theoretical diffraction limit. A more realistic approach needs to take into account the specific optical characteristics of the human eye and this poses some specific issues due to the human vision process.
The eye is a complex optical system that does not follow the diffraction theory. Unlike Eq. (17), aberrations in the pupil periphery degrade the MTF as the eye pupil diameter increases [16]. On the other hand, it has been shown that the coherent laser interfering imaging process can alleviate the pupil peripheral aberration distortion and improve the PSF [17]. The question of the effective sharpness of the self-focusing effect in the eye is an open question and needs to be studied with modern eye models and physiologic experiments in a coherent imaging process.
The measurement of the ANSI contrast is also a mean to evaluate the efficiency of an imaging system. It consists in forming a checkerboard image and measuring the ratio of intensity between the dark and white cells. In the case of self-focusing imaging, the PSF can be divided in two parts: a thin central spot and a large noisy speckle contribution, as shown in Fig. 7. This leads us to introduce a double Gaussian model in which the thin central Gaussian spot contributes to the imaging process and the large Gaussian noise reduces the overall contrast [18]. Improving the contrast requires limiting the impact of the large Gaussian contribution. This is done by an optimal EPD design that improves the energy ratio between the two Gaussian contributions and by limiting the number of pixels required to form an image (typically by the use of nonadjacent pixel distribution, as shown below). Characterization of our system in terms of ANSI contrast simulation will be described in an upcoming paper.
As we have mentioned in Section 3, the rendering of the image is related to the number of emissive points nep of a given EPD. This number depends on the eye pupil aperture size and on the choice of the EPD function, in particular, the EPD period Λ1:

We can define the following consistent parameters:

This gives a total number of pixels Npix=93,750 that corresponds to a conventional image resolution of about 300×300 pixels.
We have simulated in Fig. 13(a) a retinal projection on the basis of an image of 300×300 pixels projected on a 15°×15° field of view (FOV). This artistic view highlights the differences between our retinal projection concept and a conventional display. The image is formed by separated luminous dots rather than by adjacent pixels. The angular distance that separates the dots is not necessarily uniform and can be adapted to the content for a given region of the FOV. In the example the text is projected on the retina with an angular distance between the dots of 3 arcmin and the value is increased to 4.5 arcmin for the GPS pictogram.
Fig. 13. (a) Simulation of an image projection according to our concept. The image on top shows an external view that covers a 100° wide FOV. The red square presents a projection zone of 15°×15° with a resolution of 300×300 pixels. Over is a zoom on a section of the projected image, with an angular resolution of the text and the GPS pictogram of 3 arcmin and 4.5 arcmin, respectively, (b) detail of the text projected in a dots distribution, (c) detail of the same text resolution projected in an adjacent pixel distribution.
Download Full Size | PDF
Equation (22) leads to low pixel number values. However, even if the resolution of the available image is low, the total number of pixels can be optimized by selecting specific regions in the FOV. This possibility underlines once again the unconventional approach of the concept: the perceived FOV, traditionally given by the product of the resolution with the angular pixel increment, can be increased here for a given constant total number of pixels. The dynamic image addressing for a specific region of the FOV and with a specific resolution is, however, fixed for a given device. Each EPD can be modulated in power emission but not in angular reference.
In terms of image rendering, the dotted aspect of the projected image can modify the perceived resolution and improve the result in comparison to a conventional display due to half-toning visual effects. As an illustration, we compare the same text coded with a resolution of 86×9 pixels for an unconventional dots pattern display [Fig. 13(b)] and for a conventional adjacent pixels display [Fig. 13(c)].
Imaging properties are currently investigated in an extensive study to evaluate the impact of the speckle noise. This theoretical study will be shown by experimental evaluations incorporating visual tests.
To conclude the technological review of our concept we focus on power considerations to check if the device is consistent with the objective of near-eye integration.
We target the projection of a full bright image on a circular FOV of 15°. The image is characterized by a brightness B. We calculate the power required in relation to the etendue of the eye [Fig. 14]:

Fig. 14. Description of the etendue and eye box distribution in the intraretinal projection process.
Download Full Size | PDF
The etendue of the beam seen from the eye is equal to the etendue of the eye seen from the beam. The emitting surface characterized by a diameter De is given by the relation

The total amount of optical power emitted from the display is given by

We note this overall device efficiency ηd, which leads to the optical power emitted by the lasers:

Another power consideration is that of eye safety. The image is projected on the retina in a scanning mode (Fig. 3) and must not lead to hazard for the retina. Laser safety is based on the calculation of the maximum permissible exposure (MPE) on the cornea. For the case of a collimated laser beam, a blinking reflex of 0.25 s, and a pupil diameter of 7 mm, MPE is about 6.4 J/m2 [20] and corresponds to a laser power limit of about 1 mW.
In our display the exposure of the cornea for one pixel during 0.25 s is given by

The device seems to present no hazard for the eye. However, the case of the blinking reflex can be discussed and long-term laser exposure will have to be studied in more detail.
We present a complete theoretical overview of an unconventional imaging concept that could allow the development of a near-eye integrated transparent display. We describe the concept of the self-focusing effect that could allow image formation by retinal projection in a lens-free device configuration. The concept is simulated, and first results on evaluation characteristics such as the focus SNR give first insight on the device feature. The waveguide design and hoel concept are introduced, and we give some first perspectives on image rendering, device manufacturability, and power consumption.
Initial limitations are identified in terms of image rendering and commercial implementation. Image resolution is constrained by the waveguide integration and self-focusing efficiency. The use of holographic elements limits the projection to a monochromatic image, and a fast holographic recording process has yet to be demonstrated. These limitations can be balanced by the new opportunities opened by the unconventional imaging approach. In particular, the ability to adapt locally the resolution of the image inside a discontinuous field of view can open interesting applications.
More generally, this research can be seen as a fundamental reflection on the new opportunities for retinal projection that are opened by recent technological achievements in integrated photonics and holography. As our laboratory is strongly involved in conventional microdisplay design and manufacturing for AR/VR/MR applications [21], such investigations may anticipate potential technological evolutions.
We thank Pr. Haeberle from Laboratoire MIPS of Université de Haute-Alsace for fruitful discussions on diffraction and holographic issues.
1. C. Martinez, “Image projection device,” U.S. patent 2015/0370073 A1 (December 24, 2015).
2. C. Martinez, V. Krotov, D. Fowler, and O. Haeberle, “Lens-free near-eye intraocular projection display, concept and first evaluation,” in Imaging and Applied Optics, OSA Technical Digest (Optical Society of America, 2016), paper CW1C.5.
3. A. Maimone, D. Lanman, K. Rathinavel, K. Keller, D. Luebke, and H. Fuchs, “Pinlight displays: wide field of view augmented reality eyeglasses using defocused point light sources,” ACM Trans. Graph. 33, 89 (2014). [CrossRef]  
4. J. Sun, E. Timurdogan, A. Yaacobi, E. Shah Hosseini, and M. R. Watts, “Large-scale nanophotonic phased array,” Nature 493, 195–199 (2013). [CrossRef]  
5. S. S. Hong, B. K. Horn, D. M. Freeman, and M. S. Mermelstein, “Lensless focusing with subwavelength resolution by direct synthesis of the angular spectrum,” Appl. Phys. Lett. 88, 261107 (2006). [CrossRef]  
6. M. Heck, “Highly integrated optical phased arrays: photonic integrated circuits for optical beam shaping and beam steering,” Nanophotonics 6, 93–107 (2017). [CrossRef]  
7. M. Born and E. Wolf, Principle of Optics, 7th ed. (Cambridge University, 1999).
8. A. W. Lohmann, R. G. Dorsch, D. Mendlovic, Z. Zalevsky, and C. Ferreira, “Space-bandwidth product of optical signals and systems,” J. Opt. Soc. Am. A 13, 470–473 (1996). [CrossRef]  
9. A. Z. Subramanian, P. Neutens, A. Dhakal, R. Jansen, T. Claes, X. Rottenberg, F. Peyskens, S. Selvaraja, P. Helin, B. Du Bois, K. Leyssens, S. Severi, P. Deshpande, R. Baets, and P. Van Dorpe, “Low-loss singlemode PECVD silicon nitride photonic wire waveguides for 532–900 nm wavelength window fabricated within a CMOS pilot line,” IEEE Photon. J. 5, 2202809 (2013). [CrossRef]  
10. T. Buß, C. L. C. Smith, and A. Kristensen, “Electrically modulated transparent liquid crystal-optical grating projection,” Opt. Express 21, 1820–1829 (2013). [CrossRef]  
11. G. Barbastathis, M. Levene, and D. Psaltis, “Shift multiplexing with spherical reference waves,” Appl. Opt. 35, 2403–2417 (1996). [CrossRef]  
12. G. Wetzstein, “Light field, focus-tunable, and monovision near-eye displays,” SID Symp. Dig. Tech. Pap. 47, 358–360 (2016). [CrossRef]  
13. L. Hesselink, S. S. Orlov, and M. C. Bashaw, “Holographic data storage systems,” Proc. IEEE 92, 1231–1280 (2004). [CrossRef]  
14. H. Bjelkhagen and D. Brotherton-Ratcliffe, Ultra-Realistic Imaging, Advanced Techniques in Analogue and Digital Colour Holography (CRC Press, 2013).
15. C. Martinez, V. Krotov, and D. Fowler, “Holographic recording setup for integrated see-through near-eye display evaluation,” in Imaging and Applied Optics, OSA Technical Digest (Optical Society of America, 2017), paper JTu5A.36.
16. F. W. Campbell and R. W. Gubisch, “Optical quality of the human eye,” J. Physiol. 186, 558–578 (1966). [CrossRef]  
17. B. A. Wandell, Foundations of Vision (Sinaur Associates, 1995), p. 54.
18. V. Krotov, C. Martinez, and O. Haeberlé, “Imaging performance analysis of a lens-free near to eye display,” in Imaging and Applied Optics, OSA Technical Digest (Optical Society of America, 2017), paper JTu5A.5.
19. R. LiKamWa, Z. Wang, A. Carroll, F. X. Lin, and L. Zhong, “Draining our glass: an energy and heat characterization of Google Glass,” in 5th Asia-Pacific Workshop on Systems (APSYS) (2014).
20. F. C. Delori, R. H. Webb, and D. H. Sliney, “Maximum permissible exposures for ocular safety (ANSI 2000), with emphasis on ophthalmic devices,” J. Opt. Soc. Am. A 24, 1250–1265 (2007). [CrossRef]  
21. F. Templier, L. Dupré, S. Tirano, M. Marra, V. Verney, F. Olivier, B. Aventurier, D. Sarrasin, F. Marion, T. Catelain, F. Berger, L. Mathieu, B. Dupont, and P. Gamarra, “75-1: Invited paper: GaN-based emissive microdisplays: a very promising technology for compact, ultra-high brightness display systems,” SID Symp. Dig. Tech. Pap. 47, 1013–1016 (2016). [CrossRef]  



















Abstract
1. INTRODUCTION
2. GENERAL CONCEPT
3. SELF-FOCUSING EFFECT
4. WAVEGUIDE DISTRIBUTION ARRAY
5. DIRECTIONAL HOLOGRAPHIC ELEMENTS
6. IMAGING PROPERTIES
7. POWER CONSIDERATIONS
8. CONCLUSION
Acknowledgment
REFERENCES
This website uses cookies to deliver some of our products and services as well as for analytics and to provide you a more personalized experience. Click here to learn more. By continuing to use this site, you agree to our use of cookies. We've also updated our Privacy Notice. Click here to see what's new.
Chanhyung Yoo, Minseok Chae, Seokil Moon, and Byoungho Lee




We present a retinal-projection-based near-eye display with switchable multiple viewpoints by polarization-multiplexing. Active switching of viewpoints is provided by the polarization grating, multiplexed holographic optical elements and polarization-dependent eyepiece lens that can generate one of the dual-divided focus groups according to the pupil position. The lightguide-combined optical devices have a potential to enable a wide field of view (FOV) and short eye relief with compact form factor. Our proposed system can support a pupil movement with an extended eyebox and mitigate image problem caused by duplicated viewpoints. We discuss the optical design for guiding system and demonstrate that proof-of-concept system provides all-in-focus images with 37 degrees FOV and 16 mm eyebox in horizontal direction.

© 2020 Optical Society of America under the terms of the OSA Open Access Publishing Agreement
An augmented reality (AR) is a promising technology of superimposing additional virtual images on real objects, which is expected to change life style more conveniently and productively. Near-eye displays (NEDs) are essential devices for implementing AR technique in various applications. Except for a video-type see-through AR using camera images, typical AR NEDs are optically see-through, providing a real scene without distortion. They require a core optical component that can simultaneously deliver virtual images with high optical transparency for the real scene, which is called an image combiner. Various image combiners have been proposed to offer immersive AR experiences, and each method has some advantages over the other and weakness as well in terms of visual performance and a form factor [1,2]. For a long time wearing in daily life, AR NED systems should be designed with a compact and lightweight form factor and moderate image quality.
In addition to the factors mentioned above, focus cues are one of the important factors to accurately provide depth perception of three-dimensional images. In display systems not supporting focus cues, vergence distance due to binocular disparity does not coincide with accommodation distance where a fixed focal plane of virtual images is physically located. This physiological discrepancy between vergence and accommodation is called the vergence-accommodation conflict (VAC), which causes visual fatigue or nausea, in stereoscopic viewing environment [3–5]. Moreover, the VAC problems can result in blurred virtual images depending on the distance of the real object being watched, which may hinder vivid AR experiences. To alleviate discomfort by the VAC, various solutions enabling focus cues have been proposed and are summarized in [6–11]. Most of them can be summarized into two representative methods: stereoscopic displays with add-on and holographic displays.
First, stereoscopic displays with focus cues are enabled by using focus tunable optical systems, multiple displays or freeform optics [12–24]. They provide multiple or changeable focal planes of virtual images with time- or space-multiplexing. Although these techniques can mitigate VAC problems by visualizing 3D images with a wide-range zone of comfort [3,25–27], additional optical elements and ultra-fast-driven display module are required for real time operating; which inevitably makes the optical system bulky. Second, holographic displays provide true focal cues by reproducing same wavefront as real objects [28–32]. However, holographic techniques have still low-performance images with speckle issues and a heavy computational load for calculating precise wavefront [33].
Retinal projection displays (RPDs), which are also known as Maxwellian-view displays, are based on focus-free system [34–36]. The RPDs provide a wide range of acceptable sharp focus regardless of accommodation response of the human eye, and can be implemented with a relatively simple configuration and a small form factor. As shown in Fig. 1(a), by focusing the light of display images on the pupil of the eye, a sharp image can be directly projected onto the retina irrespective of the focal power of the crystalline lens. Focus-invariant systems can alleviate VAC problems compared to displays with a single focal plane [37]. Despite these advantages, RPDs have an intrinsic limitation that they provide a small eyebox around a focal spot. If the entrance pupil of the eye is out of the eyebox due to rotation of the eyeball and misalignment of the eye position, virtual images are blocked partially or entirely. This limited eyebox is a major obstacle for the commercialization of the retinal projection-type NED. To enlarge the size of eyebox, several solutions have been proposed as follows.
Fig. 1. (a) Simplified schematic diagram of the retinal projection display, (b) Illustration of the double image problem with a narrow viewpoint spacing and (c) blank image problem with wide viewpoint spacing in the RPD with multiple viewpoints.
Download Full Size | PDF
Jang et al. proposed a pupil-tracked RPD system using a holographic optical element (HOE) and a steering mirror [38]. This system provides a dynamic eyebox, which can be realized by laterally shifting a focal point by the HOE combiner to track the movement of the pupil. Kim et al. suggested a dynamically moving focal point by using a mechanically moving HOE module [39]. This approach is quite similar to Jang’s method, but can provide a wider eyebox with a shorter eye relief. Both of the above mentioned methods use mechanical movement of the optical element, which causes an increase in form factor or weight due to additional moving parts. Furthermore, if the moving speed is not fast enough, image sticking or lag may occur in real time. A steerable eyebox is realized by using an array of light emitting diodes (LEDs) synchronized with pupil tracker by Hedili et al. [40]. This method can switch fixed focal spots with low motion-to-photon latency. Kim et al. showed a lightguide-type RPD with an enlarged eyebox by the multiplexed HOE [41]. Jeong et al. suggested a customized HOE combiner using a holographic printer [42]. This method has an advantage that mechanical control is not required due to multiple and concurrent focal spots. However, in an enlarged eyebox with always-on focal spots, double images or a blank can be displayed depending on the spacing between viewpoints, as shown in Figs. 1(b) and 1(c).
We proposed the lightguide-type RPD enabling switchable viewpoints with polarization- multiplexing. This novel method is implemented by polarization grating and multiplexed HOEs, which selectively change the diffraction angle of the output beam depending on the polarization state of the input beam. The proposed configuration divides the multiple focal spots into two groups, so that it can switch between each other. By employing the active control of viewpoints, image problems can be mitigated in RPD systems with multi-viewpoints. In the following chapters, a detailed operating principle and system structure are discussed and a proof-of-concept system is implemented to verify the feasibility of the proposed scheme.
The goal of the proposed method is to provide an extended eyebox that can resolve the blank or double image problem in the RPD system. We designed to switch between two viewpoint groups, which are alternately located in Sagittal plane, according to the pupil position. Polarization-dependent devices and multiplexed HOEs are employed to control the viewpoints switching depending on the polarization state.
The key element for implementing the switchable viewpoints is a polarization grating that controls the incidence angle on the in-coupler depending on the polarization state of the input beam. The transmission-type polarization grating diffracts an incidence beam into two transmitted beams with opposite circular polarization states [43]. The diffraction angle depends on the design parameter such as the grating period and axis orientation, and is symmetrical across the axis of the incident beam. In conventional polarization gratings, only the diffracted beam with the 1st and the -1st diffraction orders is highly efficient; e.g. the zeroth or high order beams have very low efficiency. These polarization gratings are fabricated by an anisotropic medium (polymerized liquid crystal) or a meta-structure, all of which can be implemented with ultra-thin and lightweight characteristics [44–47]. Based on such advantages as the high-efficiency and form-factor property, the polarization grating can be used as a beam steering optical element without mechanical movement [48]. In addition, it is easy to attach to a planar lightguide due to flat film-type. The polarization grating used in our experiments diffracts the input beam with the right-handed circular polarization (RCP) in a clockwise tilted direction; whereas diffracts the input beam with the left-handed circular polarization (LCP) in a counterclockwise tilted direction, as shown in Fig. 2.
Fig. 2. Schematic diagram of the transmissive polarization grating with the diffraction angle (θ) for the first-order. The polarization state of the diffracted beam is orthogonal to the input beam.
Download Full Size | PDF
In this paper, the lightguide is used to deliver a virtual image from the display source to the eye of users with a small form factor. The guide structure is widely used in NED devices because of the compactness [49,50]. Moreover, a design flexibility of the lightsource module can be improved compared to free-space propagation (FSP) methods that the input beam from the display module propagates into the air and directly enters the image combiner. In the glasses-type RPD with FSP methods, the incidence angle on the image combiner is limited due to the bone-structure of the skull. Moreover, the angular resolution and the spatial uniformity of the angular resolution depend on the inclination angle of the input beam, as shown in Fig. 3 [39]. Meanwhile, in the lightguide structure, the propagation angle of the input beam is determined by the diffractive optical elements (DOEs), which can mitigate these issues by designing the lightsource module undeterred by the incidence angle.
Fig. 3. Spatial distribution of the angular resolution depending on the incidence angle of the input beam (θ). In the free-space propagation method, the incidence angle toward the center of the HOE (C) is changed to provide the dynamic eyebox. The simulation is conducted under the following conditions; incidence angle of the input beam: 40°∼90°, HOE size: 190 mm, eye relief: 20 mm, and divergence angle of the lightsource: 30°.
Download Full Size | PDF
In the proposed configuration, we utilized HOEs as the in-coupler and out-coupler, which delivers the virtual images. HOEs have been widely used as the transparent image combiner, and have high diffraction efficiency and high angular-selectivity compared to the surface-relief DOE [51]. The in-coupler HOE is recorded using the angular-multiplexing technique [52], so that two separate beams entering the lightguide at different angles by the polarization grating are diffracted at different propagating angles. Similarly, the out-coupler is multiplexed to form multiple viewpoints. In the recording process of the out-coupler, the recording angle of the reference beam is the same as the propagating angles within lightguide, and the recording angle of signal beam is predetermined in consideration of the position of focal spots, which will be described in Section 3.3. In Fig. 4, we assume that the multiplexed diffraction angles of the in-coupler are θi1 and θi2, and out-coupler angles are sequentially defined as θo1 and θo2, respectively. The out-coupler is recorded to be paired with θo1 for incidence angle θi1 and with θo2 for θi2. This approach can form viewpoints at different positions depending on the guiding angle.
Fig. 4. Schematic diagram of the angular-multiplexed out-coupler; the output beam is selectively diffracted depending on the incidence angle, which is presented in different colors.
Download Full Size | PDF
In previous related works, the out-coupler HOE is fabricated to act as a concave half mirror to form focusing points [38,39,41,42]. This method simplifies system configuration because additional eyepiece lens is not required; but the recording setup of multiplexed HOE with short focal length and wide field of view (FOV) is hard to implement. To demonstrate the proposed scheme, we adopt HOE-recorded flat-mirrors and an additional eyepiece lens presented in the next section. Output beams diffracted by the out-coupler are focused at each viewpoint as passing through the eyepiece lens. By synchronizing with pupil movement extracted by an eye tracker, only the appropriate viewpoint group is activated.
To obtain the wide FOV with a short eye relief, geometric phase lenses (GPLs), or also called Pancharatnam-Berry phase lenses, are utilized as the eyepiece lens for the AR condition. Recently, serval related researches using the GPL have been reported to enhance the system performance [53–56]. GPLs are typically fabricated within a few millimeters thickness and in flat plate or film. These thinness and flatness are suitable for the compact NEDs. The GPL used in the proposed configuration operates as either a concave or convex lens depending on the polarization state of the input beam. Figure 5 shows the operating mode of the polarization-dependent eyepiece combiner (PDEC), which is composed of a quarter wave plate (QWP), two GPLs and a right-handed circular polarizer (RHCP). By inserting an RHCP between two GPL with same focal length and stacking them, this novel eyepiece combiner operates as optical transparent window or convex lens depending on the polarization of the input beam. The front-most QWP converts linearly polarized real scene and virtual image into circularly polarized light, which is essential in lightguide system where only guided beams with linear polarization can retain their polarization state, which will be discussed in detail in Section 3.3.
Fig. 5. Illustration of the composition of the PDEC and operating mode depending on the polarization state; (a) lens mode for virtual images and (b) optical windows mode for real scene. All optical elements are so thin (less than 1 mm) that the optical focusing power of the PDEC approximately nulls each other or increases two-fold in the stacked structure.
Download Full Size | PDF
The two-dimensional schematic diagram of proposed lightguide-type RPD with switchable multi-viewpoints is shown in Fig. 6. A laser scanning projector (LSP) and collimating lens are used to realize retinal projection scheme with narrow beam-width. The polarization of the input beam is converted to RCP or LCP by a polarization rotator. A vertical linear polarizer is placed behind the polarization grating to convert the circularly polarized beam into linearly polarized light before entering the lightguide. Similarly, a horizontal linear polarizer is located in front of out-coupler so that the PDEC acts as an optical window.
Fig. 6. Schematic diagram of the proposed configuration. Each red line shows beam path with LCP and each blue line shows beam path with RCP. The LCP and RCP are controlled by the polarization controller.
Download Full Size | PDF
As described above, the input beam, passing through the polarization grating, becomes to be incident on the HOE at different angles according to polarization state, and propagates at different guiding angles by multiplexed in-coupler. The multiplexed out-coupler allows guided beams at different angles to diffract in different directions to the incidence plane of PDEC. The output beams split into two groups are focused by the PDEC for the retinal projection. Consequently, only one viewpoint group closest to the pupil position detected by eye tracker can be activated by controlling the polarization of input beam.
The proposed configuration can solve image problems caused by equally duplicated viewpoints by switching between two multi-viewpoint groups according to pupil location. Additional flat thin optical devices used for viewpoint control enable a compact and lightweight RPD system with an extended eyebox.
We adopted the lightguide structure using the HOE combiners to provide the AR scene with a high efficiency and versatile design. In a conventional lightguide or waveguide-type NED system such as Microsoft HoloLens and Magic Leap One, the eyebox size is extended by using exit-pupil expansion (EPE) techniques [57]. However, these methods cannot be intactly applied to the lightguide-type RPD system in which all beams corresponding to individual pixel images are diffracted by HOE combiners and travel through the guide slab at the same angle. In the optical configuration of the RPD, HOEs are equal to the projection screen of images. Consequently, the size and location of the HOE combiner is required to match parallel input beams. If these design requirements are not reflected, unwanted images may be displayed. Figure 7 shows illustrative case under mismatching conditions. Due to the limitation by image combiners, a simple steering of the input source is subject to restrictions for expanding eyebox in the lightguide structure. Figure 8 presents the FOV graph with respect to the change of the incidence angle. This result reveals that the FOV is sharply narrowed as rotating the input beam, and the eyebox expansion (∼3 mm) by focus shift is insufficient to cover pupil movement.
Fig. 7. Optical misdesign classification. The simulation results are obtained by changing the size and position of the HOE combiners or the diffraction angle using commercial ray-tracing software (LightTools).
Download Full Size | PDF
Fig. 8. Eyebox expansion by steering of the input lightsource in the wedge-shaped lightguide. (Left) The focal spot by HOE combiner is shifted by inducing Bragg mismatching condition. (Right). The simulation results show that the lateral shifted distance of the focal spot is 1.2 mm in a leftward direction and 1.2 mm in a rightward direction at full-width at half maximum (FWHM) FOV. The simulation condition is as follows: the thickness of lightguide is 7 mm, the slanted angle of the wedge is 60°, and eye relief is 20 mm.
Download Full Size | PDF
In order to guide the two split input beams from the in-coupler to the out-coupler without distortion of the original image, the guiding angle, size and position of image couplers must be optimized. We adopted a slab-type lightguide with both in- and out-coupler. The optical design of the planar lightguide is less constrained compared to the wedge-type lightguide in which the incidence angle of the input beam is limited by the wedge angle [58]. In addition, the aspect ratio is modulated by the incidence angle, which causes additional image calibrations to provide the same image regardless of the viewpoint shifting in the wedge lightguide.
The overall system is determined by several key parameters: (1) the thickness of the lightguide d, (2) the length of the lightguide L, (3) the guiding angle θg by the in-coupler, (4) the width of projection surface on HOEs wHOE, (5) the diffraction angle of the polarization grating θp and (6) the focal length of the GPL fGP. We can assume that the distance between the polarization grating and lightguide and the thickness of polarization-dependent optical elements are negligible because of the thinness and flatness. For optimizing the lightguide system parameters, the following requirements must be considered.
First, the maximum allowable FOV (MAFOV) Θ max of the entire system is defined by:

Fig. 9. Guiding parameters are limited by the target FOV.
Download Full Size | PDF
Second, the difference between two Bragg angles (θB) of the multiplexed HOE should be wider than the angular diffraction range of in- and out-coupler HOEs for the two beams. This requirement can be simply written as:

Fig. 10. The diffraction efficiency according to the incidence angle (angular-selectivity).
Download Full Size | PDF
Third, two split beams diffracted by the in-coupler should be projected on the same location inside the out-coupler. The difference between the two projected positions (Δp) results in a reduction of the effective FOV to avoid mislocation problem in Fig. 7. Given a first guiding angle θg1, the second guiding angle θg2 is determined by the following equation:

Optimal system parameters can be acquired by satisfying design constraints above. By employing commercialized products, several key parameters are predetermined. In our experiments, the focal length of GPL is 45 mm (Edmund Optics S/N: 34-463) and the diffraction angle of polarization grating is 5 degrees in air (Edmund Optics S/N: 12-677). The holographic medium is Covestro’s Bayfol HX photopolymer film with 16 um thickness, which is thick enough to operate as volume gratings with a narrow bandwidth. Based on the Kogelnik’s coupled-wave theory (CWT), the simulation result shows that the value of Δθi is approximately within 2 degrees, which is smaller than the 3.3 degrees of the diffraction angle by the polarization grating; so that the “Bragg angle” constraint by Eq. (3) is satisfied.
Taking into account the system performance and experimental setup, other parameters are restricted as follows; in consideration of the precision and tolerances of the HOE recording setup, the guiding angle is limited between 48° and 65° with a step size of 1°. This limitation of step size hinders exact solutions in Eq. (4). To obtain nearest-neighbor solutions of the guiding angle, the value of Δp is allowed within 1 mm. The thickness of lightguide is set between 5 and 8 mm, which is thicker than typical thickness range of the eyeglass (2∼4 mm) but is chosen in accordance with a range of allowed guiding angles. The thickness can be reduced to normal spectacles by increasing guiding angle. The length of the lightguide is limited within between 50 and 80 mm. The minimum value of the MAFOV is set to be 40 degrees.
Figure 11 presents the results of the optimal solution calculated by sweeping two guiding angles. In the right graph, the effective FOV means the modulated MAFOV calculated by considering Δp. Searching results finally determine the specification of guiding system and the recording conditions of HOE combiners. In a recording process of the in-coupler HOE, the incidence angle of the reference beam is equal to θp in HOE medium and the incidence angle of the signal beams is set to be a pair of guiding angles among searching results; In the out-coupler recording, the reference beam should be matched to the signal beam of the in-coupler and the incidence angle of the signal beam, also defined as output angle θo, is determined by the position of multiple viewpoints.
Fig. 11. Optimized results of allowable guiding configurations. (a)The positional difference between the two projected beams on the out-coupler (Δp). (b) The longitudinal length of lightguide including HOE couplers. (c) The effective FOV changed by misalignment of the two diffracted beams. The blue dot lines indicate the value used in our experiments.
Download Full Size | PDF
The quasi-parallel output beam is incident on the PDEC and then focal spots are formed at the focal length of PDEC on lens mode, ignoring aberrations. The lateral shifted position of focal spots (Pi) in horizontal direction can be given as follows under the paraxial condition:

Figure 12(a) shows the relationship between output angles and the viewpoint spacing. In our experiments, the value of dv is set to be 4 mm by considering the average pupil size of young-adult and mid-age peoples [60]. In this condition, the double image is not observed even when the pupil size becomes large up to 8 mm. The inward output angle is 2.8° and the outward angle is 8.3°, which is indicated by black dot lines in Fig. 12(a). The modulated FOV according to each viewpoint is given as

Fig. 12. Simulation results. (a) The spacing between two viewpoints according to the output angle. The red line indicates the pair of output angles with 4 mm spacing, which are target conditions in our experiment. (b) The FOV decrement according to the output angle. The red dot lines indicate reduced FOV variations at selected conditions in our experiments.
Download Full Size | PDF
Based on the simulation results and deployability of the system above, finally selected system parameters are listed in Table 1. Figure 13 shows the simulation results of non-sequential ray-tracing in LightTools, excluding a PDEC device. In ray-tracing software, which does not support nanostructure devices such as polarization grating, the simulation modeling is modified separately according to the polarization state of the input beam. When using the parameters listed in Table 1, input beam with the width of 17 mm, equal to wHOE, can be propagated to the eyepiece part without distortion.
Fig. 13. 2-D layout of the designed lightguide system in LightTools. Each solid line is ray path. Simulation result for input beam with RCP condition (upper) and result for input beam with LCP condition (lower).
Download Full Size | PDF
Table 1. Specifications of the fabricated guiding system
In this section, we will discuss the persistence and optical efficiency depending on the polarization within the lightguide. In the proposed configuration, circularly polarized light is incident on the polarization grating for the optical path control. After passing through the polarization grating, the polarization state is orthogonally converted from input state, but the transmitted beam is still circularly polarized. When input beam with circular polarization is incident on the lightguide system, a major problem is that the polarization state is changed by the HOEs and TIR phenomenon within the lightguide. First, in holographic volume grating, the polarization of input field with circular or elliptical polarization can be modified due to phase difference between two orthogonal directions [61]. Second, in lightguide, the phase shift by TIR is different according to polarization eigenmode and guiding angle. When the circularly polarized light enters into the lightguide, the polarization state of the output beam is not perfect circular polarization, as shown in Fig. 14. In this case, GPLs are simultaneously operated as convex and concave lens, resulting in unwanted image noise.
Fig. 14. Simulation results of the polarization state along the optical path; (a) input beam with circular polarization, (b) diffracted beam by in-coupler, (c), (d), (e) guided beam according to a number of TIRs, and (f) output beam extracted by out-coupler where φi is the inclination angle and ei is the ellipticity. The simulation of the polarization change by HOEs is performed using a commercial software (RSoft) based on rigorous couple-wave theory.
Download Full Size | PDF
In addition, the diffraction efficiency for circular polarization is average of values obtained by vertical and horizontal linear polarizations under isotropic medium condition. Based on the CWT, the diffraction efficiency of reflection volume grating for TE mode is given as follows [62]:

Fig. 15. Diffraction efficiency of the HOE for TE and TM mode.
Download Full Size | PDF
To validate the design of the lightguide system, the experimental proof-of-concept is demonstrated as shown in Fig. 16. The lightguide slab is fabricated using a polished glass with SCHOTT NBK7, of which thickness is 7 mm. The thickness of polarization grating and GPL is both 0.45 mm and the aperture diameter of these is 25 mm. The PDEC device is fabricated by stacking with optical adhesive, of which total thickness is 1.3 mm. To avoid frustrated TIR, all polarization-dependent optical elements are attached on the lightguide with thin air gap less than 0.1 mm. The HOE combiners are recorded with a wavelength of 532 nm, which is sequentially multiplexed by controlling the recording time and the intensity of beams. The recording angle is listed in Table 1; for quadruple multiplexing out-coupler, the pairs of recording angle (reference and signal beam) are (62°, 8°) and (62°, -3°), which are called group 1 for LCP mode, (51°, 3°) and (51°, -8°), which are called group 2 for RCP mode. The diffraction efficiency for a dual multiplexed in-coupler is 87% and 85%. The out-coupler efficiency is 47%, 44%, 18% and 47%, respectively. The fabricated HOEs are resized with the width corresponding value of wHOE. The experimental results show that each distance between focal spots is 4mm or 5 mm, which have a slight gap with target value (4 mm). Such a problem may arise from fabrication error in the HOEs. The outward focal spots are blurred than inward, observed in Fig. 16. The sharpness degradation is caused by off-axis aberration by GPL [63], and can be mitigated by using aberration-corrected GPLs [64].
Fig. 16. Experimental results according to polarization of input beam (Visualization 1). The grid paper is placed at the eye relief distance (22.5 mm).
Download Full Size | PDF
The experimental setup or the entire system is constructed on the optical bench, as shown in Fig. 17. To provide sharp ray bundles with compact system, we use a commercial LSP (Celluon Picopro) as display. The display resolution is 1280×720 pixels. The focal length of collimating lens is 75 mm, which is employed by considering effective resolution on intercross section of HOEs and divergence of the projector beam for clear image. To enhance the sharpness of image, additional beam shaping lens is employed. In our experiment, focus tunable lens is located between the LSP and collimating lens, which has optical power between -2 and 3 diopters. The polarization rotator can actively switch the polarization state of input beam with high-speed (10 ms) for real-time operating system (Thorlabs LCC1221-A). The clear aperture of the polarization rotator is 20 mm, which is critical aperture stop limiting the FOV. The smartphone with wide-field camera (f ∕1.8 and 16 mm focal length) is used to capture resulting images because a CCD camera is not suitable for short eye relief condition.
Fig. 17. Prototype of the proposed configuration.
Download Full Size | PDF
Figures 18(a) and 18(b) show the results of FOV measurement with a target paper located at 10 cm from the camera. Measured FOV at inward viewpoints is approximately 37 degrees in the horizontal direction and 45 degrees in the diagonal direction. The deteriorated FOV compared with simulation value (42°) may be caused by the misalignment of optical components. Figure 18(e) presents the AR scene in which the virtual image in Fig. 18(d) is superimposed on a printout image in Fig. 18(c), which is located behind the lightguide.
Fig. 18. Photographs of experimental results; (a) AR scene of the rendering test circle image with real FOV target, (b) virtual image with maximum FOV, (c) printed driving screen image (real object), (d) virtual information images and (e) AR scene imitated driving.
Download Full Size | PDF
To validate large depth of focus (DOF) for retinal projection, display results are acquired according to the change in focal plane of the camera, as shown in Fig. 19(a). All captured results are the same quality regardless of the camera focus, demonstrating the all-in-focus property for the proposed configuration. Figure 19(b) shows the observed images at each focal spot. It is confirmed that virtual images can be rendered at separated viewpoints. Several image problems are observed in experimental results. A keystone distortion due to an oblique incidence at viewpoints can be resolved by the additional calibration image processing [65]. In the see-through window mode, chromatic distortion of real scene is slightly occurred, which may be caused by the misalignment and wavelength dispersion of the components in the PDEC. A low contrast issue may result from an HOE manufacturing, which can be alleviated by searching optimal fabrication conditions.
Fig. 19. Experimental results: (a) with different focal lengths of the camera, (b) at different viewpoints (Visualization 2).
Download Full Size | PDF
To analyze image quality for the implemented setup, angular resolution is measured at inward viewpoint with low aberration. The maximum spatial frequency for our prototype system is 6.6 cycles per degree (cpd) in which the effective pixel size is about 30 µm in projected display on the out-coupler. The modulation transfer function (MTF) value is acquired by calculating the contrast of captured fringe patterns and is indicated in Fig. 20. The cut-off frequency is about 3.4 cpd to adopt an MTF criterion of 35% [66]. When input beam passing through the polarization rotator is directly focused by a single GPL without the lightguide module, MTF results are equivalent level; which means that resolution deterioration by the lightguide system can be negligible. The resolution degradation may be caused by Gaussian blur of laser beam, and needs a precise beam shaping for sharp images.
Fig. 20. MTF results of the prototype system. The red circle indicates measured values and the solid line indicates interpolated MTF curve using curve fitting.
Download Full Size | PDF
For dynamic switching, we synchronized the eye tracker, polarization rotator and display. The printed artificial eye on paper is attached to the smartphone camera and the lateral shift of the pupil is simulated by moving the phone on the linear stage. The polarization state of the input beam is automatically changed to activate another viewpoint group when the shifting range of the imitated eye is more than half the spacing (2 mm). The associated movies show the dynamic switching of focal spots and virtual images depending on the pupil position.
In this paper, we proposed and demonstrated a novel retinal projection NED that can independently activate two separate groups of viewpoints using polarization-multiplexing technique. The proposed system is constructed by polarization-dependent elements and multiplexed HOE combiners to provide an extended eyebox with compact form factor. We analyzed and optically designed a lightguide system in detail. The fabricated guiding system using optimal design parameters can provide quadruple focal spots for retinal projection with short eye relief 22.5 mm. We built a proof-of-concept system in which wide eyebox 16 mm and moderate FOV 37° are achieved for 4 mm pupil condition. Previous studies with dynamic eyebox have several problems such as relatively bulky form factor and operating speed. The proposed configuration alleviates these problems with lightweight and thin optical devices. In addition, double or blank image arising from always-on focal spots can be mitigated. The proposed concept can also be applied in other optical systems for optical path control. We expect that the proposed concept can provide an efficient way to resolve obstacles that hinder the proliferation of AR NEDs in various applications.
Samsung Electronics.
The authors declare no conflicts of interest.
1. H. Hua, D. Cheng, Y. Wang, and S. Liu, “Near-eye displays: state-of-the-art and emerging technologies,” Proc. SPIE 7690, 769009 (2010). [CrossRef]  
2. Y. Lee, T. Zhan, and S. Wu, “Prospects and challenges in augmented reality displays,” VRIH 1(1), 10–20 (2019). [CrossRef]  
3. S. Yano, M. Emoto, T. Mitsuhashi, and H. Thwaites, “A study of visual fatigue and visual comfort for 3D HDTV/HDTV images,” Displays 23(4), 191–201 (2002). [CrossRef]  
4. D. M. Hoffman, A. R. Girshick, K. Akeley, and M. S. Banks, “Vergence-accommodation conflicts hinder visual performance and cause visual fatigue,” J. Vis. 8(3), 33 (2008). [CrossRef]  
5. T. Shibata, J. Kim, D. M. Hoffman, and M. S. Banks, “The zone of comfort: predicting visual discomfort with stereo displays,” J. Vis. 11(8), 11 (2011). [CrossRef]  
6. G. Kramida and A. Varshney, “Resolving the vergence-accommodation conflict in head-mounted displays,” IEEE Trans. Visual. Comput. Graphics 22(7), 1912–1931 (2016). [CrossRef]  
7. K. Terzić and M. Hansard, “Methods for reducing visual discomfort in stereoscopic 3D: a review,” Signal Process Image. 47, 402–416 (2016). [CrossRef]  
8. N. Matsuda, A. Fix, and D. Lanman, “Focal surface displays,” ACM Trans. Graph. 36(4), 1–14 (2017). [CrossRef]  
9. H. J. Jang, J. Y. Lee, J. Kwak, D. Lee, J.-H. Park, B. Lee, and Y. Y. Noh, “Progress of display performances: AR, VR, QLED, OLED, and TFT,” J. Inf. Disp. 20(1), 1–8 (2019). [CrossRef]  
10. R. Zabels, K. Osmanis, M. Narels, U. Gertners, A. Ozols, K. Rutenbergs, and I. Osmanis, “AR Displays: next-generation technologies to solve the vergence–accommodation conflict,” Appl. Sci. 9(15), 3147 (2019). [CrossRef]  
11. G. A. Koulieris, K. Aksit, M. Stengel, R. Mantiuk, K. Mania, and C. Richardt, “Near-eye display and tracking technologies for virtual and augmented reality,” Comput Graph Forum. 38(2), 493–519 (2019). [CrossRef]  
12. S. Liu and H. Hua, “Time-multiplexed dual-focal plane head-mounted display with a liquid lens,” Opt. Lett. 34(11), 1642–1644 (2009). [CrossRef]  
13. S. Ravikumar, K. Akeley, and M. S. Banks, “Creating effective focus cues in multi-plane 3D displays,” Opt. Express 19(21), 20940–20952 (2011). [CrossRef]  
14. X. Hu and H. Hua, “High-resolution optical see-through multi-focal-plane head-mounted display using freeform optics,” Opt. Express 22(11), 13896–13903 (2014). [CrossRef]  
15. F. C. Huang, K. Chen, and G. Wetzstein, “The light field stereoscope: immersive computer graphics via factored near-eye light field displays with focus cues,” ACM Trans. Graph. 34(4), 60 (2015). [CrossRef]  
16. D. Dunn and K. Myszkowski, “Wide field of view varifocal near-eye display using see-through deformable membrane mirrors,” IEEE Trans. Visual. Comput. Graphics 23(4), 1322–1331 (2017). [CrossRef]  
17. K. Akşit, W. Lopes, J. Kim, P. Shirley, and D. Luebke, “Near-eye varifocal augmented reality display using seethrough screens,” ACM Trans. Graphic 36(6), 1–13 (2017). [CrossRef]  
18. R. E. Stevens, D. P. Rhodes, A. Hasnainb, and P. Y. Laffontb, “Varifocal technologies providing prescription and VAC mitigation in HMDs using Alvarez lenses,” Proc. SPIE 10676, 18 (2018). [CrossRef]  
19. D. Kim, S. Lee, S. Moon, J. Cho, Y. Jo, and B. Lee, “Hybrid multi-layer displays providing accommodation cues,” Opt. Express 26(13), 17170–17184 (2018). [CrossRef]  
20. J.-H. R. Chang, B. V. K. V. Kumar, and A. C. Sankaranarayanan, “Towards multifocal displays with dense focal stacks,” ACM Trans. Graph. 37(6), 1–13 (2018). [CrossRef]  
21. K. Rathinavel, H. Wang, A. Blate, and H. Fuchs, “An extended depth-at-field volumetric near-eye augmented reality display,” IEEE Trans. Visual. Comput. Graphics 24(11), 2857–2866 (2018). [CrossRef]  
22. S. Lee, Y. Jo, D. Yoo, J. Cho, D. Lee, and B. Lee, “Tomographic near-eye displays,” Nat. Commun. 10(1), 2497 (2019). [CrossRef]  
23. Y. Takaki, “Development of super multi-view displays,” MTA 2(1), 8–14 (2014). [CrossRef]  
24. K. Aksit, P. Chakravarthula, K. Rathinavel, Y. Jeong, R. Albert, H. Fuchs, and D. Luebke, “Manufacturing application-driven foveated near-eye displays,” IEEE Trans. Visual. Comput. Graphics 25(5), 1928–1939 (2019). [CrossRef]  
25. K. J. MacKenzie, D. M. Hoffman, and S. J. Watt, “Accommodation to multiple-focal-plane displays: implications for improving stereoscopic displays and for accommodation control,” J. Vis. 10(8), 22 (2010). [CrossRef]  
26. P. V. Johnson, J. A. Parnell, J. Kim, C. D. Saunter, G. D. Love, and M. S. Banks, “Dynamic lens and monovision 3D displays to improve viewer comfort,” Opt. Express 24(11), 11808–11827 (2016). [CrossRef]  
27. N. Padmanaban, R. Konrad, T. Stramer, E. A. Cooper, and G. Wetzstein, “Optimizing virtual reality for all users through gaze-contingent and adaptive focus displays,” Proc. Natl. Acad. Sci. U. S. A. 114(9), 2183–2188 (2017). [CrossRef]  
28. Q. Y. J. Smithwick, J. Barabas, D. Smalley, and V. M. Bove Jr, “Interactive holographic stereograms with accommodation cues,” Proc. SPIE 7619, 761903 (2010). [CrossRef]  
29. H. Zhang, Y. Zhao, L. Cao, and G. Jin, “Fully computed holographic stereogram based algorithm for computer generated holograms with accurate depth cues,” Opt. Express 23(4), 3901–3913 (2015). [CrossRef]  
30. A. Maimone, A. Georgiou, and J. S. Kollin, “Holographic near-eye displays for virtual and augmented reality,” ACM Trans. Graph. 36(4), 1–16 (2017). [CrossRef]  
31. J.-H. Park, “Recent progresses in computer generated holography for three-dimensional scene,” J. Inf. Disp. 18(1), 1–12 (2017). [CrossRef]  
32. J. Mäkinen and E. Sahin., and A. Gotchev, “Analysis of accommodation cues in holographic stereograms,” in IEEE 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON) (2018)
33. St. Hilaire, S. A. Benton, M. Lucente, M. L. Jepsen, J. Kollin, H. Yoshikawa, and J. Underkoffler, “Electronic display system for omputational holography,” Proc. SPIE 1212, 174–182 (1990). [CrossRef]  
34. G. Westheimer, “The Maxwellian view,” Vision Res. 6(11-12), 669–682 (1966). [CrossRef]  
35. M. Sugawara, M. Suzuki, and N. Miyauchi, “Retinal imaging laser eyewear with focus-free and augmented reality,” in SID Symposium Digest of Technical Papers, 164–167 (2016).
36. P. K. Shrestha, M. J. Pryn, J. Jia, J. Chen, H. Fructuoso, A. Boev, Q. Zhang, and D. Chu, “Accommodation-free head mounted display with comfortable 3D perception and an enlarged eye-box,” AAAS Research (2019).
37. R. Konrad, N. Padmanaban, K. Molner, E. A. Cooper, and G. Wetzstein, “Accommodation-invariant computational near-eye displays,” ACM Trans. Graph. 36(4), 1–12 (2017). [CrossRef]  
38. C. Jang, K. Bang, S. Moon, J. Kim, S. Lee, and B. Lee, “Retinal 3D: augmented reality near-eye display via pupil-tracked light field projection on retina,” ACM Trans. Graph. 36(6), 1–13 (2017). [CrossRef]  
39. J. Kim, Y. Jeong, M. Stengel, K. Akşit, R. Albert, B. Boudaoud, T. Greer, W. Lopes, Z. Majercik, P. Shirley, J. Spjut, M. McGuire, and D. Luebke, “Foveated AR: dynamically-foveated augmented reality display,” ACM Trans. Graph 38(4), 1–15 (2019). [CrossRef]  
40. M. Hedili, B. Soner, E. ulusoy, and H. Urey, “Light-efficient augmented reality display with steerable eyebox,” Opt. Express 27(9), 12572–12581 (2019). [CrossRef]  
41. S.-B. Kim and J.-H. Park, “Optical see-through Maxwellian near-to-eye display with an enlarged eyebox,” Opt. Lett. 43(4), 767–770 (2018). [CrossRef]  
42. J. Jeong, J. Lee, C. Yoo, S. Moon, B. Lee, and B. Lee, “Holographically customized optical combiner for eye-box extended near-eye display,” Opt. Express 27(26), 38006–38018 (2019). [CrossRef]  
43. S. R. Nersisyan, N. V. Tabiryan, D. M. Steeves, and B. R. Kimball, “The promise of diffractive waveplates,” Opt. Photonics News 21(3), 40–45 (2010). [CrossRef]  
44. Y. Weng, D. Xu, Y. Zhang, X. Li, and S. T. Wu, “Polarization volume grating with high efficiency and large diffraction angle,” Opt. Express 24(16), 17746–17759 (2016). [CrossRef]  
45. X. Xiang, J. Kim, R. Komanduri, and M. J. Escuti, “Nanoscale liquid crystal polymer Bragg polarization gratings,” Opt. Express 25(16), 19298–19308 (2017). [CrossRef]  
46. M. Khorasaninejad and F. Capasso, “Broadband multifunctional efficient meta-gratings based on dielectric waveguide phase shifters,” Nano Lett. 15(10), 6709–6715 (2015). [CrossRef]  
47. S. Kim, S. Choi, C. Cho, Y. Lee, J. Sung, H. Yun, J. Jeong, S.-E. Mun, Y. Lee, and B. Lee, “Broadband efficient modulation of light transmission with high contrast using reconfigurable VO2 diffraction grating,” Opt. Express 26(26), 34641–34654 (2018). [CrossRef]  
48. H. Chen, Y. Weng, D. Xu, N. V. Tabiryan, and S.-T. Wu, “Beam steering for virtual/augmented reality displays with a cycloidal diffractive waveplate,” Opt. Express 24(7), 7287–7298 (2016). [CrossRef]  
49. K. Sarayeddine and K. Mirza, “Key challenges to affordable see-through wearable displays: the missing link for mobile AR mass deployment,” Proc. SPIE 8720, 87200D (2013). [CrossRef]  
50. M. U. Erdenebat, Y. T. Lim, K. C. Kwon, N. Darkhanbaatar, and N. Kim, “Waveguide-type head-mounted display system for AR application,” (2018).
51. J. A. Arns, W. S. Colburn, and S. C. Barden, “Volume phase gratings and their potentials for astronomical applications,” Proc. SPIE 3355, 866–876 (1998). [CrossRef]  
52. A. Pu and M. Spie, “Exposure schedule for multiplexing holograms in photopolymer films,” Opt. Eng. 35(10), 2824–2829 (1996). [CrossRef]  
53. T. Zhan, Y. H. Lee, and S. T. Wu, “High-resolution additive light field near-eye display by switchable Pancharatnam-Berry phase lenses,” Opt. Express 26(4), 4863–4872 (2018). [CrossRef]  
54. C. Yoo, K. Bang, C. Jang, D. Kim, C.-K. Lee, G. Sung, H.-S. Lee, and B. Lee, “Dual-focus waveguide see-through near-eye display with polarization dependent lenses,” Opt. Lett. 44, 1920–1923 (2019). [CrossRef]  
55. S. Moon, C.-K. Lee, S.-W. Nam, C. Jang, G.-Y. Lee, W. Seo, G. Sung, H.-S. Lee, and B. Lee, “Augmented reality near-eye display using Pancharatnam-Berry phase lenses,” Sci. Rep. 9(1), 6616 (2019). [CrossRef]  
56. T. Zhan, Y. H. Lee, G. Tan, J. Xiong, K. Yin, F. Gou, J. Zou, N. Zhang, D. Zhao, J. Yang, S. Liu, and S. T. Wu, “Pancharatnam-Berry optical elements for head-up and near-eye Displays,” J. Opt. Soc. Am. B 36(5), D52–D65 (2019). [CrossRef]  
57. P. Äyräs, P. Saarikko, and T. Levola, “Exit pupil expander with a large field of view based on diffractive optics,” J. Soc. Inf. Disp. 17(8), 659–664 (2009). [CrossRef]  
58. D. Cheng, Y. Wang, C. Xu, W. Song, and G. Jin, “Design of an ultra-thin near-eye display with geometrical waveguide and freeform optics,” Opt. Express 22(17), 20705–20719 (2014). [CrossRef]  
59. K. Ratnam, R. Konrad, D. Lanman, and M. Zannoli, “Retinal image quality in near-eye pupil-steered systems,” Opt. Express 27(26), 38289–38311 (2019). [CrossRef]  
60. I. M. Borish, Clinical refraction, 3rd ed. (Professional press, Chicago, 1970).
61. A. M. López, M. P. Arroyo, and M. Quintanilla, “Some polarization effects in holographic volume gratings,” J. Opt. A: Pure Appl. Opt. 1(3), 378–385 (1999). [CrossRef]  
62. H. Kogelnik, “Coupled wave theory for thick hologram gratings,” Bell Syst. Tech. J. 48(9), 2909–2947 (1969). [CrossRef]  
63. M. Khorasaninejad, W. T. Chen, J. Oh, and F. Capasso, “Super-dispersive off-axis meta-lenses for compact high resolution spectroscopy,” Nano Lett. 16(6), 3732–3737 (2016). [CrossRef]  
64. K. J. Hornburg, X. Xiang, J. Kim, M. Kudenov, and M. Escuti, “Design and fabrication of an aspheric geometric-phase lens doublet,” Proc. SPIE 10735, 37 (2018). [CrossRef]  
65. A. Bauer, S. Vo, K. Parkins, F. Rodriguez, O. Cakmakci, and J. P. Rolland, “Computational optical distortion correction using a radial basis function-based mapping method,” Opt. Express 20(14), 14906–14920 (2012). [CrossRef]  
66. E. A. Villegas, C. González, B. Bourdoncle, T. Bonin, and P. Artal, “Correlation between optical and psychophysical parameters as function of defocus,” Optom. Vis. Sci. 79(1), 60–67 (2002). [CrossRef]  


























Abstract
1. Introduction
2. Principle of the switchable viewpoints
3. Optical design of a lightguide and image combiner
4. Experimental setup and analysis of display system
5. Conclusions
Funding
Disclosures
References
This website uses cookies to deliver some of our products and services as well as for analytics and to provide you a more personalized experience. Click here to learn more. By continuing to use this site, you agree to our use of cookies. We've also updated our Privacy Notice. Click here to see what's new.
Zi Wang, Kefeng Tu, Yujian Pang, Miao Xu, Guoqiang Lv, Qibin Feng, Anting Wang, and Hai Ming







Holographic retinal projection display (RPD) can project images directly onto the retina without any lens by encoding a convergent spherical wave phase with the target images. Conventional amplitude-type holographic RPD suffers from strong zero-order light and conjugate. In this paper, a lensless phase-only holographic RPD based on error diffusion algorithm is demonstrated. It is found that direct error diffusion of the complex Fresnel hologram leads to low image quality. Thus, a post-addition phase method is proposed based on angular spectrum diffraction. The spherical wave phase is multiplied after error diffusion process, and acts as an imaging lens. In this way, the error diffusion functions better due to reduced phase difference between adjacent pixels, and a virtual image with improved quality is produced. The viewpoint is easily deflected just by changing the post-added spherical phase. A full-color holographic RPD with adjustable eyebox is demonstrated experimentally with time-multiplexing technique.

© 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement
Holographic retinal projection display (RPD) is a kind of emerging augmented reality (AR) near-eye display (NED) technology [1,2]. It directly forms a focus-free image on the retina by encoding a convergent spherical wave phase with the target image. The always-in-focus image naturally solves the fundamental vergence accommodation conflict (VAC) issue in conventional NED [3,4]. Based on this characteristic, the holographic RPD has wide applications in visual aid for visually impaired people, realizing stereoscopic display without visual fatigue, increasing the safety of auxiliary vehicle displays and so on. Its lensless feature makes the total system compact and aberration-free. Compared with lens-type RPD or laser scanning display, which usually uses a lens or lens-type holographic optical element (HOE) to converge the image light [5–13], the holographic RPD has the advantages of flexible control of beam width, image depth and convergence position, thus easily realizing adjustable viewpoint positions and meeting the requirement of eye movement.
Previous holographic RPD usually uses interference with reference light to convert the complex hologram to an amplitude-type hologram [14–21]. It suffers from strong direct current (DC) noise and conjugate noise. The encoded DC term in the hologram occupies most energy and causes low optical efficiency. Many research works have been reported to convert the complex hologram to a phase-only hologram for high diffraction efficiency and low noise [22–29]. The Gerchberg-Saxton algorithm, error diffusion algorithm, and double-phase method are the three widely used algorithms for generating phase-only holograms [22]. However, the GS algorithm is computationally inefficient and brings random noise to the reconstructed image. And the double-phase holography reduces the spatial resolution and leads to more complex spectral components. In our work, the bidirectional error diffusion method is adopted to optimize the phase distribution on the holographic plane for RPD, which reconstructs high quality images and suppresses speckle noise. Although some researchers have reported the phase-only holographic RPD, an optical lens is still used to converge the light rays, which increases aberration and system complexity [30,31].
In this paper, we propose a phase-only holographic RPD display with lensless feature. Firstly, the error diffusion algorithm is directly used on the complex Fresnel hologram and its performance is analyzed. It is found that the reconstruction image quality is low, especially at the image edge. It is mainly caused by the phase variation of the pre-added spherical wave phase. Second, a post-addition phase method based on angular spectrum diffraction is proposed to improve the error diffusion process. The reconstruction image quality is improved by reducing the phase difference among adjacent pixels caused by the pre-added spherical wave phase. Finally, a full-color holographic RPD is demonstrated in optical experiment with adjustable viewpoint positions.
Figure 1 shows the conventional Fresnel diffraction calculation of holographic RPD. The target image A(x1,y1) is first multiplied with a convergent spherical wave phase to form the complex amplitude distribution:

Then, the complex amplitude distribution H(x2,y2) is usually encoded into an amplitude-type hologram as:

Fig. 1. Conventional Fresnel diffraction calculation of holographic RPD.
Download Full Size | PDF
Fig. 2. (a) Scanning sequence of error diffusion. The errors are diffused to neighbor pixels when in (b) odd rows and (c) even rows.
Download Full Size | PDF
The error diffusion algorithm extracts the phase in each pixel sequentially, and simultaneously spreads the error caused by phase extraction to adjacent pixels regularly [21]. Starting from the first pixel, the phase is extracted by:

Next, the error is diffused to the neighborhood pixels that have not been visited previously. Its neighborhood members are updated according to the following equations:

Figure 3 shows the simulated reconstruction results of the hologram after error diffusion. The parameters are set as: z1 = 474 mm, z2 = 130 mm, λ = 520 nm, and the hologram contains 4096 × 2160 pixels with 3.6 µm pixel interval. To reconstruct the image of the hologram, the wavefront is first propagated to the pupil plane, and multiplied with a circular function which acts as a human eye. Since the human pupil aperture usually ranges from 2-8 mm, we choose 4 mm aperture size as the size of the circular function. Then, the filtered wavefront is back propagated to the image plane to reconstruct the image. Figure 3(e)–(h) show the corresponding reconstruction results of the original images in Fig. 3(a)–(d). It shows that the reconstructed images based on the conventional Fresnel diffraction calculation of holographic RPD has severe noises especially on the edge part. The peak signal to noise ratios (PSNR) are quite low. The quality of the reconstructed images are limited by the error diffusion algorithm. It can be explained from the phase variation of the added spherical wave phase. In the conventional holographic RPD hologram calculation, the target image is firstly multiplied with a spherical wave phase as shown in Eq. (1), and propagated to the hologram plane. Compared with a uniform phase, the spherical wave phase increases the phase difference among adjacent pixels, especially in the image edge. It is known that the error diffusion functions by spreading error to adjacent pixels. If the phase variations among adjacent pixels are large, the error diffusion will not function well.
Fig. 3. (a)-(d) Original images. (e)-(h) The corresponding simulated reconstruction results of the direct error diffusion method.
Download Full Size | PDF
To verify this inference, the simulated reconstruction results with different values of z2 are presented in Fig. 4, while keeping z1 unchanged. Figure 4(a) shows that the image quality with z2 = 200 mm is better than that with z2 = 60 mm. Figure 4(b) shows that as z2 increases, the PSNR also increases. It can be explained that as z2 increases, the focal length of the spherical wave increases, and the curvature decreases. It results in a smaller phase difference between adjacent pixels. Thus, increasing z2 helps improve the effect of error diffusion. However, a larger z2 means a longer eye relief, reducing the compactness of the total system. Thus, there exists a trade-off between image quality and system compactness in terms of the value of z2.
Fig. 4. (a) Two reconstruction results with z2 = 60 mm and z2 = 200 mm. (b) The relation curve between z2 and image quality.
Download Full Size | PDF
To improve the error diffusion process for better reconstruction quality, we propose a post-addition phase method. Compared with the aforementioned calculation process, the difference here is that the spherical wave phase is added after the error diffusion to reduce the conversion error. Figure 5 shows that the proposed method contains three steps. First, the target image is multiplied with a uniform phase at the image plane, and propagated to the hologram plane by angular spectrum diffraction (ASD):

Compared with previous spherical wave phase, the uniform phase will cause less phase variations in the complex hologram. Second, H(x2,y2) is converted to an intermediate phase-only hologram Hm(x2,y2) by error diffusion. Finally, Hm(x2,y2) is multiplied with a convergent spherical wave phase to form the final phase hologram:

It is noted that the last added spherical phase acts as a lens with focal length z2. Thus, the target image will be imaged to a magnified virtual image through the lens imaging equation:

Another advantage of ASD is that the image size remains unchanged regardless of diffraction distance. Thus, the FOV remains the same at each image depth. However, in conventional Fresnel calculation, the FOV changes with different image depths [17]. Figure 6(a)–(d) shows the simulated reconstruction results of proposed method. Compared to the results in Fig. 3, the PSNRs of the reconstructed images are greatly increased, mainly because the image edges are improved a lot.
Fig. 5. (a) The process to generate phase-only holograms by post-addition phase method. (b) The principle of retinal projection display based on post-addition phase method.
Download Full Size | PDF
Fig. 6. (a)-(d) The simulated reconstruction results of proposed post-addition phase method. (e)-(h) The optical reconstruction results of direct error diffusion method. (i)-(l) The optical reconstruction results of proposed post-addition phase method.
Download Full Size | PDF
The local fringe frequency of the post-added spherical phase along the x-direction is expressed as:

At the edge of the hologram, the maximum local fringe frequency is NΔx2/2λz2. The maximum local fringe frequency should not exceed the Nyquist frequency of the hologram, leading to:

Thus, there is a lower limit of focal length z2.
Optical experiment was performed to verify the proposed method. A laser beam with 532 nm wavelength was collimated to illuminate the SLM. A phase-type SLM (3.6 µm pixel pitch, 4096 × 2160 resolution) was used to load the hologram. The distances z1 and z2 were set to 1.5 m and 0.11 m. Figure 6(e)–(h) shows the optical reconstruction results of the conventional direct error diffusion method. We can see that in the red box, strong noises appear and degrade the image quality, which is consistent with the simulation results. Figure 6(i)–(l) shows the optical reconstruction results of the proposed post-addition phase method. Since the spherical wave phase no longer affects the error diffusion process, the quality of the reconstructed image is improved. The experimental improvement is not as good as the simulation because of the speckle noise.
The left parts of Figs. 6(i)–(l) look darker than other regions because the intensity of the perceived image is modulated by sinc2(pxx/λz1,pxy/λz1) due to the diffraction of the limited pixel aperture px, so the edge part is darker than the center. In addition, a deflected spherical phase is used to separate the viewpoint from the zero-order light (caused by the dead-space area of the SLM), so the image position is deflected as well. This increases the nonuniform intensity distribution. It can be improved by intensity compensation [19].
Next, full-color holographic RPD display is demonstrated in Fig. 7. The R, G, B sub-holograms were sequentially loaded on the SLM, while the R, G, B lasers synchronously illuminated the SLM. Due to the persistence of vision, a full-color image was perceived. Figure 7(b) shows the optical reconstruction results when the camera lens was focused at 0.8 m and 1.6 m, respectively. The virtual image is always in-focus while the real objects are out of focus. Another advantage of the proposed method is that it has no chromatic dispersion. In conventional Fresnel diffraction method, the image sizes of different colors are related with the corresponding wavelengths. The blue laser will reconstruct the smallest image size, so the red and green channels need to be demagnified to match the blue channel. While in the proposed method, the ASD reconstruct the same image size regardless of the wavelength. In full-color display, each sub-hologram is multiplied with the corresponding spherical phase of the same focal length, so the imaging relations of different wavelengths are the same.
Fig. 7. (a) The process of computer-generated RGB three-channel hologram and experimental setup for full-color retinal projection display (b) Optical reconstruction results captured at different depths.
Download Full Size | PDF
To match the pupil position, the viewpoint position can be adjusted by adding a deflected spherical wave phase in Eq. (15):

Fig. 8. Positions of (a) viewpoint 1, (b) viewpoint 2, (c) viewpoint 3, and (d) viewpoint 4 in the pupil plane and their reconstruction results.
Download Full Size | PDF
Then the maximum adjustable range is given as:

Although all the results are based on 2D image display, the proposed method can support 3D display in 3 aspects. Firstly, by combining binocular parallax-based 3D displays and proposed RPD, two parallax images are projected onto the retinas of both left and right eyes, and 3D display is realized without vergence accommodation conflict. Secondly, in our previous work, the holographic super multi-view (SMV) display [21], multiple parallax images of 3-D objects captured from different viewpoints are converged into the pupil, which makes the retinal projection display have monocular depth cues. This study is based on multiple 2D parallax images to provide a correct accommodation depth cue. Thus, by combining the proposed RPD and SMV, 3D display can be realized. In addition, in future work, we will study how to combine multi-plane display and the proposed RPD to provide depth cues, which can be useful for RPD in 3D display applications.
In this paper, a lensless phase-only holographic RPD is proposed with improved image quality. The error diffusion algorithm is adopted to convert the complex Fresnel hologram to a phase-only hologram. Its performance is examined and analyzed. It is found that direct error diffusion does not function well due to the phase variations of pre-added spherical wave phase. A post-addition phase method based on angular spectrum diffraction is proposed to make the error diffusion algorithm more effective. The post-added spherical phase acts as a lens and produces a virtual image. The image quality is improved compared with direct error diffusion. A full-color holographic RPD with adjustable viewpoint position is demonstrated with time-multiplexing technique. Each color channel shares the same FOV and no chromatic dispersion appears. The viewpoint is easily deflected just by changing the post-added spherical phase. The proposed method is promising for future RPD near-eye display with compact structure and adjustable eyebox.
National Natural Science Foundation of China (61805065, 62275071); Major Science and Technology Projects in Anhui Province (202203a05020005); Fundamental Research Funds for the Central Universities (JZ2021HGTB0077).
The authors declare no conflicts of interest.
Data underlying the results presented in this paper are not publicly available at this time but may be obtained from the authors upon reasonable request.
1. J. Xiong, E. L. Hsiang, Z. He, T. Zhan, and S. T. Wu, “Augmented reality and virtual reality displays: emerging technologies and future perspectives,” Light: Sci. Appl. 10(1), 216 (2021). [CrossRef]  
2. Z. He, X. Sui, G. Jin, and L. Cao, “Progress in virtual reality and augmented reality based on holographic display,” Appl. Opt. 58(5), A74–A81 (2019). [CrossRef]  
3. C. P. Chen, L. Zhou, J. Ge, Y. Wu, L. Mi, Y. Wu, B. Yu, and Y. Li, “Design of retinal projection displays enabling vision correction,” Opt. Express 25(23), 28223–28235 (2017). [CrossRef]  
4. L. Mi, C. P. Chen, Y. Lu, W. Zhang, J. Chen, and N. Maitlo, “Design of lensless retinal scanning display with diffractive optical element,” Opt. Express 27(15), 20493–20507 (2019). [CrossRef]  
5. X. Shi, J. Liu, Z. Zhang, Z. Zhao, and S. Zhang, “Extending eyebox with tunable viewpoints for see-through near-eye display,” Opt. Express 29(8), 11613–11626 (2021). [CrossRef]  
6. C. Jang, K. Bang, S. Moon, J. Kim, S. Lee, and B. Lee, “Retinal 3D: augmented reality near-eye display via pupil-tracked light field projection on retina,” ACM Trans. Graph. 36(6), 1–13 (2017). [CrossRef]  
7. J. Xiong, Y. Li, K. Li, and S. T. Wu, “Aberration-free pupil steerable Maxwellian display for augmented reality with cholesteric liquid crystal holographic lenses,” Opt. Lett. 46(7), 1760–1763 (2021). [CrossRef]  
8. M. K. Hedili, B. Soner, E. Ulusoy, and H. Urey, “Light-efficient augmented reality display with steerable eyebox,” Opt. Express 27(9), 12572–12581 (2019). [CrossRef]  
9. S. B. Kim and J. H. Park, “Optical see-through Maxwellian near-to-eye display with an enlarged eyebox,” Opt. Lett. 43(4), 767–770 (2018). [CrossRef]  
10. C. Jang, K. Bang, G. Li, and B. Lee, “Holographic near-eye display with expanded eye-box,” ACM Trans. Graph. 37(6), 1–14 (2018). [CrossRef]  
11. S. Zhang, Z. Zhang, and J. Liu, “Adjustable and continuous eyebox replication for a holographic Maxwellian near-eye display,” Opt. Lett. 47(3), 445–448 (2022). [CrossRef]  
12. D. Wang, C. Liu, C. Shen, Y. Xing, and Q. H. Wang, “Holographic capture and projection system of real object based on tunable zoom lens,” PhotoniX 1(1), 6 (2020). [CrossRef]  
13. Y. Wu, C. Chen, L. Mi, W. Zhang, J. Zhao, Y. Lu, W. Guo, B. Yu, Y. Li, and N. Maitlo, “Design of retinal-projection-based near-eye display with contact lens,” Opt. Express 26(9), 11553–11567 (2018). [CrossRef]  
14. Y. Takaki and N. Fujimoto, “Flexible retinal image formation by holographic Maxwellian-view display,” Opt. Express 26(18), 22985–22999 (2018). [CrossRef]  
15. C. Chang, W. Cui, J. Park, and L. Gao, “Computational holographic Maxwellian near-eye display with an expanded eyebox,” Sci. Rep. 9(1), 18749 (2019). [CrossRef]  
16. Z. Wang, X. Zhang, G. Lv, Q. Feng, H. Feng, and A. Wang, “Hybrid holographic Maxwellian near-eye display based on spherical wave and plane wave reconstruction for augmented reality display,” Opt. Express 29(4), 4927 (2021). [CrossRef]  
17. Z. Wang, X. Zhang, K. Tu, G. Lv, Q. Feng, A. Wang, and H. Ming, “Lensless full-color holographic Maxwellian near-eye display with a horizontal eyebox expansion,” Opt. Lett. 46(17), 4112–4115 (2021). [CrossRef]  
18. Z. Wang, X. Zhang, G. Lv, Q. Feng, A. Wang, and H. Ming, “Conjugate wavefront encoding: an efficient eyebox extension approach for holographic Maxwellian near-eye display,” Opt. Lett. 46(22), 5623–5626 (2021). [CrossRef]  
19. Z. Wang, K. Tu, Y. Pang, G. Lv, Q. Feng, A. Wang, and H. Ming, “Enlarging the FOV of lensless holographic retinal projection display with two-step Fresnel diffraction,” Appl. Phys. Lett. 121(8), 081103 (2022). [CrossRef]  
20. Z. Wang, K. Tu, Y. Pang, X. Zhang, G. Lv, Q. Feng, A. Wang, and H. Ming, “Simultaneous multi-channel near-eye display: a holographic retinal projection display with large information content,” Opt. Lett. 47(15), 3876–3879 (2022). [CrossRef]  
21. X. Zhang, Y. Pang, T. Chen, K. Tu, Q. Feng, G. Lv, and Z. Wang, “Holographic super multi-view Maxwellian near-eye display with eyebox expansion,” Opt. Lett. 47(10), 2530–2533 (2022). [CrossRef]  
22. P. W. M. Tsang and T. C. Poon, “Review on the State-of-the-Art Technologies for Acquisition and Display of Digital Holograms,” IEEE Trans. Ind’l. Info. 12(3), 886–901 (2016). [CrossRef]  
23. P.W.M. Tsang and T. C. Poon, “Novel method for converting digital Fresnel hologram to phase-only hologram based on bidirectional error diffusion,” Opt. Express 21(20), 23680–23686 (2013). [CrossRef]  
24. X. Sui, Z. He, G. Jin, and L. Cao, “Spectral-envelope modulated double-phase method for computer-generated holography,” Opt. Express 30(17), 30552–30563 (2022). [CrossRef]  
25. D. Pi, J. Liu, and Y. Wang, “Review of computer-generated hologram algorithms for color dynamic holographic three-dimensional display,” Light: Sci. Appl. 11(1), 231 (2022). [CrossRef]  
26. Y. W. Zheng, D. Wang, Y. L. Li, N. N. Li, and Q. H. Wang, “Holographic near-eye display system with large viewing area based on liquid crystal axicon,” Opt. Express 30(19), 34106–34116 (2022). [CrossRef]  
27. S. Jiao, D. Zhang, C. Zhang, Y. Gao, T. Lei, and X. Yuan, “Complex-amplitude holographic projection with a digital micromirror device (DMD) and error diffusion algorithm,” IEEE J. Sel. Topics Quantum Electron. 26(5), 1–8 (2020). [CrossRef]  
28. X. Yang, S. Jiao, Q. Song, G. B. Ma, and W. Cai, “Phase-only color rainbow holographic near-eye display,” Opt. Lett. 46(21), 5445–5448 (2021). [CrossRef]  
29. H. Pang, J. Z. Wang, M. Zhang, A. X. Cao, L. F. Shi, and Q. L. Deng, “Non-iterative phase-only Fourier hologram generation with high image quality,” Opt. Express 25(13), 14323–14333 (2017). [CrossRef]  
30. A. Maimone, A. Georgiou, and J. S. Kollin, “Holographic near-eye displays for virtual and augmented reality,” ACM Trans. Graph 36(4), 1–16 (2017). [CrossRef]  
31. W. T. Song, X. Li, Y. J. Zheng, Y. Liu, and Y. T. Wang, “Full-color retinal-projection near-eye display using a multiplexing-encoding holographic method,” Opt. Express 29(6), 8098–8107 (2021). [CrossRef]  














Abstract
1. Introduction
2. Direct error diffusion of complex Fresnel hologram
3. Post-addition phase method based on angular spectrum diffraction
4. Full-color holographic RPD with adjustable viewpoint positions
5. Conclusion
Funding
Disclosures
Data availability
References
This website uses cookies to deliver some of our products and services as well as for analytics and to provide you a more personalized experience. Click here to learn more. By continuing to use this site, you agree to our use of cookies. We've also updated our Privacy Notice. Click here to see what's new.
Yuhang Wu, Chao Ping Chen, Lantian Mi, Wenbo Zhang, Jingxin Zhao, Yifan Lu, Weiqian Guo, Bing Yu, Yang Li, and Nizamuddin Maitlo




We propose a design of a retinal-projection-based near-eye display for achieving ultra-large field of view, vision correction, and occlusion. Our solution is highlighted by a contact lens combo, a transparent organic light-emitting diode panel, and a twisted nematic liquid crystal panel. Its design rules are set forth in detail, followed by the results and discussion regarding the field of view, angular resolution, modulation transfer function, contrast ratio, distortion, and simulated imaging.

© 2018 Optical Society of America under the terms of the OSA Open Access Publishing Agreement
In recent years, the notion of augmented reality (AR) [1] has been going viral thanks to the staggering venture investments and countless media hypes. With AR, users are able to view the real world overlaid with computer-generated imagery and information. Such user experience can be realized by two types of optical solutions, i.e. video see-through displays and optical see-through near-eye displays (NEDs) [2]. The former is usually deployed on well-established mobile devices such as smartphones and tablets, while the latter on the immature wearable devices, e.g. smart glasses or headsets. As far as user experience is concerned, optical see-through NEDs outperform video see-through displays in that what you see is what you get. But sadly, an ideal solution for optical see-through NEDs that could perfectly live up to the requirements of AR is still a big challenge. For example, combiner-based NEDs―including beam splitters [3–5], semi-reflective mirrors [6–8], and holographic reflectors [9–11]―are often bulky and heavy if designed for a large field of view (FOV). Waveguide-based NEDs―including both planar [12–14] and freeform [15–17] waveguides―are more compact in terms of form factor as the optical path can be compressed into the waveguide. However, once the light enters into a waveguide, the maximum angle, at which it could leave, will be bound by the total internal reflection and the ways of out-coupling. Retinal-projection-based or direct-view NEDs―including retinal scanning [18–20], pinlight [21], and iOptik [22], in which the image is directly projected on the retina―may have both compactness and large FOVs, and yet each one has its own issues. The retinal scanning is vulnerable to the rotation of eyeball. The pinlight struggles with the change in the gaze direction, pupil size, and eye’s focal state. The iOpitk―a proprietary technology of Innovega―is identified as a contact lens embedded with a polarizer and a band-pass filter. Despite years of development, the manufacturability of such contact lens remains to be improved.
Unlike video see-through displays, optical see-through NEDs are of wearable devices. Therefore, optics aside, ergonomics merits special care as well. One of the ergonomic pain points to solve is to save the visually impaired users from the trouble of wearing extra eyeglasses. As earlier attempts, we introduced combiner-based [23–25], waveguide-based [26–28], and retinal-projection-based NEDs [29–31], which are merged with the prescription or corrective lenses for vision correction. In this paper, we shall extend to a different scenario when a subset of population would prefer to wear contact lens in the hope of yielding a better performance in outdoor activities. To satisfy this niche, a retinal-projection-based NED, which features a contact lens combo, a transparent organic light-emitting diode (OLED) panel, and a twisted nematic liquid crystal (TN-LC) panel, is proposed. In what follows, its structure, design rules, and results and discussion are to be elaborated.
Figure 1 is a schematic drawing of the proposed NED, which involves four major components, i.e. an OLED panel, a TN-LC panel, a contact lens combo―including a contact lens, a patterned analyzer, and a microlens―and an eye. The OLED panel is supposed to be transparent and responsible for delivering the virtual images. To the inner side of the OLED panel is attached a polarizer that is vertically polarized. The TN-LC panel is capable of switching the polarization. The contact lens is tailored to correct the refractive anomalies of the eye, depending on the user’s acuity. On top of the contact lens is coated a patterned analyzer, whose transmission axis (TA) at the inner part is same as that of polarizer but at the outer part orthogonal to the former, as shown in the inset of Fig. 1, where the blue double arrows denote TAs. On top of the inner part of analyzer is laminated a microlens, which is used to converge the light of OLED. L is the diagonal dimension of active area of OLED panel. D is the distance between the OLED panel and eye. Ri is the radius of inner part of patterned analyzer.
Fig. 1 Schematic drawing of the proposed NED, which involves four major components, i.e. an OLED panel, a TN-LC panel, a contact lens combo―including a contact lens, a patterned analyzer, and a microlens―and an eye. L is the diagonal dimension of active area of OLED panel. D is the distance between the OLED panel and eye. Ri is the radius of inner part of patterned analyzer.
Download Full Size | PDF
Prior to explaining the design rules of our NED, it is essential to understand the mechanism of eye. From the perspective of geometric optics, human eye is equivalent to a zoom lens system, mainly consisting of two focusing elements―i.e. cornea and lens―and a sensor―i.e. retina [32]. For the sake of easy calculation and explanation, a simplified eye is exploited, as shown in Fig. 2. It is composed of the cornea (anterior and posterior), chambers (anterior and posterior) filled with aqueous humor, pupil, lens (anterior and posterior), vitreous chamber filled with vitreous humor, and retina. The cornea accounts for approximately two thirds of the eye’s total diopter [33]. The lens, on the other hand, is responsible for fine-tuning the diopter of eye in response to the object distance.
Fig. 2 Simplified eye structure, which is composed of the cornea (anterior and posterior), chambers (anterior and posterior) filled with aqueous humor, pupil, lens (anterior and posterior), vitreous chamber filled with vitreous humor, and retina.
Download Full Size | PDF
When light emitting from object S first arrives at the anterior cornea, the refraction occurs. By neglecting the thickness between adjacent surfaces and treating each surface as spherical, object distance s, image distance si after ith surface and final image distance s′ could be correlated as [34]

Fig. 3 Calculated object distance s as a function of the diopter of eye. If the target value of object distance s is set as 3 m, Pe shall be 41.92 m−1. If the target value of object distance s is set as 1.5 cm, Pe shall be 91.50 m−1, which is obviously impractical as Pe usually maximizes at 53 m−1.
Download Full Size | PDF
From the perspective of ophthalmology, human eye is a complex and delicate sensory organ. On the retina, there are three types of photoreceptor cells―i.e. rods, cones, and ganglion cells [36]. Rods are sensitive to the brightness for both high and low light levels. Cones are sensitive to the colors but only work at high light levels. Ganglion cells indirectly contribute to the sight for being credited for the circadian rhythm and pupillary reflex. The distribution of photoreceptor cells throughout the retina is uneven and highly concentrated at the center of retina. Near the center of retina is located a 1.5-mm pit, known as fovea, which is richest in cones. The fovea is responsible for the 100% acuity or sharpest central vision―sometimes dubbed as foveal vision. A bigger area surrounding the fovea is called macula, which is 5.5 mm across and houses the largest amount of both cones and rods [37]. If aligning the optical axis to the center of macula, the angular size of macula θ, seen from the air, can be approximated as

The design of the proposed NED deals with two optical paths, one for imaging the real object and the other for imaging the virtual object. The optical path diagrams for the real and virtual images are illustrated in Fig. 4 and Fig. 5, respectively. In imaging the real object, both OLED and TN-LC are turned off. As shown in Fig. 4, light rays emitting from the real object will first become vertically polarized after passing through the polarizer. When light is incident on the off-state TN-LC, a phenomenon known as optical activity is incurred [34], for which, the polarization of light at the exit will be rotated by 90°―i.e. horizontally polarized if viewed head-on. When rays reach the inner part of analyzer―which is vertically polarized―they will be blocked, implying that brightness of real image heavily leans on the size of inner part of analyzer. Only when rays reach the outer part of analyzer―which is horizontally polarized―they could be transmitted and then refracted in turn by the contact lens, cornea, and lens. Finally, an inverted image will be formed on the retina. The design of contact lens shall follow from the lensmaker’s equation [34], as given by Eq. (9)

Fig. 4 Optical path diagram for imaging the real image, for which both OLED and TN-LC are turned off.
Download Full Size | PDF
Fig. 5 Optical path diagram for imaging the virtual image, for which both OLED and TN-LC are turned on.
Download Full Size | PDF
In imaging the virtual object, both OLED and TN-LC are turned on. As shown in Fig. 5, light rays emitting from the virtual object―i.e. the screen of OLED―will first become vertically polarized after passing through the polarizer. When incident on the on-state TN-LC, the optical activity is deactivated, for which, the polarization of light at the exit will remain unchanged. When rays reach the outer part of analyzer, they will be blocked. On the other hand, when rays reach the inner part of analyzer, they could be transmitted. This arrangement of patterned analyzer guarantees that when viewing the virtual object, no rays from the real object would stand in the way. This is particularly important for the outdoor usage as the strong ambient light would easily overwhelm the virtual object. Now the table could be turned for the ambient light will be substantially dampened by the patterned analyzer and even outshined by the OLED. In other words, the occlusion [2] between the real and virtual objects is enabled. Considering that OLED is too close to the eye that it is out of the range of accommodation, a microlens to pre-converge the rays is required to compensate the upper limit of range of accommodation. Again, employing lensmaker’s equation, the diopter of microlens Pm is determined by

FOV is a key indicator for evaluating the performance of NED. Because the contact lens is usually way larger than the pupil and tightly adheres to the eye through the surface tension of tears [40], FOV with or without contact lens remains the same. To avoid ambiguity, FOV of the real image, FOVr―by default along the diagonal direction―can be calculated by

Fig. 6 FOV of virtual image, which is defined as the angular size of OLED. It is apparent that FOVv hinges on the size of OLED and enlarges as the eye gets closer to OLED.
Download Full Size | PDF
The contact lens combo consists of a contact lens, a patterned analyzer, and a microlens. The positions of contact lens and patterned analyzer are interchangeable. The microlens, patterned analyzer and contact lens are center-aligned. Patterned analyzer can be fashioned via the photoalignment technique [41]. Consider a case that a user has 3 diopters of myopia, disregarding the astigmatism and other types of refractive anomalies. Per the said design rules, a contact lens combo can be tentatively designed using the parameters as given in Table 1. It should be mentioned that those parameters are subject to change after the optimization, as will be done later. Incidentally, for the fact that contact lens is a medical device, it is highly recommended to resort to an optometrist or ophthalmologist for professional advice on whether is suitable or not to wear contact lens, frequency of use, choice of materials, water content, oxygen permeability etc.
Table 1. Parameters for the contact lens combo
OLED panel, acting as a virtual object, consists of an OLED and a polarizer. For the real image, it is switched off, whereas for the virtual image, it is switched on. Preferably, it is supposed to be highly transparent to enhance the light utilization. Alternatively, OLEDs can be replaced by the quantum dot light-emitting diodes [42] or other types of transparent displays. Due to the unavailability of transparent OLEDs of merely a couple of inches, a set of customized parameters are listed in Table 2, where the resolution is 1024 × 768, diagonal is 1.7 inch, pixel size is 33.73 µm, transmittance of OLED is 30%, contrast ratio (CR) is 10000, transmittance of polarizer is 49%. The overall transmittance or transparency of OLED panel is 14.7%. For a better transparency, the resolution has to be reduced so as to increase the aperture ratio, meaning that there is a tradeoff between the transparency and resolution.
Table 2. Parameters for OLED panel
TN-LC panel, acting as a polarization rotator, consists of a TN-LC [43], which is sandwiched between two glass substrates coated with indium tin oxide (ITO) electrodes and polyimide (PI) alignment layers, as shown in Fig. 7. The switching of TN-LC should be synchronized with OLED. In the off-state―null voltage is applied―LC directors at the entrance and exit are perpendicular to one another. Under such configuration, the polarization of emerging light will be rotated by 90° via the optical activity [43]. In the on-state―a voltage is applied―the twist of LC directors is unwound, thereby lifting the optical activity. As a result, the polarization of emerging light will remain intact. To fulfill the first maximum of Mauguin condition [43], cellgap of LC layer dlc, birefringence of LC Δn, and wavelength λ shall meet

Fig. 7 Polarization switching of TN-LC panel. In the (a) off-state―null voltage is applied―LC directors at the entrance and exit are perpendicular to one another. Under such configuration, the polarization of emerging light will be rotated by 90° via the optical activity. In the (b) on-state―a voltage is applied―the twist of LC directors is unwound, thereby lifting the optical activity. As a result, the polarization of emerging light will remain intact.
Download Full Size | PDF
Our simulation is implemented with an optical design software Code V (Synopsys), which is based on the ray tracing [44] and capable of analyzing the imaging properties, including modulation transfer function (MTF), distortion, and imaging simulation. The design wavelengths are 458, 543, and 632.8 nm. The fields of 0° (center of fovea) and 9° (periphery of macula), and 55° are selected. As OLED, polarizer, TN-LC, and patterned analyzer are free of diopters, they are omitted during the calculation of imaging properties.
The numbering of surfaces is labelled as in Fig. 8. The real and virtual objects are situated at 3 m and 15 mm away from the eye, respectively. Surfaces 1 to 2 (S1 to S2) make up the microlens. Surfaces 2 to 3 (S2 to S3) make up the contact lens. Surfaces 3 to 8 (S3 to S8) make up the eye, of which, S3 is anterior cornea, S4 posterior cornea, S5 pupil, S6 anterior lens, S7 posterior lens, and S8 retina. In calculating the real image, real object and surfaces from S2 to S8 are active, of which S5 is assigned as stop. In calculating the virtual image, virtual object and surfaces from S1 to S8 are active, of which S2 is assigned as stop.
Fig. 8 Numbering of surfaces. The real and virtual objects are situated at 3 m and 15 mm away from the eye, respectively. Surfaces 1 to 2 (S1 to S2) make up the microlens. Surfaces 2 to 3 (S2 to S3) make up the contact lens. Surfaces 3 to 8 (S3 to S8) make up the eye, of which, S3 is anterior cornea, S4 posterior cornea, S5 pupil, S6 anterior lens, S7 posterior lens, and S8 retina. In calculating the real image, real object and surfaces from S2 to S8 are active. In calculating the virtual image, virtual object and surfaces from S1 to S8 are active.
Download Full Size | PDF
To model the eye, the structural parameters of eye are originally adopted from a schematic eye proposed by Navarro et al. [45]. Along with the preliminary parameters enumerated in the previous section, we could build an initial NED design by presetting the surfaces of each element. Two optimizations are carried out in turn for the real and virtual images. At first, an optimization for the real image is done by constraining the length of the eye to be 24 mm. Then, fixing the as-optimized parameters for eye and contact lens, an optimization for the virtual image is done by tweaking the microlens only. The final parameters obtained after the optimization are summarized in Table 3. Besides, more detailed parameters for defining aspherical surfaces are disclosed in Table 4.
Table 3. Parameters used for the simulation
Table 4. Parameters for aspherical surfaces
Table 5 lists the parameters necessary for evaluating FOVs. According to Eqs. (13) and (14), FOVr and FOVv are calculated as 153° (diagonal) and 110° (diagonal), respectively.
Table 5. Parameters for calculating FOVs
Angular resolution―measured in arcminute (′)―of the image formed on the retina relies on both the resolutions of object and eye. For the resolution of real object is usually way higher than that of eye, angular resolution of the real image, ARr, shall be equal to the latter, which is the reciprocal of visual acuity [38]. Thus,

By computing the auto-correlation of the pupil function [47], diffraction MTFs of both real and virtual images are calculated as a function of spatial frequency―the number of cycles or line pairs per millimeter [48]―for the diffraction limit and fields of 0°, 9° and 55° (tangential and radial), as shown in Fig. 9. For the real image, MTFs within the macula are above 0.4 at 6 cycles/mm. For the virtual image, MTFs within the macula are above 0.4 at 20 cycles/mm.
Fig. 9 Calculated MTFs of (a) real and (b) virtual images. For the real image, MTFs within the macula are above 0.4 at 6 cycles/mm. For the virtual image, MTFs within the macula are above 0.4 at 20 cycles/mm.
Download Full Size | PDF
Contrast ratio―if treated as a transient quantity―is defined as the ratio of maximum intensity to minimum intensity [44], and it can be derived as [31]

Distortions of real and virtual images, defined as the displacement of image height or ray location, are plotted in Fig. 10, where the distortions of real and virtual images are less than 29% and 45%, respectively. Considering the fact that the distortion is an intrinsic property of eye [49], an absolutely distortion-free NED might not be very necessary. Instead, a certain distortion would be advantageous for the virtual world to be meshed perfectly with the real world, as long as the distortions of real and virtual images could be overlapped.
Fig. 10 Calculated distortions of real and virtual images. For real and virtual images, the distortions are less than 29% and 45%, respectively.
Download Full Size | PDF
For a qualitative analysis of imaging quality, both real and virtual images are visualized from the imaging simulation that takes into account the effects of distortion, aberration blurring, diffraction blurring, and relative illumination, as shown in Fig. 11. By comparing the original and simulated images, it can be seen that the real image is inherently distorted at large field angles, while the virtual image turns out to be more blurred and more pronounced in the chromatic aberration. It has to be mentioned that those simulated images are what appear on the retina.
Fig. 11 (a) Original (photographer: C. P. Chen, location: Flaming Mountains, Turpan, China), (b) real, and (c) virtual images. By comparing the original and simulated images, it can be seen that the real image is inherently distorted at large field angles, while the virtual image turns out to be more blurred and more pronounced in the chromatic aberration.
Download Full Size | PDF
A retinal-projection-based NED and design rules thereof are proposed. Its structure is characterized by a contact lens combo, a transparent OLED panel, and a TN-LC panel. Based on the simulation, its key performance including FOV, angular resolution, MTF, CR, and distortion has been studied. For the real image, FOV is 153° (diagonal), angular resolution is 1′, MTF is above 0.4 at 6 cycles/mm, CR is 1999, and the distortion is less than 29%. For the virtual image, FOV is 110° (diagonal), angular resolution is 5.16′, MTF is above 0.4 at 20 cycles/mm, CR is 11, and the distortion is less than 45%. Targeting the niche market on the contact-lens-wearing users and outdoor AR applications, our solution would offer several technical edges or possibilities that might be difficult with the current practices. First, its ultra-large FOVs for both real and virtual images are unparalleled by the others. Second, as opposed to eyeglasses, contact lens combo saves more room. Moreover, similar to polarized sunglasses, the analyzer within the combo could block the ultraviolet light and mitigate the glare [50]. Third, apart from being an optical device, contact lens combo can even cater to cosmetic needs by tinting the non-optical area of lens. Fourth, the occlusion between real and virtual objects is achieved by the patterning of analyzer and polarization switching of TN-LC.
Science and Technology Commission of Shanghai Municipality (1701H169200); Shanghai Jiao Tong University (AF0300204, WF101103001/085); Shanghai Rockers Inc. (15H100000157).
Special thanks to Prof. Yi-Hsin Lin (National Chiao Tung University) for her valuable advice during the submission.
1. Wikipedia, “Augmented reality,” https://en.wikipedia.org/wiki/augmented_reality.
2. W. Barfield, Fundamentals of Wearable Computers and Augmented Reality 2nd Edition (Chemical Rubber Company, 2015).
3. H.-S. Chen, Y.-J. Wang, P.-J. Chen, and Y.-H. Lin, “Electrically adjustable location of a projected image in augmented reality via a liquid-crystal lens,” Opt. Express 23(22), 28154–28162 (2015). [CrossRef]   [PubMed]  
4. Y.-J. Wang, P.-J. Chen, X. Liang, and Y.-H. Lin, “Augmented reality with image registration, vision correction and sunlight readability via liquid crystal devices,” Sci. Rep. 7(1), 433 (2017). [CrossRef]   [PubMed]  
5. Q. Gao, J. Liu, X. Duan, T. Zhao, X. Li, and P. Liu, “Compact see-through 3D head-mounted display based on wavefront modulation with holographic grating filter,” Opt. Express 25(7), 8412–8424 (2017). [CrossRef]   [PubMed]  
6. J. P. Rolland, “Wide-angle, off-axis, see-through head-mounted display,” Opt. Eng. 39(7), 1760–1767 (2000). [CrossRef]  
7. S. Liu, H. Hua, and D. Cheng, “A novel prototype for an optical see-through head-mounted display with addressable focus cues,” IEEE Trans. Vis. Comput. Graph. 16(3), 381–393 (2010). [CrossRef]   [PubMed]  
8. R. Zhu, G. Tan, J. Yuan, and S.-T. Wu, “Functional reflective polarizer for augmented reality and color vision deficiency,” Opt. Express 24(5), 5431–5441 (2016). [CrossRef]   [PubMed]  
9. C. Jang, C.-K. Lee, J. Jeong, G. Li, S. Lee, J. Yeom, K. Hong, and B. Lee, “Recent progress in see-through three-dimensional displays using holographic optical elements.,” Appl. Opt. 55(3), A71–A85 (2016). [CrossRef]   [PubMed]  
10. A. Maimone, A. Georgiou, and J. S. Kollin, “Holographic near-eye displays for virtual and augmented reality,” ACM Trans. Graph. 36(4), 85 (2017). [CrossRef]  
11. C. Jang, K. Bang, S. Moon, J. Kim, S. Lee, and B. Lee, “Retinal 3D: augmented reality near-eye display via pupil-tracked light field projection on retina,” ACM Trans. Graph. 36(6), 190 (2017). [CrossRef]  
12. Y. Amitai, “Extremely compact high-performance HMDs based on substrate-guided optical element,” in SID Symposium (2004), pp. 310–313. [CrossRef]  
13. T. Levola, “Diffractive optics for virtual reality displays,” J. Soc. Inf. Disp. 14(5), 467–475 (2006). [CrossRef]  
14. H. Mukawa, K. Akutsu, I. Matsumura, S. Nakano, T. Yoshida, M. Kuwahara, and K. Aiki, “A full-color eyewear display using planar waveguides with reflection volume holograms,” J. Soc. Inf. Disp. 17(3), 185–193 (2009). [CrossRef]  
15. D. Cheng, Y. Wang, H. Hua, and M. M. Talha, “Design of an optical see-through head-mounted display with a low f-number and large field of view using a freeform prism,” Appl. Opt. 48(14), 2655–2668 (2009). [CrossRef]   [PubMed]  
16. Q. Wang, D. Cheng, Y. Wang, H. Hua, and G. Jin, “Design, tolerance, and fabrication of an optical see-through head-mounted display with free-form surface elements,” Appl. Opt. 52(7), C88–C99 (2013). [CrossRef]   [PubMed]  
17. X. Hu and H. Hua, “High-resolution optical see-through multi-focal-plane head-mounted display using freeform optics,” Opt. Express 22(11), 13896–13903 (2014). [CrossRef]   [PubMed]  
18. S. C. McQuaide, E. J. Seibel, J. P. Kelly, B. T. Schowengerdt, and T. A. Furness III, “A retinal scanning display system that produces multiple focal planes with a deformable membrane mirror,” Displays 24(2), 65–72 (2003). [CrossRef]  
19. M. Sugawara, M. Suzuki, and N. Miyauchi, “Retinal imaging laser eyewear with focus-free and augmented reality,” in SID Display Week (2016), pp. 164–167.
20. T. North, M. Wagner, S. Bourquin, and L. Kilcher, “Compact and high-brightness helmet-mounted head-up display system by retinal laser projection,” J. Disp. Technol. 12(9), 982–985 (2016). [CrossRef]  
21. A. Maimone, D. Lanman, K. Rathinavel, K. Keller, D. Luebke, and H. Fuchs, “Pinlight displays: wide field of view augmented reality eyeglasses using defocused point light sources,” ACM Trans. Graph. 33(4), 89 (2014). [CrossRef]  
22. R. Sprague, A. Zhang, L. Hendricks, T. O’Brien, J. Ford, E. Tremblay, and T. Rutherford, “Novel HMD concepts from the DARPA SCENICC program,” Proc. SPIE 8383, 838302 (2012). [CrossRef]  
23. C. P. Chen, Z. Zhang, and X. Yang, “A head-mounted smart display device for augmented reality,” CN Patent 201610075988.7 (2016).
24. L. Zhou, C. P. Chen, Y. Wu, K. Wang, and Z. Zhang, “See-through near-eye displays for visual impairment,” in The 23rd International Display Workshops in conjunction with Asia Display (2016), pp. 1114–1115.
25. L. Zhou, C. P. Chen, Y. Wu, Z. Zhang, K. Wang, B. Yu, and Y. Li, “See-through near-eye displays enabling vision correction,” Opt. Express 25(3), 2130–2142 (2017). [CrossRef]   [PubMed]  
26. C. P. Chen, Y. Wu, and L. Zhou, “An optical display device for augmented reality,” CN Patent, 201610112824.7 (2016).
27. Y. Wu, C. P. Chen, L. Zhou, Y. Li, B. Yu, and H. Jin, “Near-eye display for vision correction with large FOV,” in SID Display Week (2017), pp. 767–770.
28. Y. Wu, C. P. Chen, L. Zhou, Y. Li, B. Yu, and H. Jin, “Design of see-through near-eye display for presbyopia,” Opt. Express 25(8), 8937–8949 (2017). [CrossRef]   [PubMed]  
29. C. P. Chen, Y. Wu, L. Zhou, J. Ge, J. Liu, and J. Xu, “A near-eye display integrated with vision correction,” CN Patent, 201710065160.8 (2017).
30. C. P. Chen, Y. Wu, J. Zhao, Y. Li, and B. Yu, “A retinal-projection-based near-eye display,” CN Patent, 201710751238.1 (2017).
31. C. P. Chen, L. Zhou, J. Ge, Y. Wu, L. Mi, Y. Wu, B. Yu, and Y. Li, “Design of retinal projection displays enabling vision correction,” Opt. Express 25(23), 28223–28235 (2017). [CrossRef]  
32. Wikipedia, “Human eye,” https://en.wikipedia.org/wiki/human_eye.
33. Wikipedia, “Cornea,” https://en.wikipedia.org/wiki/cornea.
34. F. L. Pedrotti, L. M. Pedrotti, and L. S. Pedrotti, Introduction to Optics 3rd Edition (Addison-Wesley, 2006).
35. University of Notre Dame, “Physics of the eye,” https://www3.nd.edu/~nsl/Lectures/mphysics.
36. Wikipedia, “Photoreceptor cell,” https://en.wikipedia.org/wiki/Photoreceptor_cell.
37. Wikipedia, “Macula of retina,” https://en.wikipedia.org/wiki/Macula_of_retina.
38. Wikipedia, “Visual acuity,” https://en.wikipedia.org/wiki/Visual_acuity.
39. I. P. Howard, Perceiving in Depth, Volume 1: Basic Mechanisms (Oxford University, 2012).
40. Wikipedia, “Surface tension,” https://en.wikipedia.org/wiki/Surface_tension.
41. V. G. Chigrinov, V. M. Kozenkov, and H.-S. Kwok, Photoalignment of Liquid Crystalline Materials: Physics and Applications (Wiley, 2008).
42. J. Cao, J.-W. Xie, X. Wei, J. Zhou, C.-P. Chen, Z.-X. Wang, and C. Jhun, “Bright hybrid white light-emitting quantum dot device with direct charge injection into quantum dot,” Chin. Phys. B 25(12), 128502 (2016). [CrossRef]  
43. P. Yeh and C. Gu, Optics of Liquid Crystal Displays 2nd Edition (Wiley, 2009).
44. R. E. Fischer, B. Tadic-Galeb, and P. R. Yoder, Optical System Design 2nd Edition (McGraw-Hill Education, 2008).
45. I. Escudero-Sanz and R. Navarro, “Off-axis aberrations of a wide-angle schematic eye model,” J. Opt. Soc. Am. A 16(8), 1881–1891 (1999). [CrossRef]   [PubMed]  
46. Wikipedia, “Nyquist–Shannon sampling theorem,” https://en.wikipedia.org/wiki/Nyquist-Shannon_sampling_theorem.
47. H. H. Hopkins, “The numerical evaluation of the frequency response of optical systems,” Proc. Phys. Soc. B 70(10), 1002–1005 (1957). [CrossRef]  
48. Wikipedia, “Spatial frequency,” https://en.wikipedia.org/wiki/Spatial_frequency.
49. M. Bass, C. DeCusatis, J. Enoch, V. Lakshminarayanan, G. Li, C. MacDonald, V. Mahajan, and E. V. Stryland, Handbook of Optics 3rd Edition Volume III: Vision and Vision Optics (McGraw-Hill Education, 2009).
50. Wikipedia, “Sunglasses,” https://en.wikipedia.org/wiki/Sunglasses.


























Abstract
1. Introduction
2. Design principle
3. Results and discussion
4. Conclusions
Funding
Acknowledgments
References and links
This website uses cookies to deliver some of our products and services as well as for analytics and to provide you a more personalized experience. Click here to learn more. By continuing to use this site, you agree to our use of cookies. We've also updated our Privacy Notice. Click here to see what's new.
Chao Ping Chen, Lei Zhou, Jiahao Ge, Yuhang Wu, Lantian Mi, Yishi Wu, Bing Yu, and Yang Li




We propose a retinal-projection-based near-eye display that is able to merge with the vision correction for myopia. Our solution is highlighted by a corrective lens coated with an array of tiled organic light-emitting diodes and a transmissive spatial light modulator. Its design rules are set forth in detail, followed by the results and discussion regarding the field of view, modulation transfer function, contrast ratio, distortion, and simulated imaging.

© 2017 Optical Society of America under the terms of the OSA Open Access Publishing Agreement
It has been almost three decades since the term “augmented reality (AR)” was coined at Boeing in 1990 by Thomas Caudell [1]. Today, AR is undergoing a season that might be called “game of thrones” [2] as there are a host of AR players or technologies being active and seemingly promising but none of them is predominant. Among other things, the solution for a see-through near-eye display (NED) that would perfectly live up to the AR standards is still an open question. As far as the cost of fabrication is concerned, combiner-based NEDs, which rely on either beam splitters [3–5] or semi-reflective mirrors [6–8], are preferred. However, due to the size of beam splitters and semi-reflective mirrors, such NEDs―if designed with a large field of view (FOV)―are often bulky and heavy. As far as the form factor is concerned, waveguide-based NEDs, including both planar [9–11] and freeform [12–14] waveguides, are favorable as the optical path can be compressed into the waveguide. However, once the light enters into a waveguide, the maximum angle, at which it could leave, will be bound by the total internal reflection and the ways of out-coupling. For this reason, FOVs of such NEDs are usually below 50° [15]. As far as FOV is concerned, retinal-projection-based NEDs [16–18] are unparalleled by the former two. For example, pinlight display [17], co-developed by University of North Carolina at Chapel Hill and Nvidia, achieved 110° FOV on a very simple structure that merely comprises a plastic substrate etched with V-shaped notches and a transmissive spatial light modulator (SLM).
For wearable NEDs, optics aside, ergonomics needs to be taken into account as well. One of the ergonomic pain points to solve is to save the visually impaired users from the trouble of wearing extra eyeglasses or contact lens. As earlier attempts, we introduced both combiner-based [19–21] and waveguide-based [22–24] NEDs, which are merged with the prescription or corrective lenses for vision correction. In this paper, a retinal-projection-based NED―we shall refer to it as retinal projection display (RPD) hereafter―that enables vision correction is proposed. In what follows, its structure, design rules and simulation results are to be elaborated.
Figure 1 is the schematic drawing of the proposed RPD, which involves four major components, i.e. a corrective lens, an array of organic light-emitting diodes (OLEDs) [25], a transmissive SLM, and an eye. The corrective lens is used for compensating the refractive anomalies of the eye, say myopia. Preferably, its outer surface is concave, while its inner surface is flat, upon which OLEDs can be easily fabricated or laminated. OLEDs serve as the light source to illuminate the virtual image. Since the size of an individual OLED is way smaller than that of SLM, each OLED can be regarded as a point light source. The SLM is used for generating virtual images. ds is the distance between the SLM and eye. dc is the distance between the corrective lens and eye. D is the center spacing between two adjacent OLEDs. L is the dimension of SLM.
Fig. 1 Schematic drawing of the proposed RPD. ds is the distance between the SLM and eye. dc is the distance between the corrective lens and eye. D is the center spacing between two adjacent OLEDs. L is the dimension of SLM.
Download Full Size | PDF
Prior to explaining the design rules of our RPD, it is essential to understand the mechanism of eye. Human eye is an adjustable lens system, mainly consisting of two focusing elements―i.e. cornea and lens―and a light receptor―i.e. retina [26]. For the sake of easy calculation and yet considerable precision, a simplified eye, as shown in Fig. 2, which is composed of the cornea (anterior and posterior), anterior and posterior chambers filled with aqueous humor, pupil, lens (anterior and posterior), vitreous chamber filled with vitreous humor, and retina, is adopted. The cornea accounts for approximately two thirds of the eye’s total diopter [27]. The lens, on the other hand, is responsible for fine-tuning the diopter of eye in response to the object distance. Similar to a double lens system, the diopter of eye Pe can be written as [28]

Fig. 2 Simplified eye structure, which consists of cornea (anterior and posterior), anterior and posterior chambers filled with aqueous humor, pupil, lens (anterior and posterior), vitreous chamber filled with vitreous humor, and retina.
Download Full Size | PDF
Fig. 3 Range of accommodation, where Pc = 39 m−1, Pl varies from 2 to 18 m−1, and t = 6 mm. It can be seen that the range of accommodation starts at 40.5 m−1 and ends at 52.8 m−1.
Download Full Size | PDF
Fig. 4 Calculated object distance s as a function of the diopter of eye Pe. If the target value of object distance s is set as 3 m, Pe shall be 41.15 m−1.
Download Full Size | PDF
The design of the proposed RPD deals with two optical paths, one for imaging the real objects and the other for imaging the virtual objects. The optical path diagrams for the real and virtual images are illustrated in Fig. 5 and Fig. 6, respectively. For the real image, as shown in Fig. 5, where both SLM and OLEDs are supposed to be transparent, light rays emitting from the real object will be first diverged by the corrective lens, and then converged by the eye. Through the accommodation of eye and the compensation of corrective lens, a clear image will be maintained on the retina as long as the real object is within the range of accommodation. The diopter or optical power P of the corrective lens is related to the visual acuity and it can be obtained directly from the eyeglass prescription. The design of the corrective lens shall follow from the lensmaker’s equation [29], as given by Eq. (1)

Fig. 5 The optical path diagram for imaging the real object, where both SLM and OLEDs are supposed to be transparent. For the real image, light rays emitting from the real object will be first diverged by the corrective lens, and then converged by the eye.
Download Full Size | PDF
Fig. 6 The optical path diagram for imaging the virtual object, when merely a single OLED is turned on.
Download Full Size | PDF
For the virtual image, as shown in Fig. 6, let us first consider a scenario when merely a single OLED is turned on. Light rays emitting from the OLED will first encounter the SLM and illuminate its pixels, which in turn form a real object A that is composed of the actual rays (solid lines). In order for the real object A not to be directly perceived by the eye, it is required that SLM be placed out of the range of accommodation. In other words, SLM is too close to the eye that the eye is not powerful enough―the diopter of eye usually maximizes at 53 m−1 [28]―to converge the rays of A on the retina. Therefore, instead of seeing the real object A, eye will trace back along the extended virtual rays (dashed lines) to see a virtual object S being formed at a distance s that is given by

Given s = 3 m, Pe = 43.15 m−1, P = −2 m−1, s′ = 24.5 mm, ap = 4 mm, dc = 20 mm and, θ is only 11.3°. For θ = 100°, dc shall be decreased to 1.68 mm, which is apparently impractical. Hence, a tilted configuration with a multiple of OLEDs is required to achieve large FOV.
Now consider a scenario when two adjacent OLEDs are simultaneously turned on, as illustrated in Fig. 7. There will be two virtual objects being imaged at the distance s. In order for these two virtual objects to be seamlessly tiled, the centers of two adjacent OLEDs should be spaced at an optimal distance D that is given by [17]

Fig. 7 The optical path diagrams for imaging multiple virtual objects, when two adjacent OLEDs are simultaneously turned on.
Download Full Size | PDF
As can be inferred from Eq. (9), the optimal distance D will be subject to change as both pupil and diopter of eye may vary from time to time. When pupil expands or shrinks, shifting the pupil size away from the predetermined value, there will be an overlapping or gap between the neighboring virtual objects. To handle the change in pupil size, the array of tiled OLEDs shall be addressed in a way that it is able to dynamically tune the distance D to match with the current pupil size [17]. Since the distance dc is usually way smaller than the object distance s, the change in diopter of eye, on the other hand, barely affects the distance D unless the object distance s becomes very close.
FOV is a key indicator for evaluating the performance of RPD. Referring to Fig. 8, FOV of the real image, FOVr―defined as the angular extent of the corrective lens―can be calculated as

Fig. 8 Illustration of FOVs for both real and virtual images. FOVr is defined as the angular extent of the corrective lens, whereas FOVv is defined as the angular extent of the SLM.
Download Full Size | PDF
Consider a case that a user has only 2 diopters of myopia, disregarding the astigmatism and other types of refractive anomalies. The material of corrective lens is chosen as the polycarbonate, which is predominantly used for eyeglasses. The outer surface of corrective lens is concave, while the inner surface is flat, upon which OLEDs can be easily fabricated or laminated. Following the above design rules, a corrective lens can be designed using the parameters as given in Table 1.
Table 1. Design parameters for the corrective lens
The function of spatial light modulator is to modulate the intensity of light emitted from the OLEDs. Plus, it is desirable to be highly transparent. As a viable option, a backlight-free, monochrome liquid crystal display (LCD)―e.g. L3D13U-55G00 (Epson)―can be used for this purpose. Its parameters are listed in Table 2, where the contrast ratio (CR) is 500 and transmittance or transparency is 24%. For a better transparency, a higher aperture ratio is needed [32]. If not to shrink the size of opaque thin-film transistors and electrodes, then the resolution has to be reduced, meaning that there is a tradeoff between the transparency and resolution.
Table 2. Parameters for SLM
OLEDs in tandem with a SLM constitute an array of miniature projectors that deliver the virtual image directly to the retina. Basically, each OLED can be regarded as an individual point light source. The fabrication of OLEDs can be made either on the inner surface of corrective lens or on a separate substrate, which will be later laminated to the corrective lens. For being transparent to the upfront view, OLEDs with a top-emitting structure [33] are favored. Since the active matrix [34] is not needed for driving the OLEDs and each OLED can be as tiny as possible as long as the spacing D is guaranteed, a reasonably high transparency can be expected. Alternatively, OLEDs can be replaced by the quantum dot light-emitting diodes [35] or other types of thin-film, transparent lighting technologies. For the tiled configuration of RPD, the total number of OLEDs shall be determined by

Our simulation is implemented with the optical design software Code V (Synopsys), which is based on the ray tracing [37] and capable of analyzing the imaging properties, including modulation transfer function (MTF), distortion, and imaging simulation. Our design wavelength is 550 nm. Although a configuration with tiled OLEDs is proposed, our simulation is limited to the case of a single OLED. This is because it is technically impossible to set up more than one object in Code V. As the sharpest or best vision occurs at the center of retina [38], where the fovea is located, a configuration with a single OLED being center-aligned with retina is adopted for the simulation. To avoid duplicate simulations, other cases are omitted.
The numbering of surfaces is labelled as in Fig. 9. The object represents either the real or virtual object that is 3 m away from the eye. Surfaces 1 to 2 (S1 to S2) make up the corrective lens. Surfaces 3 to 7 (S3 to S7) make up the eye, of which, S3 is anterior cornea, S4 is posterior cornea, S5 is anterior lens with pupil, S6 is posterior lens, and S7 is retina. In calculating the real image, all surfaces are active. In calculating the virtual image, surfaces 2 to 7 are active, while S1 is inactive. To treat the OLED, which is situated on S2, as a point light source, the aperture of S2 is decreased to 0.1 mm.
Fig. 9 The numbering of surfaces. The object represents either the real or virtual object that is 2 m away from the eye. Surfaces 1 to 2 (S1 to S2) make up the corrective lens. Surfaces 3 to 7 (S3 to S7) make up the eye, of which, S3 is anterior cornea, S4 is posterior cornea, S5 is anterior lens with pupil, S6 is posterior lens, and S7 is retina. In calculating the real image, all surfaces are active. In calculating the virtual image, only S1 is inactive.
Download Full Size | PDF
According to the said design rules, we could create an initial structure by presetting the parameters for each element. Then, an optimization, whose error function type is set as transverse ray aberration [37], is carried out by constraining the effective focal length of the eye to be 24.5 mm―i.e. length of eye ball. The parameters obtained after the optimization are summarized in Table 3. Besides, more detailed parameters for defining aspherical surfaces are disclosed in Table 4.
Table 3. Parameters used for the simulation
Table 4. Parameters for aspherical surfaces
Table 5 lists the parameters necessary for evaluating FOV of the tiled configuration, in which dc and ds are optimized as 20 mm and 10.03 mm, respectively. From Eqs. (11) and (12), FOVr and FOVv are calculated as 114° (diagonal) and 117° (diagonal), respectively. For the single OLED configuration, we shall recall Eq. (8), with which, FOV of single OLED is calculated as 11.3°.
Table 5. Parameters for calculating FOVs
MTF is the most comprehensive performance criterion for NEDs. By computing the auto-correlation of the pupil function [39], MTFs of both real and virtual images are calculated, as shown in Fig. 10, where black dotted lines the MTFs inclusive of diffraction, while other lines the MTFs exclusive of diffraction. Since the tiniest aperture in our RPD―i.e. the pixel of SLM―is 15 μm across, which is almost two orders of magnitude larger than the wavelength, the diffraction is negligible. Moreover, owing to the rotational symmetry, the marginal angle is set as 5.65°―half of FOV of single OLED. For the real image, MTFs for all angles are above 0.4 at 280 cycles/mm. For the virtual image, MTFs for all angles are above 0.4 at 120 cycles/mm.
Fig. 10 Calculated MTFs of (a) real and (b) virtual images. For the real image, MTFs for all angles are above 0.4 at 280 cycles/mm. For the virtual image, MTFs for all angles are above 0.4 at 120 cycles/mm.
Download Full Size | PDF
Contrast ratio (CR) is defined as the ratio of maximum intensity to minimum intensity, and it can be derived as [37]

Distortion of real and virtual images, defined as the displacement of image height or ray location, are plotted in Figs. 11(a) and 11(b), respectively, where distortion of the real images is less than 0.04%, and distortion of the virtual images is less than 0.73%.
Fig. 11 Calculated distortion of (a) real and (b) virtual images. For the real image, the distortion is less than 0.04%. For the virtual image, the distortion is less than 0.73%.
Download Full Size | PDF
For a qualitative analysis of imaging quality, both real and virtual images are visualized from the imaging simulation that takes into account the effects of distortion, aberration blurring, diffraction blurring, and relative illumination, as shown in Fig. 12. By comparing the original and simulated images, it can be seen that the real image is identical to the original one despite some chromatic aberration, while the virtual image turns out to be kind of blurred. It has to be mentioned that those simulated images are what exactly appear on the retina.
Fig. 12 (a) Original, (b) real, and (c) virtual images. By comparing the original and simulated images, it can be seen that the real image is identical to the original one despite some chromatic aberration, while the virtual image turns out to be kind of blurred.
Download Full Size | PDF
A retinal-projection-based NED, also termed RPD, which enables vision correction is proposed. Its structure is highlighted by a corrective lens, an array of tiled OLEDs, and a transmissive SLM. Based on the simulation, its key performance including FOV, MTF, and distortion has been studied. For the real image, FOV is 114° (diagonal), MTF is above 0.4 at 280 cycles/mm, CR is 666, and distortion is less than 0.04%. For the virtual image, FOV is 117° (diagonal), MTF is above 0.4 at 120 cycles/mm, CR is 31, and distortion is less than 0.73%. As opposed to the combiner and waveguide based NEDs, RPD exhibits several unique features. First, instead of using an eyepiece or ocular lens, RPD relies on the eye itself in imaging the virtual objects. Second, a more compact form factor is expected as no folding optics are needed for RPD. Third, the distance and size of virtual objects hinge on the status of eye, including its diopter and pupil. Fourth, hopefully, its ultra-large FOV could be a trump card in playing the game of thrones of NEDs.
Ministry of Science and Technology of China (MOST) (2013CB328804, 2015AA017001); Shanghai Rockers Inc. (15H100000157); Shanghai Jiao Tong University (AF0300204, WF101103001/085).
1. Wikipedia, “Augmented reality,” https://en.wikipedia.org/wiki/augmented_reality.
2. Wikipedia, “Game of thrones,” https://en.wikipedia.org/wiki/game_of_thrones.
3. H.-S. Chen, Y.-J. Wang, P.-J. Chen, and Y.-H. Lin, “Electrically adjustable location of a projected image in augmented reality via a liquid-crystal lens,” Opt. Express 23(22), 28154–28162 (2015). [PubMed]  
4. Q. Gao, J. Liu, J. Han, and X. Li, “Monocular 3D see-through head-mounted display via complex amplitude modulation,” Opt. Express 24(15), 17372–17383 (2016). [PubMed]  
5. Q. Gao, J. Liu, X. Duan, T. Zhao, X. Li, and P. Liu, “Compact see-through 3D head-mounted display based on wavefront modulation with holographic grating filter,” Opt. Express 25(7), 8412–8424 (2017). [PubMed]  
6. J. P. Rolland, “Wide-angle, off-axis, see-through head-mounted display,” Opt. Eng. 39(7), 1760–1767 (2000).
7. S. Liu, H. Hua, and D. Cheng, “A novel prototype for an optical see-through head-mounted display with addressable focus cues,” IEEE Trans. Vis. Comput. Graph. 16(3), 381–393 (2010). [PubMed]  
8. R. Zhu, G. Tan, J. Yuan, and S.-T. Wu, “Functional reflective polarizer for augmented reality and color vision deficiency,” Opt. Express 24(5), 5431–5441 (2016).
9. Y. Amitai, S. Reinhorn, and A. A. Friesem, “Visor-display design based on planar holographic optics,” Appl. Opt. 34(8), 1352–1356 (1995). [PubMed]  
10. Y. Amitai, “Extremely compact high-performance HMDs based on substrate-guided optical element,” in SID Symposium (2004), pp. 310–313.
11. H. Mukawa, K. Akutsu, I. Matsumura, S. Nakano, T. Yoshida, M. Kuwahara, and K. Aiki, “A full-color eyewear display using planar waveguides with reflection volume holograms,” J. Soc. Inf. Disp. 17(3), 185–193 (2009).
12. D. Cheng, Y. Wang, H. Hua, and M. M. Talha, “Design of an optical see-through head-mounted display with a low f-number and large field of view using a freeform prism,” Appl. Opt. 48(14), 2655–2668 (2009). [PubMed]  
13. Q. Wang, D. Cheng, Y. Wang, H. Hua, and G. Jin, “Design, tolerance, and fabrication of an optical see-through head-mounted display with free-form surface elements,” Appl. Opt. 52(7), C88–C99 (2013). [PubMed]  
14. X. Hu and H. Hua, “High-resolution optical see-through multi-focal-plane head-mounted display using freeform optics,” Opt. Express 22(11), 13896–13903 (2014). [PubMed]  
15. O. Cakmakci and J. Rolland, “Head-worn displays: a review,” J. Disp. Technol. 2(3), 199–216 (2006).
16. S. C. McQuaide, E. J. Seibel, J. P. Kelly, B. T. Schowengerdt, and T. A. Furness III, “A retinal scanning display system that produces multiple focal planes with a deformable membrane mirror,” Displays 24(2), 65–72 (2003).
17. A. Maimone, D. Lanman, K. Rathinavel, K. Keller, D. Luebke, and H. Fuchs, “Pinlight displays: wide field of view augmented reality eyeglasses using defocused point light sources,” ACM Trans. Graph. 33(4), 89 (2014).
18. M. Sugawara, M. Suzuki, and N. Miyauchi, “Retinal imaging laser eyewear with focus-free and augmented reality,” in SID Display Week (2016), pp. 164–167.
19. C. P. Chen, Z. Zhang, and X. Yang, “A head-mounted smart display device for augmented reality,” CN Patent 201610075988.7 (2016).
20. L. Zhou, C. P. Chen, Y. Wu, K. Wang, and Z. Zhang, “See-through near-eye displays for visual impairment,” in The 23rd International Display Workshops in conjunction with Asia Display (2016), pp. 1114–1115.
21. L. Zhou, C. P. Chen, Y. Wu, Z. Zhang, K. Wang, B. Yu, and Y. Li, “See-through near-eye displays enabling vision correction,” Opt. Express 25(3), 2130–2142 (2017).
22. C. P. Chen, Y. Wu, and L. Zhou, “An optical display device for augmented reality,” CN Patent 201610112824.7 (2016).
23. Y. Wu, C. P. Chen, L. Zhou, Y. Li, B. Yu, and H. Jin, “Design of see-through near-eye display for presbyopia,” Opt. Express 25(8), 8937–8949 (2017). [PubMed]  
24. Y. Wu, C. P. Chen, L. Zhou, Y. Li, B. Yu, and H. Jin, “Near-eye display for vision correction with large FOV,” in SID Display Week (2017), pp. 767–770.
25. C. Chen, H. Li, Y. Zhang, C. Moon, W. Y. Kim, and C. G. Jhun, “Thin-film encapsulation for top-emitting organic light-emitting diode with inverted structure,” Chin. Opt. Lett. 12(2), 022301 (2014).
26. Wikipedia, “Human eye,” https://en.wikipedia.org/wiki/human_eye.
27. Wikipedia, “Cornea,” https://en.wikipedia.org/wiki/cornea.
28. University of Notre Dame, “Physics of the eye,” https://www3.nd.edu/~nsl/Lectures/mphysics.
29. F. L. Pedrotti, L. M. Pedrotti, and L. S. Pedrotti, Introduction to Optics, 3rd ed. (Addison-Wesley, 2006).
30. Y.-J. Wang, P.-J. Chen, X. Liang, and Y.-H. Lin, “Augmented reality with image registration, vision correction and sunlight readability via liquid crystal devices,” Sci. Rep. 7(1), 433 (2017). [PubMed]  
31. H. K. Walker, W. D. Hall, and J. W. Hurst, Clinical Methods: The History, Physical and Laboratory Examinations 3rd Edition (Butterworth-Heinemann, 1990), Chap. 58.
32. E. Lueder, Liquid Crystal Displays: Addressing Schemes and Electro-Optical Effects, 2nd ed. (Wiley, 2010).
33. X. Shi, J. Wang, J. Liu, S. Huang, X. Wu, C. Chen, J. Lu, Y. Su, Y. Zheng, W. Y. Kim, and G. He, “High-performance green phosphorescent top-emitting organic light-emitting diodes based on FDTD optical simulation,” Org. Electron. 15(4), 864–870 (2014).
34. S. D. Brotherton, Introduction to Thin Film Transistors: Physics and Technology of TFTs (Springer, 2013).
35. J. Cao, J.-W. Xie, X. Wei, J. Zhou, C.-P. Chen, Z.-X. Wang, and C. Jhun, “Bright hybrid white light-emitting quantum dot device with direct charge injection into quantum dot,” Chin. Phys. B 25(12), 128502 (2016).
36. C. P. Chen, Y. Li, Y. Su, G. He, J. Lu, and L. Qian, “Transmissive interferometric display with single-layer Fabry-Pérot filter,” J. Disp. Technol. 11(9), 715–719 (2015).
37. R. E. Fischer, B. Tadic-Galeb, and P. R. Yoder, Optical System Design, 2nd ed. (McGraw-Hill Education, 2008).
38. B. Brown, The Low Vision Handbook for Eyecare Professionals, 2nd ed. (Slack Inc., 2007).
39. H. H. Hopkins, “The numerical evaluation of the frequency response of optical systems,” Proc. Phys. Soc. B 70(10), 1002–1005 (1957).



























Abstract
1. Introduction
2. Design principle
3. Results and discussion
4. Conclusions
Funding
References and links
This website uses cookies to deliver some of our products and services as well as for analytics and to provide you a more personalized experience. Click here to learn more. By continuing to use this site, you agree to our use of cookies. We've also updated our Privacy Notice. Click here to see what's new.
Vladimir Krotov, Christophe Martinez, and Olivier Haeberlé





The augmented reality (AR) industry requires both aesthetic designs and high performances of AR devices. This complex dilemma challenges R&D groups from all over the world to improve existing systems or propose new, breakthrough designs. The unconventional concept of direct retinal projection display may be one. It is based on see-through holographic retinal projection, with the image being formed via the so-called self-focusing effect. In this paper, we describe an experimental validation of this self-focusing effect and introduce a possible approach of self-focusing performance evaluation. Experimental image formation capability is demonstrated and compared with simulation results. Main present limitations of the concept are discussed, such as pixel addressing design and image resolution/sharpness conflict.

© 2019 Optical Society of America under the terms of the OSA Open Access Publishing Agreement
Augmented reality (AR) comes smoothly but steadily into our daily life. There are a lot of applications for AR [1,2], in very diverse domains such as medicine and surgery, education construction safety, or maintenance, and numerous others [3–8]. Many scientific groups are in the process of creating their own smart glasses devices [9,10]. In this field, the need to overlay digital information to the surrounding real world usually requires the use of micro-displays. These devices, in turn, require complex optical systems to provide a comfortable vision. Furthermore, the formation of the virtual image of the screen by the device is accompanied by constraints related to the management of pupils that limit the viewing angle and the positioning latitude of the eye while leading to bulky systems.
Bernard Kress and Thad Starner [11] described a wide range of different types of head-mounted displays (HMD). Most of them use conventional approaches for image formation based on the use of a micro-display and an optical system. The CEA proposed in 2014 an original concept of image projection that combines holography, integrated photonics and free space optics [12]. The concept aims at projecting an image into the observer's eye, onto the retina, without any lens. Convergence is provided by the eye through a phenomenon that we describe as self-focusing. This unconventional approach was explained recently for a concept of retinal projection display [13].
The concept is based on the design of an emissive screen close to the eye, where the emission points are duplicated so as to form the illusion of a plane wavefront. This is the case of Fig. 1(a). Thanks to the self-focusing effect, the multiple beams emitted from the display focalize onto the retina without any lens between the emission surface and the eye. The resulting focalized spot corresponds to one Emissive Point Distribution (EPD). An image, composed of several spots, requires a respective number of EPDs, which means that the surface of the display is covered by various EPDs to be able to form an image. Every EPD, in turn, consists in emissive points that are activated simultaneously and coherently for a given EPD, and designed to generate a directive spherical wave front with main wave vector
k
→
s
.
Fig. 1 Self-focusing image formation: (a) concept of near to eye display where a distribution of spherical wavefronts is emitted and produces the illusion of a plane wavefront that the eye lens focuses on the retina; (b) our validation setup: a laser beam passes through a transmission device consisting of a distribution of pinholes in an opaque metallic layer.
Download Full Size | PDF
As the wavefronts emitted from every point are phase adjusted for a given EPD and angularly orientated, they produce a resulting planar wavefront of wave vector
k
→
p
. Through multiple interferences the spherical wavefronts focalize the beams onto the retina due to self-focusing effect through the eye pupil.
Self-focusing effect has already been demonstrated experimentally for data storage applications [14]. The use of a distribution of small apertures that approximates the behavior of a large, continuous aperture has also been demonstrated experimentally for astronomical applications [15–17]. However, the unconventional aspect of this approach for display applications opens questions about the perception of the so-formed images on the retina. Theoretical part was discussed previously [13], and we now experimentally investigate the capability of an unconventional image formation directly onto the retina using a device that simulates the self-focusing display concept as shown in Fig. 1(b). A planar wavefront with a given angular direction of wave vector
k
→
p
passes through a transmission device consisting of a distribution of apertures in an opaque metallic layer. The device simulates a distribution of emitters that generate coherent, phase-adjusted spherical wavefronts
k
→
s
with main wave vector direction
k
→
p
.
The pinhole distribution represents the EPD and the device is placed in front of an optical system that mimics the eye.
In this article, we present first experimental results of unconventional image formation for the concept of retinal projection display developed by our laboratory. Section 2 presents the choice of the EPD and the mathematical development used to evaluate the randomness of the distributions. The third section describes the experimental validation of the self-focusing effect. Various EPDs are evaluated and a double Gaussian model is introduced to describe the signal formed by a central peak and a surrounding noise. This model is used in the last section to theoretically evaluate the behavior of the concept as an image-forming device. Simulations are then compared to experimental measurements of a self-focused image obtained from an optical system that mimics the eye.
The EPD design plays a dominant role for self-focusing, therefore the best distribution to implement must be found. We introduce some possible solutions and discuss their advantages and drawbacks. As previously shown [13], the best theoretical EPD for image formation is a random distribution. However, such a solution represents a technological challenge for the device manufacturing and the EPD addressing.
On the opposite, the easiest solution for the device manufacturing and addressing is a periodic EPD, but this solution is not acceptable from the image formation point of view. Hence, a compromise is to be found, such as a quasi-random distribution.
We introduce the Emissive Unit Cell (EUC) to refer to the elementary building block of the display. EUC is a region with sizes Λ1 and Λ2, composed by one emissive point from each EPD. As described in [13], each emissive point corresponds to the intersection of a waveguide and an electrode. To provide the interference phenomenon, each EPD is addressed by a single laser. Image formation is then conceived in a scanning mode with a frequency that depends on the number of lasers used to project the image. It comes that each EUC will be addressed by several lasers nlaser and electrodes nelect that fixes the number of the pixels N of the projected image:
The size of the EUC is then given by the distances between rows of waveguides dg and between columns of electrodes de :
A simple way to solve the addressing problem is to create a 2D grid of periodically-distributed, straight electrodes. The coordinates of the EPD xi,j and yi,j are then given by the equation:
Another solution for the EPD is to define a quasi-random distribution inside the EUC:
Where rnd is a random number, with rnd ∈ [0:1].
This solution is theoretically interesting but is hard to implement in practice because of an intersection distribution between a waveguide and an electrode trajectory function. To generalize the addressing problem, we define these two trajectory functions as fe(x,y) and fg(x,y). Solving the EPD design problem then requires solving the intersection equation between a distribution of waveguide and electrode function.
With respect to Eqs. (3) and (4), this can be done in two ways. First by keeping a periodic distribution:
Secondly by introducing a random access addressing:
To facilitate the determination of the intersections between fe and fg we choose to describe these functions as the trajectory of a moving particle. The location
r
→
(
x
,
y
)
of the particle is given by the time varying relation:
The velocity function
V
→
(
t
)
that fixes the trajectory is defined by a time varying amplitude and direction:
In this work, we choose a simple sinusoidal model with the following definition:
Starting from these equations, we calculate the intersections of the functions fe and fg according to the two cases described in Eq. (5) (Cross Sinusoidal distribution CS) and (6) (Cross Random Sinusoidal distribution CRS). The resulting EPDs are given in Figs. 2(e) and 2(g). It consists on a set of NEPD points. The cases of periodic and quasi-random distributions are described in Figs. 2(a) and 2(c) respectively.
Fig. 2 Calculated distributions and their histograms of the closest distance between the nearest pinholes: (a) and (b) periodic distribution; (c) and (d) quasi-random distribution; (e) and (f) CS distribution; (g) and (h) CRS distribution.
Download Full Size | PDF
The physical process involved in self-focusing is based on a multiple interference phenomenon. It can be described as a superposition of series of Young’s fringes figure, with a period and an orientation given by the amplitude and orientation of the vector
u
→
p
,
q
that connects two interfering emissive points p and q from a given EPD.
The efficiency of the self-focusing effect is related to the number and to the diversity of this vector distribution. To evaluate this diversity, we introduce the histogram of the minimal distance distribution.
We first calculate the distribution of the minimal distance s from a point to its neighbors:
We consider the case of a square EUC: Λ1 = Λ2 = Λ. The histogram is calculated with a number Nh of bins calculated between 0 and Λ:
The histograms of the various EPDs are presented Fig. 2, right part. In the case of the periodic distribution, the histogram is a Dirac located at the period of the distribution, see Fig. 2(b). The histogram is a large distribution for the quasi-random EPD as shown in Fig. 2(d). The choice of a sinusoidal design for the waveguides and electrodes allows to enlarge the histogram function particularly in the case of the random access, case of Fig. 2(h) with respect to the periodic one, as depicted in Fig. 2(f).
This first mathematical approach to the design of the EPD shows some orientations that will be followed in next investigations to improve the histogram distribution in order to achieve better self-focusing performances.
To validate the link between the EPD histogram and the self-focusing behavior, the various EPDs considered in this study have been manufactured in an opaque layer, and experimentally evaluated using an optical set-up that transposes the self-focusing behavior from an emissive to a transmissive configuration.
The optical set-up consists in a 513 nm laser diode, emitting through an optical fiber. The beam impacts an opaque sample made out of a thin glass, coated with an opaque, black chrome layer. A distribution of apertures of various sizes and distribution functions has been etched through the opaque layer with a maskless lithographic process Fig. 3(a) [18]. In order to simulate a phase-adjusted emission through the EPD, the fiber laser is located at a large distance from the sample. The beam incident on the EPD sample can then be considered as representing a planar wave, as shown in Fig. 3(b).
Fig. 3 Experimental set-up for self-focusing effect validation: (a) pictures of a sinusoidal periodic apertures distribution mask, (b) set-up description.
Download Full Size | PDF
Beyond the aperture distribution sample, we evaluate the self-focusing effect with an imaging system that mimics the eye. We use as optical system an objective with a focal length f0 = 51 mm and a high resolution 6.6 MegaPixels monochrome CMOS sensor with a 3.5 µm pixel size. The distance between the aperture distribution sample and the optical system is about 20 mm.
The first results given in Figs. 4(a) and 4(b) present the case of a single aperture. In this case self-focusing does not occur, and one observes the blurred image of the aperture. Figure 4(a) shows the intensity of the beam measured on the CMOS sensor in angular coordinates. Figure 4(b) depicts the cross section of the signal.
Fig. 4 Diffraction patterns produced by various EPD, on the left the image on the image sensor (in green false color), on the right the intensity cross section compared with Gaussian function: (a) and (b) one pinhole; (c) and (d) periodic distribution; (e) and (f) quasi-random distribution; (g) and (h) CS distribution; (i) and j) CRS distribution. Figure (f), (h) and (j) are plotted in logarithmic intensity scale. Inset in Fig. (f) is a detail of the spel theory/measurement comparison in linear scale near 0 arcmin.
Download Full Size | PDF
We use Gaussian beam formalism as described in [13]. The waist w0 of the beam emitted from the aperture is taken as the radius of the aperture. The angular radius Δθ (HWHM) of the blurred signal is then given by:
With a 50 µm diameter aperture, an angular radius of 22 arcmin is calculated. The value is consistent with the value of the waist deduced from the Gaussian model of Fig. 4(b) (waist 30 arcmin).
Figures 4(c) and 4(d) describe the image resulting from a periodic aperture distribution. The period Λ of the 50 µm diameter apertures distribution is 400 µm. The periodic aperture distribution generates diffraction orders with an angular period given by λ/Λ. It corresponds to an angular period of 4.4 arcmin, consistent with our measurements.
Figures 4(e) and 4(f) describe the case of a quasi-random distribution calculated on a 400 µm periodic EUC. These results confirm the self-focusing behavior. One observes a central peak, that we name spel (for elementary spot), with an angular radius δθ (first zero of the Airy function) given by the entrance pupil diameter Dp:
The aperture of the optical system is given by the choice of the f#. In order to image a spel over several pixels, while still keeping a sufficiently high signal level, we choose an aperture of f/15 that corresponds to Dp = 3.4 mm. The calculated angular radius of 0.6 arcmin is consistent with the measurements (the inset of Fig. 4(f) shows the cross section pixel signal of the spel and a Gaussian model with waist 0.45 arcmin).
The signal outside of the spel is distributed as speckle noise.
Figures 4(g) and 4(h) show the result for the CS distribution. The resonant contribution is strongly reduced, compared to Fig. 4(d), but speckle noise still presents remaining peaks, compared to Fig. 4(f).
Finally, the choice of a CRS distribution allows for reducing the contribution of the unwanted peak, as shown on Figs. 4(i) and 4(j).
The experimental results confirm that the self-focusing effect implemented with a random EPD allows for isolating a spel on the retina. This behavior is accomplished at the price of a speckle noise surrounding the spel. The characteristics of this noise contribution are related to the EPD:
To evaluate the impact of the noise on the imaging performance of the self-focusing effect, we introduce a double Gaussian model to describe the spel and the noise. Figure 5 shows a cross section of the spel for the quasi-random EPD case.
Fig. 5 Double-Gaussian model for the spel. (a), (b) and (c) spel intensity distributions for three LD power (d) cross intensity function of the concatenated images (blue dotted curve) and Gaussian model for the peak and the noise (orange curves).
Download Full Size | PDF
One can separate the signal in two contributions:
Both intensity functions are described by the Gaussian beam formula:
I0i being the maximum intensity of the signal that fixes the total energy of the signal:
The size of the spel w1 is given by the total pupil aperture:
The size of the noise contribution w2 is given by the pinhole aperture diameter dp as for the case of Eq. (12):
The terms E1 and E2 represent the energy contains in the spel and in the noise, respectively. We define the parameter γ as the ratio in dB between the energy of the spel and the energy of the self-focused signal:
Figure 5 shows the experimental characterization of the spel in the case of a random EPD with the aperture diameter dp = 75 µm. To fit both Gaussian models, three intensities of the LD are used to overcome the limited CMOS sensor dynamic: Fig. 5(a) to Fig. 5(c).
The cross intensities functions are given in spatial coordinates on the CMOS sensor. The double Gaussian parameters are: I01 = 1; w1 = 6.5 µm; I02 = 0.065; w2 = 200 µm and are consistent with the theoretical value given by Eqs. (16) and (17): w1 = 6.7 µm and w2 = 224 µm. The Gaussian model parameters give a noise ratio γ = 18.
The double Gaussian model has the great interest to give an analytical expression for the spel that can be used to evaluate the image forming process involved in self focusing as a function of γ.
Simulation of imaging process in conventional imaging is based on the convolution of an image by the Points Spread Function (PSF) of the optical system. This process related to linear systems traduces the continuity of the image in the object and image plane of the optical system. Here, one has to consider a process with no image in the object plane of the optical system, and an image representation that is not continuous in the image plane. The simulation can still be made on the basis of the convolution with the optical response of the system, but we must consider here the image as a series of Dirac that localize the spels on the retina.
Considering an image formed onto the retina by a succession of spels with positions on the retina given by the vector
r
→
u
,
v
and an intensity given by the matrix Mu,v, one describes the spel intensity distribution with the function
g
(
r
→
)
. The image Iret on the retina is given by the sum of the spel contributions:
We must here underline the specificity of the imaging process in self-focusing. In conventional optics the image on the retina is expressed by:
The function rectW represents the geometric image of the square pixel on the retina (with size and pitch W) and traduces the continuity of the image as a succession of adjacent squares with size given by the display characteristics and the magnification of the optical system.
According to Eq. (20), if we consider a micro display with resolution Nl x Nc pixels, an optical system gives the same result
I
˜
r
e
t
(
r
→
)
for an image of size Nl /2 x Nc/2 or for the same image oversampled by a factor 2.
In the case of self-focusing imaging, the oversampling by a factor 2 leads to an increase of a factor 4 on the number of spels and modifies the resulting image. This effect is described in Fig. 6. The image to be displayed is the letter T and is shown in Fig. 6(a) as given in a conventional display with adjacent pixel. In self-focusing each pixel can be represented by one spel as in Fig. 6(b), by four spels as in Fig. 6(c) or more. Increasing the number of spel is supposed to increase the image quality. However, as the total number of emissive points is fixed for a given display, increasing the number of spel reduces the number of emissive points allocated to each spel. This reduction of the EPD size degrades each individual spel, and by this way is supposed to degrade the overall image quality.
Fig. 6 (a) Image of the letter “T” coded with 5x5 pixels, (b) self-focusing imaging with one spel per pixel and (c) self-focusing imaging with four spels per pixel.
Download Full Size | PDF
A compromise must be found between the number of spel and the efficiency of the spel in terms of image quality, so-called resolution/sharpness conflict. The goal of this paper being to present first experimental validations of self-focusing imaging, this complex aspect of self-focusing will not be more discussed here.
The set-up used to evaluate self-focusing imaging is described in Fig. 7. As we don’t have yet retinal display devices, the principle of the validation is to pass through a distribution of apertures as in the case of Fig. 3. The objective is not to generate a single planar wave through our EPD but a series of planar wave that project an image onto the retina. We use a micro-display with a pixel size δ and a collimating optics with focal fc as wavefront generator.
Fig. 7 Optical set-up used to evaluate self-focusing imaging.
Download Full Size | PDF
Each pixel u,v of the display generates a wavefront of vector
k
→
u
,
v
with the angular coordinates:
With the angular relations given in the paraxial approximation:
The wavefronts are transmitted to the imaging system through the apertures holes of the EPD and form the spels of the image at coordinates
r
→
u
,
v
on the CMOS sensor that mimics the retina.
The microdisplay used in this work is depicted on Fig. 8. As the aperture distribution used in this work has a very low transmission factor due to the small size of the pinholes, a specific high power monochromatic microdisplay developed in our laboratory has been used. This binary microdisplay forms a static image of the word “LETI” on a 13 x 5 pixels resolution, and a single, isolated pixel is also activated. The size of the pixel is 8 x 8 µm2 with a pixel pitch of 10 µm. Typical brightness for this display is about 3000 Cd/m2 [19].
Fig. 8 MicroLED display used to generate the image wavefront. On the left the display size compared with 1 eurocent; on the right the magnification of the display with an isolated activated pixel as well as the “LETI” acronym.
Download Full Size | PDF
For these measurements, the resolution of the sensor is increased and a new CMOS sensor is used with a smaller 1.67 µm pixel size.
We show on Fig. 9 the results of the image forming process on the various EPDs considered in this study. Figures are given in gray level and angular coordinates. To give a better visual rendering the gray scale of the figure is reversed. Figure 9(a) shows the image of the micro display without any aperture. It represents the perfect image only limited by the aberration of the optical system. Due to a too low energy level, the experimental evaluation of the imaging process through a single aperture was not possible.
Fig. 9 Results on the experiment (a) diffraction limited, (b) periodic distribution, (c) quasi-random EPD and (d) CRS EPD.
Download Full Size | PDF
Figure 9(b) shows the imaging through a periodic aperture distribution (dp = 75 µm, Λ = 400 µm). We observe a duplication of the elementary image as expected from the theory. Figures 9(c) and 9(d) show the result of imaging through the quasi-random and CRS EPDs.
We use as image a 13 x 5 pixels sampling of the acronym “LETI” representative of the micro-display of Fig. 8. We choose a 130 x 50 pixels resolution for the whole image, with an emissive-pixel periodicity of the display of 10 pixels, and each emissive zone being itself sampled with 8 x 8 pixels.
Simulations are made on the basis of the double Gaussian model. The results based on the parameters given from Fig. 5 give low correspondence with our experimental measurements and correcting factors must be introduced in the value of w1, w2 and γ as given by Eqs. (16)– (18). Both values of the waist must be increased by a factor 1.5 and a noise ratio of 15 dB is used instead of 18 dB.
Figure 10(a) shows simulation for a conventional imaging behavior. We use the double Gaussian model with a ratio factor γ ~0 dB.
Fig. 10 Results on the imaging simulation process (a) diffraction limited, (b) periodic distribution, (c) quasi-random EPD, (d) CRS EPD and (e) comparison of the middle cross section of the measured and simulated images in the quasi-random EPD.
Download Full Size | PDF
We simulate the case of a periodic EPD with a function that describes the spel distribution as shown on Fig. 4(c). The following definition is used for g (with the corrected waist values):
First term G0 is used to normalize the total energy. Second term is the envelope of the apodization function. Third term is the grid of resonances due to the periodic distribution, vector
r
→
k
,
l
giving the coordinates of the resonant peaks:
Figure 10(b) gives the result of the simulation. One can see that the repetition of the resonant peaks prevents the recognition of the word LETI, as observed in the measurements and as predicted by the theory.
The quasi-random distribution is simulated with the double Gaussian formalism:
The simulation result given in Fig. 10(c) is used as a comparison between theory and experiment. Figure 10(e) compares the cross section of the experimental and simulated image made on the middle of the image in the quasi-random case. This case is used as a reference as it corresponds to the optimal imaging result for the EPD design. Correcting factors are determined to give the best fit between the two curves. Note that the Cross Sinusoidal and Cross Random Sinusoidal distributions (CS and CRS) distributions are easier to fabricate but don’t correspond to an optimal distribution realistic with our manufacturing capabilities. Future work will focus on determining such distributions with better self-focusing performances than CS and CRS. Note also that the brightness of the image is not strongly modified by the kind of EPD used, as long as the main characteristics of the EPD (size and density of the apertures) remain the same.
We can now recognize in Fig. 10(c) the word LETI surrounded by a strong noise contribution that degrades the contrast.
The CRS case is described by a function that mixes the formalism of Eqs. (24) and (26).
The first two terms of the equation correspond to the spel and the noise, respectively, as in Eq. (24). The last term corresponds to the ghost peaks that appear at coordinate
r
→
q
,
p
with an angular extension w3 given in the experimental results of Fig. 4(i).
Figure 10(d) gives the result of image simulation with the CRS distribution. The image of the word LETI can be recognized but some ghost images appear at the ghost peak location as in the measurements.
The experimental results confirm the ability to produce an image through the self-focusing effect. The model of a double Gaussian intensity distribution is efficient to describe the imaging process on the retina. It allows for fast simulations as summations of the various spels that form the image.
The main drawback of the device will concern the contrast of the retinal image and the presence of ghost images produced by limited random distributions. The EPDs that have been considered throughout this paper are related to simple sinusoidal models, and more complex configurations are currently under investigations to improve the image quality.
The best fit between simulation and measurements in Fig. 10 requires introducing some correcting factors. This correction can be explained by the difference in spectral characteristics between the laser diode used in Fig. 5 (FWHM ~1 nm) and the pixel of the microLED display used in Fig. 9 (FWHM ~30 nm) [20].
The EPD parameters have been chosen in order to simplify the visual analysis as shown on Figs. 9 and 10. Future work will focus on the parameters corresponding to our device with an emissive aperture diameter smaller than 10 µm and an entrance pupil diameter of about 6 mm. The three orders of magnitude difference between these two values make visual analysis difficult if one wants to evaluate on the same graphic the effect of the spel and of the whole surrounding noise as in Fig. 10(e).
For this analysis it is also of primary importance to consider the eye model, particularly in terms of extended sensing dynamic and non-uniform resolution. Specific diffusion behavior of the holographic part of the device also has to be considered for a rigorous analysis of the system performance [21].
It is also worth mentioning the influence of the source bandwidth. In particular, the increase of the spectral bandwidth expands the speckle figure around the central peak and tends to increase the ratio between the energy of the spel and the energy of the noise. This aspect still requires an in-depth analysis, however, effects of wavelength and bandwidth have been described experimentally in [20], to which the interested reader is referred.
Then, the display concept we propose works with a set of different Emissive Point Distributions (EPDs) each associated with an image pixel. To be fully representative of the concept, each pixel of the word “LETI” should have been associated with a specific EPD. This configuration is not possible in the frame of this first experiment, as all the display pixels highlight a transmission aperture distribution. We are currently investigating use of a holographic printer, to evaluate a configuration with different EPDs, for a closer-to-real-case application. Nevertheless, even if the aperture distribution is here still common to all the pixels, it anyway well describes the effect of image formation through self-focusing effect, to demonstrate its practical interest.
After the introduction of a new concept of retinal projector [13], we studied an implementation of the imaging process through the self-focusing effect. This first experimental demonstration allows introducing first elements of design for an Emissive Point Distribution consistent with the requirement of the addressing process through the intersection of sinusoidal waveguides and electrodes. First results are positive, but show that more complex addressing design has to be found in view of improving the visual rendering of the retinal projection.
Main limitations of the concept have been highlighted. The noise generated by the sampling of the planar wave by a series of spherical waves is a first limitation, inherent to the concept. Its reduction requires a compromise between the size and the number of the EPD, that is, between the resolution and the sharpness of the image (resolution/sharpness conflict). It is well described by a simple double Gaussian model that allows for fixing a signal to noise design parameter γ.
Another limitation is the presence of ghost images, directly related to the choice of the EPD and to its actual degree of randomness.
The link between γ, the EPD definition, the sampling and the angular size of the images is difficult to establish and will require pursuing research with the introduction of a more detailed eye model, in order to finely tune the image formation parameters, which are specific to this new image projection modality.
Finally, a natural extension of this work would be to study application for colors display. Here, monochrome demonstration of the self-focusing effect is provided, but the use of multiple wavelengths is possible, using same EPD for all the pixels of the image. It could allow for evaluating the spectral behavior of the self-focusing effect in a red-green-blue (RGB) configuration. However, in the case of the originally proposed retinal display concept [12,13], each EPD is to be associated with a specific hologram that fixes the emissive angle. As holograms are selective in wavelength, one would then have to use three separate holographic layers in order to encode three EPD color families for full RGB display.
The authors would like to thank both anonymous reviewers for their valuable suggestions to improve the manuscript.
1. B. Kress, E. Saeedi, and V. Brac-de-la-Perriere, “The segmentation of the HMD market: optics for smart glasses, smart eyewear, AR and VR headsets,” Proc. SPIE 9202, 92020D (2014).
2. M. Billinghurst, A. Clark, and G. Lee, “A Survey of Augmented Reality,” Found. Trends Human–Comp. Inter. 8, 73–272 (2015).
3. A. Meola, F. Cutolo, M. Carbone, F. Cagnazzo, M. Ferrari, and V. Ferrari, “Augmented reality in neurosurgery: a systematic review,” Neurosurg. Rev. 40(4), 537–548 (2017). [CrossRef]   [PubMed]  
4. J. W. Yoon, R. E. Chen, E. J. Kim, O. O. Akinduro, P. Kerezoudis, P. K. Han, P. Si, W. D. Freeman, R. J. Diaz, R. J. Komotar, S. M. Pirris, B. L. Brown, M. Bydon, M. Y. Wang, R. E. Wharen Jr., and A. Quinones-Hinojosa, “Augmented reality for the surgeon: Systematic review,” Int. J. Med. Robot. 14(4), e1914 (2018). [CrossRef]   [PubMed]  
5. J. Bacca, S. Baldiris, R. Fabregat, S. Graf, and Kinshuk, “Augmented Reality Trends in Education: A Systematic Review of Research and Applications,” J. Educ. Technol. Soc. 17(4), 133–149 (2014).
6. P. Chen, X. Liu, W. Cheng, and R. Huang, “A review of using Augmented Reality in Education from 2011 to 2016,” in Innovations in Smart Learning. Lecture Notes in Educational Technology, E. Popescu, Kinshuk, M. K. Khribi, R. Huang, M. Jemni, N.-S. Chen, and D. G. Sampson (eds). (Springer, 2017).
7. R. Palmarini, J. A. Erkoyuncu, R. Roy, and H. Torabmostaedi, “A systematic review of augmented reality applications in maintenance,” Robot. Comput.-Integr. Manuf. 49, 215–228 (2018). [CrossRef]  
8. X. Li, W. Yi, H.-L. Chi, X. Wang, and A. P. C. Chan, “A critical review of virtual and augmented reality (VR/AR) applications in construction safety,” Autom. Construct. 86, 150–162 (2018). [CrossRef]  
9. K. Akşit, W. Lopes, J. Kim, P. Shirley, and D. Luebke, “Near-eye varifocal augmented reality display using see-through screens,” ACM Trans. Graph. 36(6), 189 (2017). [CrossRef]  
10. A. Maimone, A. Georgiou, and J. S. Kollin, “Holographic near-eye displays for virtual and augmented reality,” ACM Trans. Graph. 36(4), 85 (2017). [CrossRef]  
11. B. Kress and T. Starner, “A review of head-mounted displays (HMD) technologies and applications for consumer electronics,” Proc. SPIE 8720, 87200A (2013). [CrossRef]  
12. C. Martinez, “Image projection Device,” US patent 2015/0370073 A1 (2015).
13. C. Martinez, V. Krotov, B. Meynard, and D. Fowler, “See-through holographic retinal projection display concept,” Optica 5(10), 1200–1209 (2018). [CrossRef]  
14. S. S. Hong, B. K. P. Horn, D. M. Freeman, and M. S. Mermelstein, “Lensless focusing with subwavelength resolution by direct synthesis of the angular spectrum,” Appl. Phys. Lett. 88(26), 261107 (2006). [CrossRef]  
15. P. S. Salvaggio, J. R. Schott, and D. M. McKeown, “Laboratory validation of a sparse aperture image quality model,” Proc. SPIE 9617, 961708 (2015).
16. A. Jiang, S. Wang, Z. Dong, J. Xue, J. Wang, and Y. Dai, “Wide-band white light sparse-aperture Fizeau imaging interferometer testbed for a distributed small-satellites constellation,” Appl. Opt. 57(11), 2736–2746 (2018). [CrossRef]   [PubMed]  
17. F. Eisenhauer, G. Perrin, W. Brandner, C. Straubmeier, K. Perraut, A. Amorim, M. Schöller, S. Gillessen, P. Kervella, M. Benisty, C. Araujo-Hauck, L. Jocou, J. Lima, G. Jakob, M. Haug, Y. Clénet, T. Henning, A. Eckart, J.-P. Berger, P. Garcia, R. Abuter, S. Kellner, T. Paumard, S. Hippler, S. Fischer, T. Moulin, J. Villate, G. Avila, A. Gräter, S. Lacour, A. Huber, M. Wiest, A. Nolot, P. Carvas, R. Dorn, O. Pfuhl, E. Gendron, S. Kendrew, S. Yazici, S. Anton, Y. Jung, M. Thiel, É. Choquet, R. Klein, P. Teixeira, P. Gitton, D. Moch, F. Vincent, N. Kudryavtseva, S. Ströbele, E. Sturm, P. Fédou, R. Lenzen, P. Jolley, C. Kister, V. Lapeyrère, V. Naranjo, C. Lucuix, R. Hofmann, F. Chapron, U. Neumann, L. Mehrgan, O. Hans, G. Rousset, J. Ramos, M. Suarez, R. Lederer, J.-M. Reess, R.-R. Rohloff, P. Haguenauer, H. Bartko, A. Sevin, K. Wagner, J.-L. Lizon, S. Rabien, C. Collin, G. Finger, R. Davies, D. Rouan, M. Wittkowski, K. Dodds-Eden, D. Ziegler, F. Cassaing, H. Bonnet, M. Casali, R. Genzel, and P. Lena, “GRAVITY: Observing the Universe in Motion,” Messenger (Los Angel.) 143, 16–24 (2011).
18. V. Krotov, C. Martinez, and O. Haeberlé, “Multiple beam diffractive setup for intraocular accommodation evaluation,” in Imaging and Applied Optics 2016 (2016), paper JT3A.32.
19. F. Templier, L. Dupré, B. Dupont, A. Daami, B. Aventurier, F. Henry, D. Sarrasin, S. R. Berger, F. Olivier, and L. Mathieu, “High-resolution active-matrix 10-μm pixel-pitch GaN LED microdisplays for augmented reality applications,” Proc. SPIE 10556, 105560I (2018).
20. C. Martinez, V. Krotov, and O. Haeberlé, “Experimental evaluation of self-focusing image formation in unconventional near-eye display,” Proc.SPIE 10676, 106760N (2018).
21. C. Martinez, B. Meynard, and Y. Lee, “Multiplexed pixelated hologram recording process for retinal projection device,” Proc. SPIE 10944, 109440N (2019).















Abstract
1. Introduction
2. Emissive points distribution for self-focusing
3. Experimental validation of self-focusing effect
4. Self-focusing image formation
5. Conclusion
Acknowledgments
References
